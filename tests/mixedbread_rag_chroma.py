# Generated by Copilot
# === Test RAG v·ªõi mixedbread-ai/mxbai-embed-large-v1 ===
# Th·ª≠ nghi·ªám m√¥ h√¨nh embedding ch·∫•t l∆∞·ª£ng cao Mixedbread AI cho ti·∫øng Vi·ªát

import os
import chromadb
from typing import List, Dict, Any, Optional
from chromadb.types import Metadata
from sentence_transformers import SentenceTransformer
import numpy as np


class MixedbreadVietnameseRAG:
    """
    RAG v·ªõi m√¥ h√¨nh Mixedbread AI mxbai-embed-large-v1 - ch·∫•t l∆∞·ª£ng cao cho multilingual
    """

    def __init__(self, db_path: str = "./mixedbread_chroma_db"):
        """
        Initialize v·ªõi mixedbread-ai/mxbai-embed-large-v1
        """
        print("üöÄ Kh·ªüi t·∫°o Mixedbread Vietnamese RAG...")
        print("üîÑ ƒêang t·∫£i m√¥ h√¨nh Mixedbread AI mxbai-embed-large-v1...")

        try:
            # Load Mixedbread AI model
            self.model = SentenceTransformer("mixedbread-ai/mxbai-embed-large-v1")
            print("‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng m√¥ h√¨nh Mixedbread AI!")
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i m√¥ h√¨nh: {e}")
            print("üîÑ Fallback v·ªÅ m√¥ h√¨nh m·∫∑c ƒë·ªãnh...")
            self.model = SentenceTransformer("all-MiniLM-L6-v2")
            print("‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh fallback!")
        # Initialize ChromaDB client (s·ª≠ d·ª•ng default embedding ƒë·ªÉ tr√°nh l·ªói protocol)
        self.client = chromadb.PersistentClient(path=db_path)

        # T·∫°o collection v·ªõi default embedding c·ªßa ChromaDB
        try:
            self.collection = self.client.get_collection("mixedbread_vietnamese")
            print("üìÇ ƒê√£ k·∫øt n·ªëi collection 'mixedbread_vietnamese'")
        except:
            self.collection = self.client.create_collection(
                name="mixedbread_vietnamese",
                metadata={
                    "model": "mixedbread-ai/mxbai-embed-large-v1",
                    "language": "multilingual",
                },
            )
            print("üìÇ ƒê√£ t·∫°o collection m·ªõi 'mixedbread_vietnamese'")

    def intelligent_text_splitter(
        self, text: str, chunk_size: int = 400, overlap: int = 50
    ) -> List[str]:
        """
        Text splitter th√¥ng minh cho ti·∫øng Vi·ªát v·ªõi Mixedbread model
        """
        # T√°ch theo ƒëo·∫°n vƒÉn tr∆∞·ªõc
        paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]

        chunks = []
        current_chunk = ""

        for para in paragraphs:
            # N·∫øu ƒëo·∫°n vƒÉn qu√° d√†i, chia nh·ªè h∆°n
            if len(para) > chunk_size:
                # T√°ch theo c√¢u
                sentences = []
                temp_sentence = ""

                for char in para:
                    temp_sentence += char
                    if char in ".!?":
                        # Ki·ªÉm tra kh√¥ng ph·∫£i s·ªë th·∫≠p ph√¢n
                        next_chars = para[
                            para.index(temp_sentence)
                            + len(temp_sentence) : para.index(temp_sentence)
                            + len(temp_sentence)
                            + 2
                        ]
                        if not (next_chars and next_chars[0].isdigit()):
                            sentences.append(temp_sentence.strip())
                            temp_sentence = ""

                if temp_sentence.strip():
                    sentences.append(temp_sentence.strip())

                # G·ªôp c√¢u th√†nh chunks
                for sentence in sentences:
                    if (
                        len(current_chunk) + len(sentence) > chunk_size
                        and current_chunk
                    ):
                        chunks.append(current_chunk.strip())

                        # Overlap: gi·ªØ l·∫°i c√¢u cu·ªëi
                        words = current_chunk.split()
                        overlap_words = min(len(words) // 3, overlap // 10)
                        if overlap_words > 0:
                            current_chunk = " ".join(words[-overlap_words:]) + " "
                        else:
                            current_chunk = ""

                    current_chunk += sentence + " "
            else:
                # ƒêo·∫°n vƒÉn ng·∫Øn, th√™m tr·ª±c ti·∫øp
                if len(current_chunk) + len(para) > chunk_size and current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""

                current_chunk += para + " "

        # Th√™m chunk cu·ªëi
        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return [chunk for chunk in chunks if len(chunk.strip()) > 20]

    def add_documents(
        self,
        texts: List[str],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None,
    ):
        """
        Th√™m documents v·ªõi Mixedbread embeddings
        """
        if ids is None:
            ids = [f"mixedbread_doc_{i}" for i in range(len(texts))]

        if metadatas is None:
            metadatas = [{"source": "unknown", "model": "mixedbread-ai"} for _ in texts]

        # Cast to compatible type
        metadata_list = [dict(meta) for meta in metadatas]  # type: ignore

        self.collection.add(
            documents=texts, metadatas=metadata_list, ids=ids  # type: ignore
        )

        print(f"üìù ƒê√£ th√™m {len(texts)} documents v·ªõi Mixedbread embeddings")
        return len(texts)

    def semantic_search(
        self,
        query: str,
        n_results: int = 5,
        filter_metadata: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """
        Semantic search v·ªõi Mixedbread embeddings
        """
        print(f"üîç ƒêang t√¨m ki·∫øm: '{query}'")

        results = self.collection.query(
            query_texts=[query], n_results=n_results, where=filter_metadata
        )

        # Format k·∫øt qu·∫£
        formatted_results = []
        if results["documents"] and results["documents"][0]:
            for i, doc in enumerate(results["documents"][0]):
                formatted_results.append(
                    {
                        "content": doc,
                        "distance": (
                            results["distances"][0][i] if results["distances"] else None
                        ),
                        "metadata": (
                            results["metadatas"][0][i] if results["metadatas"] else None
                        ),
                        "id": results["ids"][0][i] if results["ids"] else None,
                    }
                )

        return formatted_results

    def ingest_text_intelligent(
        self, text: str, metadata: Optional[Dict[str, Any]] = None
    ) -> int:
        """
        Ingest text v·ªõi intelligent chunking v√† Mixedbread embeddings
        """
        chunks = self.intelligent_text_splitter(text)

        if metadata is None:
            metadata = {"source": "manual", "model": "mixedbread-ai"}

        metadatas = [metadata for _ in chunks]
        ids = [f"{metadata.get('source', 'doc')}_{i}" for i in range(len(chunks))]

        return self.add_documents(chunks, metadatas, ids)

    def compare_models(self, query: str) -> Dict[str, Any]:
        """
        So s√°nh k·∫øt qu·∫£ v·ªõi c√°c collection kh√°c (n·∫øu c√≥)
        """
        results = {
            "mixedbread": self.semantic_search(query, n_results=3),
            "query": query,
        }

        return results


# === Test v·ªõi d·ªØ li·ªáu ti·∫øng Vi·ªát phong ph√∫ ===
if __name__ == "__main__":
    print("üéØ TESTING MIXEDBREAD AI mxbai-embed-large-v1 V·ªöI TI·∫æNG VI·ªÜT")
    print("=" * 70)

    # Kh·ªüi t·∫°o RAG v·ªõi Mixedbread model
    rag = MixedbreadVietnameseRAG(db_path="./mixedbread_chroma_db")

    # T·∫°o d·ªØ li·ªáu test phong ph√∫
    vietnamese_documents = {
        "tech_vietnam.txt": """
        C√¥ng ngh·ªá th√¥ng tin Vi·ªát Nam ƒëang b√πng n·ªï v·ªõi nhi·ªÅu ƒë·ªôt ph√° ·∫•n t∆∞·ª£ng.
        
        C√°c c√¥ng ty c√¥ng ngh·ªá h√†ng ƒë·∫ßu:
        - VNG Corporation: Zalo, Zing, game online
        - FPT Software: Ph·∫ßn m·ªÅm, AI, digital transformation
        - Tiki: Th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠, logistics th√¥ng minh  
        - Grab Vietnam: Super app, giao th√¥ng v√† giao h√†ng
        - VinSmart: Smartphone, IoT, c√¥ng ngh·ªá 5G
        
        Lƒ©nh v·ª±c AI/ML ph√°t tri·ªÉn m·∫°nh:
        X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n ti·∫øng Vi·ªát, computer vision, 
        chatbot th√¥ng minh, recommendation systems.
        
        Startup ecosystem s√¥i ƒë·ªông v·ªõi nhi·ªÅu qu·ªπ ƒë·∫ßu t∆∞ nh∆∞
        IDG Ventures, 500 Startups, Openspace Ventures.
        """,
        "programming_vietnam.txt": """
        C·ªông ƒë·ªìng l·∫≠p tr√¨nh Vi·ªát Nam r·∫•t nƒÉng ƒë·ªông v√† s√°ng t·∫°o.
        
        Ng√¥n ng·ªØ l·∫≠p tr√¨nh ph·ªï bi·∫øn:
        1. JavaScript/TypeScript - Frontend v√† Backend development
        2. Python - AI/ML, data science, automation
        3. Java - Enterprise applications, Spring framework
        4. C# - .NET development, Unity game development  
        5. Go - Microservices, cloud native applications
        6. Rust - System programming an to√†n, performance cao
        
        Framework v√† c√¥ng ngh·ªá hot:
        - React, Vue.js, Angular cho frontend
        - Node.js, Django, FastAPI cho backend
        - TensorFlow, PyTorch cho AI/ML
        - Docker, Kubernetes cho DevOps
        - AWS, Azure, GCP cho cloud computing
        
        C·ªông ƒë·ªìng h·ªçc t·∫≠p: Topcoder, Codeforces, HackerRank
        """,
        "ai_vietnam.txt": """
        Tr√≠ tu·ªá nh√¢n t·∫°o t·∫°i Vi·ªát Nam ƒëang tr·ªü th√†nh xu h∆∞·ªõng ch·ªß ƒë·∫°o.
        
        C√°c lƒ©nh v·ª±c AI ph√°t tri·ªÉn:
        - Natural Language Processing cho ti·∫øng Vi·ªát
        - Computer Vision cho y t·∫ø, n√¥ng nghi·ªáp
        - Recommendation Systems cho e-commerce
        - Chatbot v√† Virtual Assistant
        - Predictive Analytics cho t√†i ch√≠nh
        
        C√¥ng ty AI h√†ng ƒë·∫ßu:
        VinAI Research, FPT.AI, Zalo AI, VCC.ai, Anfin.
        
        ·ª®ng d·ª•ng th·ª±c t·∫ø:
        Ph√¢n t√≠ch X-quang t·ª± ƒë·ªông, d·ª± ƒëo√°n th·ªùi ti·∫øt,
        t·ªëi ∆∞u h√≥a giao th√¥ng, ph√¢n t√≠ch t√¨nh c·∫£m social media,
        chatbot chƒÉm s√≥c kh√°ch h√†ng ƒëa ng√¥n ng·ªØ.
        
        Th√°ch th·ª©c: Thi·∫øu d·ªØ li·ªáu ti·∫øng Vi·ªát ch·∫•t l∆∞·ª£ng,
        c·∫ßn ƒë·∫ßu t∆∞ nghi√™n c·ª©u fundamental AI research.
        """,
    }

    # T·∫°o th∆∞ m·ª•c docs
    os.makedirs("./docs", exist_ok=True)

    # Ghi v√† ingest documents
    total_chunks = 0
    for filename, content in vietnamese_documents.items():
        filepath = f"./docs/{filename}"
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content.strip())
        print(f"üìÑ ƒê√£ t·∫°o file: {filepath}")

        # Ingest v·ªõi metadata
        n = rag.ingest_text_intelligent(
            content,
            metadata={
                "source": filename,
                "lang": "vi",
                "topic": filename.split("_")[0],
            },
        )
        total_chunks += n
        print(f"‚úÖ ƒê√£ ingest {n} chunks t·ª´ {filename}")

    print(f"\nüìä T·ªïng c·ªông ƒë√£ ingest {total_chunks} chunks v·ªõi Mixedbread embeddings")

    # Test queries ch·∫•t l∆∞·ª£ng cao
    test_queries = [
        "Startup c√¥ng ngh·ªá n√†o th√†nh c√¥ng ·ªü Vi·ªát Nam?",
        "Ng√¥n ng·ªØ l·∫≠p tr√¨nh n√†o ph·ªï bi·∫øn cho AI/ML?",
        "VinAI Research l√†m g√¨ trong lƒ©nh v·ª±c AI?",
        "Framework JavaScript n√†o t·ªët cho frontend?",
        "Th√°ch th·ª©c c·ªßa AI Vi·ªát Nam l√† g√¨?",
        "C√¥ng ty n√†o ph√°t tri·ªÉn Zalo?",
        "Docker v√† Kubernetes d√πng ƒë·ªÉ l√†m g√¨?",
        "·ª®ng d·ª•ng AI trong y t·∫ø Vi·ªát Nam",
    ]

    print("\n" + "=" * 70)
    print("üß™ TESTING SEMANTIC SEARCH V·ªöI MIXEDBREAD AI")
    print("=" * 70)

    for i, query in enumerate(test_queries, 1):
        print(f"\n[Q{i}] {query}")
        print("-" * 60)

        try:
            results = rag.semantic_search(query, n_results=2)

            if results:
                for j, result in enumerate(results, 1):
                    distance = result["distance"]
                    content = (
                        result["content"][:200] + "..."
                        if len(result["content"]) > 200
                        else result["content"]
                    )
                    source = (
                        result["metadata"].get("source", "unknown")
                        if result["metadata"]
                        else "unknown"
                    )
                    topic = (
                        result["metadata"].get("topic", "general")
                        if result["metadata"]
                        else "general"
                    )

                    print(f"  [{j}] üéØ Distance: {distance:.3f} | Topic: {topic}")
                    print(f"      üìÑ Source: {source}")
                    print(f"      üìù Content: {content}")
                    print()
            else:
                print("  ‚ùå Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p")

        except Exception as e:
            print(f"  ‚ùå L·ªói: {e}")

        print()

    print("=" * 70)
    print("‚úÖ HO√ÄN TH√ÄNH TEST MIXEDBREAD AI RAG PIPELINE")
    print("=" * 70)

    # Th·ªëng k√™ collection
    collection_info = rag.collection.count()
    print(f"üìä Collection stats: {collection_info} documents total")
