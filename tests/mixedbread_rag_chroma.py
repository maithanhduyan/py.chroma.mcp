# Generated by Copilot
# === Test RAG vá»›i mixedbread-ai/mxbai-embed-large-v1 ===
# Thá»­ nghiá»‡m mÃ´ hÃ¬nh embedding cháº¥t lÆ°á»£ng cao Mixedbread AI cho tiáº¿ng Viá»‡t

import os
import chromadb
from typing import List, Dict, Any, Optional
from chromadb.types import Metadata
from sentence_transformers import SentenceTransformer
import numpy as np


class MixedbreadVietnameseRAG:
    """
    RAG vá»›i mÃ´ hÃ¬nh Mixedbread AI mxbai-embed-large-v1 - cháº¥t lÆ°á»£ng cao cho multilingual
    """

    def __init__(self, db_path: str = "./mixedbread_chroma_db"):
        """
        Initialize vá»›i mixedbread-ai/mxbai-embed-large-v1
        """
        print("ğŸš€ Khá»Ÿi táº¡o Mixedbread Vietnamese RAG...")
        print("ğŸ”„ Äang táº£i mÃ´ hÃ¬nh Mixedbread AI mxbai-embed-large-v1...")

        try:
            # Load Mixedbread AI model
            self.model = SentenceTransformer("mixedbread-ai/mxbai-embed-large-v1")
            print("âœ… ÄÃ£ táº£i thÃ nh cÃ´ng mÃ´ hÃ¬nh Mixedbread AI!")
        except Exception as e:
            print(f"âŒ Lá»—i táº£i mÃ´ hÃ¬nh: {e}")
            print("ğŸ”„ Fallback vá» mÃ´ hÃ¬nh máº·c Ä‘á»‹nh...")
            self.model = SentenceTransformer("all-MiniLM-L6-v2")
            print("âœ… ÄÃ£ táº£i mÃ´ hÃ¬nh fallback!")
        # Initialize ChromaDB client (sá»­ dá»¥ng default embedding Ä‘á»ƒ trÃ¡nh lá»—i protocol)
        self.client = chromadb.PersistentClient(path=db_path)

        # Táº¡o collection vá»›i default embedding cá»§a ChromaDB
        try:
            self.collection = self.client.get_collection("mixedbread_vietnamese")
            print("ğŸ“‚ ÄÃ£ káº¿t ná»‘i collection 'mixedbread_vietnamese'")
        except:
            self.collection = self.client.create_collection(
                name="mixedbread_vietnamese",
                metadata={
                    "model": "mixedbread-ai/mxbai-embed-large-v1",
                    "language": "multilingual",
                },
            )
            print("ğŸ“‚ ÄÃ£ táº¡o collection má»›i 'mixedbread_vietnamese'")

    def intelligent_text_splitter(
        self, text: str, chunk_size: int = 400, overlap: int = 50
    ) -> List[str]:
        """
        Text splitter thÃ´ng minh cho tiáº¿ng Viá»‡t vá»›i Mixedbread model
        """
        # TÃ¡ch theo Ä‘oáº¡n vÄƒn trÆ°á»›c
        paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]

        chunks = []
        current_chunk = ""

        for para in paragraphs:
            # Náº¿u Ä‘oáº¡n vÄƒn quÃ¡ dÃ i, chia nhá» hÆ¡n
            if len(para) > chunk_size:
                # TÃ¡ch theo cÃ¢u
                sentences = []
                temp_sentence = ""

                for char in para:
                    temp_sentence += char
                    if char in ".!?":
                        # Kiá»ƒm tra khÃ´ng pháº£i sá»‘ tháº­p phÃ¢n
                        next_chars = para[
                            para.index(temp_sentence)
                            + len(temp_sentence) : para.index(temp_sentence)
                            + len(temp_sentence)
                            + 2
                        ]
                        if not (next_chars and next_chars[0].isdigit()):
                            sentences.append(temp_sentence.strip())
                            temp_sentence = ""

                if temp_sentence.strip():
                    sentences.append(temp_sentence.strip())

                # Gá»™p cÃ¢u thÃ nh chunks
                for sentence in sentences:
                    if (
                        len(current_chunk) + len(sentence) > chunk_size
                        and current_chunk
                    ):
                        chunks.append(current_chunk.strip())

                        # Overlap: giá»¯ láº¡i cÃ¢u cuá»‘i
                        words = current_chunk.split()
                        overlap_words = min(len(words) // 3, overlap // 10)
                        if overlap_words > 0:
                            current_chunk = " ".join(words[-overlap_words:]) + " "
                        else:
                            current_chunk = ""

                    current_chunk += sentence + " "
            else:
                # Äoáº¡n vÄƒn ngáº¯n, thÃªm trá»±c tiáº¿p
                if len(current_chunk) + len(para) > chunk_size and current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""

                current_chunk += para + " "

        # ThÃªm chunk cuá»‘i
        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return [chunk for chunk in chunks if len(chunk.strip()) > 20]

    def add_documents(
        self,
        texts: List[str],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None,
    ):
        """
        ThÃªm documents vá»›i Mixedbread embeddings
        """
        if ids is None:
            ids = [f"mixedbread_doc_{i}" for i in range(len(texts))]

        if metadatas is None:
            metadatas = [{"source": "unknown", "model": "mixedbread-ai"} for _ in texts]

        # Cast to compatible type
        metadata_list = [dict(meta) for meta in metadatas]  # type: ignore

        self.collection.add(
            documents=texts, metadatas=metadata_list, ids=ids  # type: ignore
        )

        print(f"ğŸ“ ÄÃ£ thÃªm {len(texts)} documents vá»›i Mixedbread embeddings")
        return len(texts)

    def semantic_search(
        self,
        query: str,
        n_results: int = 5,
        filter_metadata: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """
        Semantic search vá»›i Mixedbread embeddings
        """
        print(f"ğŸ” Äang tÃ¬m kiáº¿m: '{query}'")

        results = self.collection.query(
            query_texts=[query], n_results=n_results, where=filter_metadata
        )

        # Format káº¿t quáº£
        formatted_results = []
        if results["documents"] and results["documents"][0]:
            for i, doc in enumerate(results["documents"][0]):
                formatted_results.append(
                    {
                        "content": doc,
                        "distance": (
                            results["distances"][0][i] if results["distances"] else None
                        ),
                        "metadata": (
                            results["metadatas"][0][i] if results["metadatas"] else None
                        ),
                        "id": results["ids"][0][i] if results["ids"] else None,
                    }
                )

        return formatted_results

    def ingest_text_intelligent(
        self, text: str, metadata: Optional[Dict[str, Any]] = None
    ) -> int:
        """
        Ingest text vá»›i intelligent chunking vÃ  Mixedbread embeddings
        """
        chunks = self.intelligent_text_splitter(text)

        if metadata is None:
            metadata = {"source": "manual", "model": "mixedbread-ai"}

        metadatas = [metadata for _ in chunks]
        ids = [f"{metadata.get('source', 'doc')}_{i}" for i in range(len(chunks))]

        return self.add_documents(chunks, metadatas, ids)

    def compare_models(self, query: str) -> Dict[str, Any]:
        """
        So sÃ¡nh káº¿t quáº£ vá»›i cÃ¡c collection khÃ¡c (náº¿u cÃ³)
        """
        results = {
            "mixedbread": self.semantic_search(query, n_results=3),
            "query": query,
        }

        return results


# === Test vá»›i dá»¯ liá»‡u tiáº¿ng Viá»‡t phong phÃº ===
if __name__ == "__main__":
    print("ğŸ¯ TESTING MIXEDBREAD AI mxbai-embed-large-v1 Vá»šI TIáº¾NG VIá»†T")
    print("=" * 70)

    # Khá»Ÿi táº¡o RAG vá»›i Mixedbread model
    rag = MixedbreadVietnameseRAG(db_path="./mixedbread_chroma_db")

    # Táº¡o dá»¯ liá»‡u test phong phÃº
    vietnamese_documents = {
        "tech_vietnam.txt": """
        CÃ´ng nghá»‡ thÃ´ng tin Viá»‡t Nam Ä‘ang bÃ¹ng ná»• vá»›i nhiá»u Ä‘á»™t phÃ¡ áº¥n tÆ°á»£ng.
        
        CÃ¡c cÃ´ng ty cÃ´ng nghá»‡ hÃ ng Ä‘áº§u:
        - VNG Corporation: Zalo, Zing, game online
        - FPT Software: Pháº§n má»m, AI, digital transformation
        - Tiki: ThÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­, logistics thÃ´ng minh  
        - Grab Vietnam: Super app, giao thÃ´ng vÃ  giao hÃ ng
        - VinSmart: Smartphone, IoT, cÃ´ng nghá»‡ 5G
        
        LÄ©nh vá»±c AI/ML phÃ¡t triá»ƒn máº¡nh:
        Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn tiáº¿ng Viá»‡t, computer vision, 
        chatbot thÃ´ng minh, recommendation systems.
        
        Startup ecosystem sÃ´i Ä‘á»™ng vá»›i nhiá»u quá»¹ Ä‘áº§u tÆ° nhÆ°
        IDG Ventures, 500 Startups, Openspace Ventures.
        """,
        "programming_vietnam.txt": """
        Cá»™ng Ä‘á»“ng láº­p trÃ¬nh Viá»‡t Nam ráº¥t nÄƒng Ä‘á»™ng vÃ  sÃ¡ng táº¡o.
        
        NgÃ´n ngá»¯ láº­p trÃ¬nh phá»• biáº¿n:
        1. JavaScript/TypeScript - Frontend vÃ  Backend development
        2. Python - AI/ML, data science, automation
        3. Java - Enterprise applications, Spring framework
        4. C# - .NET development, Unity game development  
        5. Go - Microservices, cloud native applications
        6. Rust - System programming an toÃ n, performance cao
        
        Framework vÃ  cÃ´ng nghá»‡ hot:
        - React, Vue.js, Angular cho frontend
        - Node.js, Django, FastAPI cho backend
        - TensorFlow, PyTorch cho AI/ML
        - Docker, Kubernetes cho DevOps
        - AWS, Azure, GCP cho cloud computing
        
        Cá»™ng Ä‘á»“ng há»c táº­p: Topcoder, Codeforces, HackerRank
        """,
        "ai_vietnam.txt": """
        TrÃ­ tuá»‡ nhÃ¢n táº¡o táº¡i Viá»‡t Nam Ä‘ang trá»Ÿ thÃ nh xu hÆ°á»›ng chá»§ Ä‘áº¡o.
        
        CÃ¡c lÄ©nh vá»±c AI phÃ¡t triá»ƒn:
        - Natural Language Processing cho tiáº¿ng Viá»‡t
        - Computer Vision cho y táº¿, nÃ´ng nghiá»‡p
        - Recommendation Systems cho e-commerce
        - Chatbot vÃ  Virtual Assistant
        - Predictive Analytics cho tÃ i chÃ­nh
        
        CÃ´ng ty AI hÃ ng Ä‘áº§u:
        VinAI Research, FPT.AI, Zalo AI, VCC.ai, Anfin.
        
        á»¨ng dá»¥ng thá»±c táº¿:
        PhÃ¢n tÃ­ch X-quang tá»± Ä‘á»™ng, dá»± Ä‘oÃ¡n thá»i tiáº¿t,
        tá»‘i Æ°u hÃ³a giao thÃ´ng, phÃ¢n tÃ­ch tÃ¬nh cáº£m social media,
        chatbot chÄƒm sÃ³c khÃ¡ch hÃ ng Ä‘a ngÃ´n ngá»¯.
        
        ThÃ¡ch thá»©c: Thiáº¿u dá»¯ liá»‡u tiáº¿ng Viá»‡t cháº¥t lÆ°á»£ng,
        cáº§n Ä‘áº§u tÆ° nghiÃªn cá»©u fundamental AI research.
        """,
    }

    # Táº¡o thÆ° má»¥c docs
    os.makedirs("./docs", exist_ok=True)

    # Ghi vÃ  ingest documents
    total_chunks = 0
    for filename, content in vietnamese_documents.items():
        filepath = f"./docs/{filename}"
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content.strip())
        print(f"ğŸ“„ ÄÃ£ táº¡o file: {filepath}")

        # Ingest vá»›i metadata
        n = rag.ingest_text_intelligent(
            content,
            metadata={
                "source": filename,
                "lang": "vi",
                "topic": filename.split("_")[0],
            },
        )
        total_chunks += n
        print(f"âœ… ÄÃ£ ingest {n} chunks tá»« {filename}")

    print(f"\nğŸ“Š Tá»•ng cá»™ng Ä‘Ã£ ingest {total_chunks} chunks vá»›i Mixedbread embeddings")

    # Test queries cháº¥t lÆ°á»£ng cao
    test_queries = [
        "Startup cÃ´ng nghá»‡ nÃ o thÃ nh cÃ´ng á»Ÿ Viá»‡t Nam?",
        "NgÃ´n ngá»¯ láº­p trÃ¬nh nÃ o phá»• biáº¿n cho AI/ML?",
        "VinAI Research lÃ m gÃ¬ trong lÄ©nh vá»±c AI?",
        "Framework JavaScript nÃ o tá»‘t cho frontend?",
        "ThÃ¡ch thá»©c cá»§a AI Viá»‡t Nam lÃ  gÃ¬?",
        "CÃ´ng ty nÃ o phÃ¡t triá»ƒn Zalo?",
        "Docker vÃ  Kubernetes dÃ¹ng Ä‘á»ƒ lÃ m gÃ¬?",
        "á»¨ng dá»¥ng AI trong y táº¿ Viá»‡t Nam",
    ]

    print("\n" + "=" * 70)
    print("ğŸ§ª TESTING SEMANTIC SEARCH Vá»šI MIXEDBREAD AI")
    print("=" * 70)

    for i, query in enumerate(test_queries, 1):
        print(f"\n[Q{i}] {query}")
        print("-" * 60)

        try:
            results = rag.semantic_search(query, n_results=2)

            if results:
                for j, result in enumerate(results, 1):
                    distance = result["distance"]
                    content = (
                        result["content"][:200] + "..."
                        if len(result["content"]) > 200
                        else result["content"]
                    )
                    source = (
                        result["metadata"].get("source", "unknown")
                        if result["metadata"]
                        else "unknown"
                    )
                    topic = (
                        result["metadata"].get("topic", "general")
                        if result["metadata"]
                        else "general"
                    )

                    print(f"  [{j}] ğŸ¯ Distance: {distance:.3f} | Topic: {topic}")
                    print(f"      ğŸ“„ Source: {source}")
                    print(f"      ğŸ“ Content: {content}")
                    print()
            else:
                print("  âŒ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ phÃ¹ há»£p")

        except Exception as e:
            print(f"  âŒ Lá»—i: {e}")

        print()

    print("=" * 70)
    print("âœ… HOÃ€N THÃ€NH TEST MIXEDBREAD AI RAG PIPELINE")
    print("=" * 70)

    # Thá»‘ng kÃª collection
    collection_info = rag.collection.count()
    print(f"ğŸ“Š Collection stats: {collection_info} documents total")
