# Generated by Copilot
# === RAG Pipeline cáº£i thiá»‡n vá»›i mixedbread-ai/mxbai-embed-large-v1 ===
# Thá»­ nghiá»‡m mÃ´ hÃ¬nh embedding cháº¥t lÆ°á»£ng cao Mixedbread AI cho tiáº¿ng Viá»‡t

import os
import chromadb
from typing import List, Dict, Any, Optional
from chromadb.types import Metadata
from sentence_transformers import SentenceTransformer
import numpy as np


def setup_huggingface_token():
    """
    Setup Hugging Face token tá»« nhiá»u nguá»“n khÃ¡c nhau
    """
    # Thá»­ nhiá»u cÃ¡ch Ä‘á»ƒ láº¥y HF_TOKEN
    token_sources = [
        os.getenv("HF_TOKEN"),
        os.getenv("HUGGINGFACE_TOKEN"),
        os.getenv("HF_TOKEN", ""),  # fallback empty
    ]

    # Thá»­ Ä‘á»c tá»« file .env náº¿u cÃ³
    try:
        if os.path.exists(".env"):
            with open(".env", "r") as f:
                for line in f:
                    if line.strip().startswith("HF_TOKEN="):
                        token_sources.append(line.strip().split("=", 1)[1])
    except:
        pass

    # Thá»­ Ä‘á»c tá»« file token.txt náº¿u cÃ³
    try:
        if os.path.exists("token.txt"):
            with open("token.txt", "r") as f:
                token_sources.append(f.read().strip())
    except:
        pass

    # TÃ¬m token Ä‘áº§u tiÃªn khÃ´ng rá»—ng
    for token in token_sources:
        if token and token.strip() and not token.startswith("hf_example"):
            print(f"âœ… TÃ¬m tháº¥y HF_TOKEN: {token[:10]}...")
            # Set vÃ o environment Ä‘á»ƒ sentence-transformers sá»­ dá»¥ng
            os.environ["HF_TOKEN"] = token.strip()
            os.environ["HUGGINGFACE_HUB_TOKEN"] = token.strip()
            return token.strip()

    print("âŒ KhÃ´ng tÃ¬m tháº¥y HF_TOKEN há»£p lá»‡")
    return None


class MixedbreadVietnameseRAG:
    """
    RAG vá»›i mÃ´ hÃ¬nh Mixedbread AI mxbai-embed-large-v1 - cháº¥t lÆ°á»£ng cao cho multilingual
    """

    def __init__(self, db_path: str = "./mixedbread_chroma_db"):
        """
        Initialize vá»›i mixedbread-ai/mxbai-embed-large-v1
        """
        print("ğŸš€ Khá»Ÿi táº¡o Mixedbread Vietnamese RAG...")

        # Setup HF_TOKEN trÆ°á»›c khi táº£i model
        token = setup_huggingface_token()
        if not token:
            print("âš ï¸ KhÃ´ng cÃ³ HF_TOKEN - má»™t sá»‘ model cÃ³ thá»ƒ khÃ´ng táº£i Ä‘Æ°á»£c")

        print("ğŸ”„ Äang táº£i mÃ´ hÃ¬nh Mixedbread AI mxbai-embed-large-v1...")

        try:
            # Thá»­ load Mixedbread AI model trÆ°á»›c
            print("ğŸ”„ Äang thá»­ táº£i mÃ´ hÃ¬nh Mixedbread AI...")
            self.model = SentenceTransformer("mixedbread-ai/mxbai-embed-large-v1")
            self.model_name = "mixedbread-ai/mxbai-embed-large-v1"
            print("âœ… ÄÃ£ táº£i thÃ nh cÃ´ng mÃ´ hÃ¬nh Mixedbread AI!")
        except Exception as e:
            print(f"âŒ Lá»—i táº£i Mixedbread model: {e}")
            print("ğŸ”„ Thá»­ cÃ¡c mÃ´ hÃ¬nh thay tháº¿...")

            # List cÃ¡c models thay tháº¿ khÃ´ng cáº§n auth
            alternative_models = [
                "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
                "sentence-transformers/distiluse-base-multilingual-cased",
                "sentence-transformers/all-MiniLM-L12-v2",
                "sentence-transformers/all-mpnet-base-v2",
            ]

            self.model = None
            self.model_name = "chromadb-default"

            for model_name in alternative_models:
                try:
                    print(f"ğŸ”„ Äang thá»­ táº£i: {model_name}")
                    self.model = SentenceTransformer(model_name)
                    self.model_name = model_name
                    print(f"âœ… ÄÃ£ táº£i thÃ nh cÃ´ng: {model_name}")
                    break
                except Exception as e2:
                    print(f"âŒ Lá»—i {model_name}: {e2}")
                    continue

            if not self.model:
                print("âŒ KhÃ´ng thá»ƒ táº£i báº¥t ká»³ sentence-transformers model nÃ o")
                print("ğŸ”„ Sá»­ dá»¥ng ChromaDB default embedding...")
                self.model = None
                self.model_name = "chromadb-default"

        # Initialize ChromaDB client
        self.client = chromadb.PersistentClient(path=db_path)

        # Táº¡o collection
        try:
            self.collection = self.client.get_collection("mixedbread_vietnamese")
            print("ğŸ“‚ ÄÃ£ káº¿t ná»‘i collection 'mixedbread_vietnamese'")
        except:
            self.collection = self.client.create_collection(
                name="mixedbread_vietnamese",
                metadata={
                    "model": self.model_name,
                    "language": "multilingual",
                },
            )
            print("ğŸ“‚ ÄÃ£ táº¡o collection má»›i 'mixedbread_vietnamese'")

    def intelligent_text_splitter(
        self, text: str, chunk_size: int = 400, overlap: int = 50
    ) -> List[str]:
        """
        Text splitter thÃ´ng minh cho tiáº¿ng Viá»‡t vá»›i Mixedbread model
        """
        # TÃ¡ch theo Ä‘oáº¡n vÄƒn trÆ°á»›c
        paragraphs = [p.strip() for p in text.split("\\n\\n") if p.strip()]

        chunks = []
        current_chunk = ""

        for para in paragraphs:
            # Náº¿u Ä‘oáº¡n vÄƒn quÃ¡ dÃ i, chia nhá» hÆ¡n
            if len(para) > chunk_size:
                # TÃ¡ch theo cÃ¢u
                sentences = []
                temp_sentence = ""

                for char in para:
                    temp_sentence += char
                    if char in ".!?":
                        # Kiá»ƒm tra khÃ´ng pháº£i sá»‘ tháº­p phÃ¢n
                        next_chars = para[
                            para.index(temp_sentence)
                            + len(temp_sentence) : para.index(temp_sentence)
                            + len(temp_sentence)
                            + 3
                        ]
                        if not any(c.isdigit() for c in next_chars):
                            sentences.append(temp_sentence.strip())
                            temp_sentence = ""

                # ThÃªm cÃ¢u cuá»‘i
                if temp_sentence.strip():
                    sentences.append(temp_sentence.strip())

                # Combine sentences to chunks
                for sentence in sentences:
                    if len(current_chunk) + len(sentence) <= chunk_size:
                        current_chunk += " " + sentence
                    else:
                        if current_chunk.strip():
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence
            else:
                # Äoáº¡n vÄƒn ngáº¯n, thÃªm trá»±c tiáº¿p
                if len(current_chunk) + len(para) <= chunk_size:
                    current_chunk += "\\n" + para
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    current_chunk = para

        # ThÃªm chunk cuá»‘i
        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return [chunk for chunk in chunks if len(chunk.strip()) > 20]

    def add_documents(
        self,
        texts: List[str],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None,
    ):
        """
        ğŸ”¥ ThÃªm documents vá»›i custom embeddings (FIX chÃ­nh)
        """
        if ids is None:
            ids = [f"mixedbread_doc_{i}" for i in range(len(texts))]

        if metadatas is None:
            metadatas = [{"source": "unknown", "model": self.model_name} for _ in texts]

        # Cast to compatible type for ChromaDB
        metadata_list: List[Metadata] = []
        for meta in metadatas:
            # Ensure all values are of correct types for ChromaDB Metadata
            clean_meta: Metadata = {}
            for k, v in meta.items():
                if isinstance(v, (str, int, float, bool)) or v is None:
                    clean_meta[k] = v
                else:
                    clean_meta[k] = str(v)  # Convert to string if not basic type
            metadata_list.append(clean_meta)

        # ğŸ”¥ Táº¡o embeddings vá»›i model Ä‘Ã£ load (thay vÃ¬ Ä‘á»ƒ ChromaDB tá»± táº¡o)
        if self.model:
            print(f"ğŸ§  Táº¡o embeddings vá»›i {self.model_name}...")
            embeddings = self.model.encode(texts, normalize_embeddings=True)
            # Convert numpy array to list for ChromaDB
            embeddings_list = embeddings.tolist()

            self.collection.add(
                documents=texts,
                metadatas=metadata_list,
                ids=ids,
                embeddings=embeddings_list,  # ğŸ¯ Sá»­ dá»¥ng custom embeddings
            )
            print(f"ğŸ“ ÄÃ£ thÃªm {len(texts)} documents vá»›i {self.model_name} embeddings")
        else:
            # Fallback: ChromaDB default embedding
            self.collection.add(documents=texts, metadatas=metadata_list, ids=ids)
            print(f"ğŸ“ ÄÃ£ thÃªm {len(texts)} documents vá»›i ChromaDB default embeddings")

        return len(texts)

    def semantic_search(
        self,
        query: str,
        n_results: int = 5,
        filter_metadata: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """
        ğŸ”¥ Semantic search vá»›i custom query embeddings (FIX chÃ­nh)
        """
        print(f"ğŸ” Äang tÃ¬m kiáº¿m: '{query}'")

        # ğŸ”¥ Táº¡o query embedding vá»›i model Ä‘Ã£ load
        if self.model:
            print(f"ğŸ§  Táº¡o query embedding vá»›i {self.model_name}...")
            query_embedding = self.model.encode([query], normalize_embeddings=True)
            query_embedding_list = query_embedding.tolist()

            results = self.collection.query(
                query_embeddings=query_embedding_list,  # ğŸ¯ Sá»­ dá»¥ng custom query embedding
                n_results=n_results,
                where=filter_metadata,
            )
        else:
            # Fallback: ChromaDB default
            results = self.collection.query(
                query_texts=[query], n_results=n_results, where=filter_metadata
            )  # Format káº¿t quáº£ Ä‘áº¹p
        formatted_results = []
        if results and results["documents"] and results["documents"][0]:
            for i in range(len(results["documents"][0])):
                result = {
                    "content": results["documents"][0][i],
                    "metadata": (
                        results["metadatas"][0][i]
                        if results["metadatas"] and results["metadatas"][0]
                        else {}
                    ),
                    "distance": (
                        results["distances"][0][i]
                        if results["distances"] and results["distances"][0]
                        else 1.0
                    ),
                    "id": (
                        results["ids"][0][i]
                        if results["ids"] and results["ids"][0]
                        else f"doc_{i}"
                    ),
                }
                formatted_results.append(result)

        return formatted_results

    def ingest_file(self, file_path: str, topic: str = "general") -> int:
        """
        Äá»c file vÃ  chunk thÃ nh documents
        """
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            chunks = self.intelligent_text_splitter(content)
            metadatas = [
                {
                    "source": os.path.basename(file_path),
                    "topic": topic,
                    "model": self.model_name,
                }
                for _ in chunks
            ]
            ids = [f"{topic}_{i}" for i in range(len(chunks))]

            return self.add_documents(chunks, metadatas, ids)
        except Exception as e:
            print(f"âŒ Lá»—i Ä‘á»c file {file_path}: {e}")
            return 0

    def compare_embeddings(self, text: str) -> Dict[str, Any]:
        """
        So sÃ¡nh cháº¥t lÆ°á»£ng embedding giá»¯a cÃ¡c models
        """
        if not self.model:
            return {"error": "KhÃ´ng cÃ³ model Ä‘á»ƒ so sÃ¡nh"}

        embedding = self.model.encode([text], normalize_embeddings=True)

        return {
            "model": self.model_name,
            "text": text[:100] + "..." if len(text) > 100 else text,
            "embedding_dim": len(embedding[0]),
            "embedding_norm": float(np.linalg.norm(embedding[0])),
            "sample_values": embedding[0][:5].tolist(),  # 5 giÃ¡ trá»‹ Ä‘áº§u
        }

    def get_collection_stats(self) -> Dict[str, Any]:
        """
        Thá»‘ng kÃª collection
        """
        try:
            collection_data = self.collection.get()
            docs_count = (
                len(collection_data["documents"]) if collection_data["documents"] else 0
            )
            return {
                "total_documents": docs_count,
                "model_used": self.model_name,
                "collection_name": self.collection.name,
                "sample_metadata": (
                    collection_data["metadatas"][:3]
                    if collection_data["metadatas"]
                    else []
                ),
            }
        except Exception as e:
            return {"error": f"Lá»—i láº¥y stats: {e}"}


def create_test_documents():
    """
    Táº¡o tÃ i liá»‡u test tiáº¿ng Viá»‡t (tÃ¡ch riÃªng khá»i class)
    """
    os.makedirs("./docs", exist_ok=True)

    # Tech Vietnam
    tech_content = """CÃ´ng nghá»‡ thÃ´ng tin Viá»‡t Nam Ä‘ang bÃ¹ng ná»• vá»›i nhiá»u Ä‘á»™t phÃ¡ áº¥n tÆ°á»£ng. CÃ¡c cÃ´ng ty cÃ´ng nghá»‡ hÃ ng Ä‘áº§u:
        - VNG Corporation: Zalo, Zing, game online
        - FPT Software: Pháº§n má»m, AI, digital transformation  
        - Tiki: ThÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­
        - Shopee Viá»‡t Nam: E-commerce platform
        
CÃ¡c startup unicorn:
        - VNPay: Fintech, thanh toÃ¡n Ä‘iá»‡n tá»­
        - VinFast: Xe Ä‘iá»‡n thÃ´ng minh
        - Be Group: Ride-hailing, logistics
        
Xu hÆ°á»›ng cÃ´ng nghá»‡ hot:
        - AI/Machine Learning cho tiáº¿ng Viá»‡t
        - Blockchain vÃ  Web3
        - IoT cho smart city
        - Cloud computing vá»›i AWS, Azure
        - DevOps vÃ  microservices architecture"""

    with open("./docs/tech_vietnam.txt", "w", encoding="utf-8") as f:
        f.write(tech_content)
    print("ğŸ“„ ÄÃ£ táº¡o file: ./docs/tech_vietnam.txt")

    # Programming Vietnam
    programming_content = """Cá»™ng Ä‘á»“ng láº­p trÃ¬nh Viá»‡t Nam ráº¥t nÄƒng Ä‘á»™ng vÃ  sÃ¡ng táº¡o. NgÃ´n ngá»¯ láº­p trÃ¬nh phá»• biáº¿n:
        1. JavaScript/TypeScript - Frontend vÃ  Backend development
        2. Python - AI/ML, data science, automation
        3. Java - Enterprise applications, Android
        4. C# - .NET ecosystem, game development
        5. Go - Microservices, system programming
        6. Rust - System programming, performance critical
        
Framework vÃ  cÃ´ng cá»¥:
        - React, Vue.js, Angular cho frontend
        - Node.js, Django, FastAPI cho backend
        - TensorFlow, PyTorch cho AI/ML
        - Docker, Kubernetes cho DevOps
        - AWS, Azure, GCP cho cloud computing
        
Coding bootcamp ná»•i tiáº¿ng:
        - Techmaster: Full-stack development
        - CodeGym: Java programming
        - MindX: AI vÃ  Data Science
        - FUNiX: ÄÃ o táº¡o láº­p trÃ¬nh online"""

    with open("./docs/programming_vietnam.txt", "w", encoding="utf-8") as f:
        f.write(programming_content)
    print("ğŸ“„ ÄÃ£ táº¡o file: ./docs/programming_vietnam.txt")

    # AI Vietnam
    ai_content = """TrÃ­ tuá»‡ nhÃ¢n táº¡o táº¡i Viá»‡t Nam Ä‘ang trá»Ÿ thÃ nh xu hÆ°á»›ng chá»§ Ä‘áº¡o. CÃ¡c lÄ©nh vá»±c AI phÃ¡t triá»ƒn:
        - Natural Language Processing cho tiáº¿ng Viá»‡t
        - Computer Vision cho y táº¿, nÃ´ng nghiá»‡p
        - Recommendation systems cho e-commerce
        - Fraud detection cho fintech
        - Chatbot vÃ  virtual assistant
        
CÃ¡c tá»• chá»©c nghiÃªn cá»©u AI hÃ ng Ä‘áº§u: VinAI Research, FPT.AI, Zalo AI, VCC.ai, Anfin. á»¨ng dá»¥ng thá»±c táº¿:
        PhÃ¢n tÃ­ch X-quang tá»± Ä‘á»™ng, dá»± Ä‘oÃ¡n thá»i tiáº¿t,
        tá»‘i Æ°u hÃ³a giao thÃ´ng, phÃ¢n tÃ­ch tÃ¬nh cáº£m social media,
        recommendation engine cho shopping, fraud detection banking."""

    with open("./docs/ai_vietnam.txt", "w", encoding="utf-8") as f:
        f.write(ai_content)
    print("ğŸ“„ ÄÃ£ táº¡o file: ./docs/ai_vietnam.txt")


def run_test():
    """
    ğŸ”¥ Cháº¡y test RAG pipeline (tÃ¡ch riÃªng)
    """
    print("ğŸ¯ TESTING MIXEDBREAD AI mxbai-embed-large-v1 Vá»šI TIáº¾NG VIá»†T")
    print("=" * 70)

    # Initialize RAG
    rag = MixedbreadVietnameseRAG()

    # Táº¡o test documents
    create_test_documents()

    # Ingest documents
    total_chunks = 0
    for file_name, topic in [
        ("tech_vietnam.txt", "tech"),
        ("programming_vietnam.txt", "programming"),
        ("ai_vietnam.txt", "ai"),
    ]:
        chunks = rag.ingest_file(f"./docs/{file_name}", topic)
        total_chunks += chunks
        print(f"âœ… ÄÃ£ ingest {chunks} chunks tá»« {file_name}")

    print(
        f"ğŸ“Š Tá»•ng cá»™ng Ä‘Ã£ ingest {total_chunks} chunks vá»›i {rag.model_name} embeddings"
    )

    # Test queries
    print("=" * 70)
    print("ğŸ§ª TESTING SEMANTIC SEARCH Vá»šI MIXEDBREAD AI")
    print("=" * 70)

    test_queries = [
        "Startup cÃ´ng nghá»‡ nÃ o thÃ nh cÃ´ng á»Ÿ Viá»‡t Nam?",
        "NgÃ´n ngá»¯ láº­p trÃ¬nh nÃ o phá»• biáº¿n cho AI/ML?",
        "VinAI Research lÃ m gÃ¬ trong lÄ©nh vá»±c AI?",
        "Framework JavaScript nÃ o tá»‘t cho frontend?",
        "ThÃ¡ch thá»©c cá»§a AI Viá»‡t Nam lÃ  gÃ¬?",
        "CÃ´ng ty nÃ o phÃ¡t triá»ƒn Zalo?",
        "Docker vÃ  Kubernetes dÃ¹ng Ä‘á»ƒ lÃ m gÃ¬?",
        "á»¨ng dá»¥ng AI trong y táº¿ Viá»‡t Nam",
    ]

    for i, query in enumerate(test_queries, 1):
        print(f"[Q{i}] {query}")
        print("-" * 60)

        results = rag.semantic_search(query, n_results=2)
        for j, result in enumerate(results, 1):
            topic = result["metadata"].get("topic", "unknown")
            source = result["metadata"].get("source", "unknown")
            content = (
                result["content"][:150] + "..."
                if len(result["content"]) > 150
                else result["content"]
            )

            print(f"  [{j}] ğŸ¯ Distance: {result['distance']:.3f} | Topic: {topic}")
            print(f"      ğŸ“„ Source: {source}")
            print(f"      ğŸ“ Content: {content}")

    # Stats
    print("=" * 70)
    print("âœ… HOÃ€N THÃ€NH TEST MIXEDBREAD AI RAG PIPELINE")
    print("=" * 70)
    stats = rag.get_collection_stats()
    print(f"ğŸ“Š Collection stats: {stats['total_documents']} documents total")

    # Compare embeddings
    if rag.model:
        print("\\nğŸ” So sÃ¡nh cháº¥t lÆ°á»£ng embedding:")
        comparison = rag.compare_embeddings("CÃ´ng nghá»‡ AI Viá»‡t Nam phÃ¡t triá»ƒn máº¡nh")
        print(f"Model: {comparison['model']}")
        print(f"Embedding dimension: {comparison['embedding_dim']}")
        print(f"Norm: {comparison['embedding_norm']:.4f}")


if __name__ == "__main__":
    run_test()
