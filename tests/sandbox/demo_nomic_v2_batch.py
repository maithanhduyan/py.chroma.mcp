# Generated by Copilot
"""
Test batch processing với Nomic Embed Text v2 MoE model
Tối ưu hóa hiệu suất cho xử lý lô lớn và đo lường performance
"""

from sentence_transformers import SentenceTransformer
import time
import numpy as np
from typing import List, Dict, Any
import gc
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Handle optional dependencies gracefully
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    class MockPsutil:
        @staticmethod
        def cpu_count():
            return "N/A"
        @staticmethod  
        def virtual_memory():
            class MockMemory:
                total = 0
            return MockMemory()
        @staticmethod
        def Process(pid):
            class MockProcess:
                def memory_info(self):
                    class MockMemInfo:
                        rss = 0
                        vms = 0
                    return MockMemInfo()
                def memory_percent(self):
                    return 0.0
            return MockProcess()
    
    psutil = MockPsutil()
    HAS_PSUTIL = False
    print("⚠️  psutil not available, memory monitoring disabled")


def get_memory_usage() -> Dict[str, float]:
    """Lấy thông tin sử dụng bộ nhớ hiện tại"""
    try:
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,  # Resident Set Size
            'vms_mb': memory_info.vms / 1024 / 1024,  # Virtual Memory Size
            'percent': process.memory_percent()
        }
    except Exception:
        return {'rss_mb': 0.0, 'vms_mb': 0.0, 'percent': 0.0}


def create_test_datasets() -> Dict[str, List[str]]:
    """Tạo các dataset test với kích thước khác nhau"""
    base_sentences = [
        "Hello, how are you today?",
        "¡Hola! ¿Cómo estás?", 
        "Xin chào! Bạn có khỏe không?",
        "Machine learning is revolutionizing technology.",
        "La inteligencia artificial cambiará el mundo.",
        "Trí tuệ nhân tạo sẽ thay đổi tương lai.",
        "Natural language processing enables better communication.",
        "El procesamiento de lenguaje natural mejora la comunicación.",
        "Xử lý ngôn ngữ tự nhiên giúp giao tiếp hiệu quả hơn.",
        "Deep learning models require significant computational resources.",
        "Los modelos de aprendizaje profundo requieren muchos recursos.",
        "Các mô hình học sâu cần tài nguyên tính toán lớn."
    ]
    
    return {
        'small': base_sentences[:5],  # 5 câu
        'medium': base_sentences * 5,  # 60 câu  
        'large': base_sentences * 20,  # 240 câu
        'xlarge': base_sentences * 50   # 600 câu
    }


def batch_encode_sequential(
    model: SentenceTransformer, 
    sentences: List[str], 
    batch_size: int = 32,
    prompt_name: str = "passage"
) -> Dict[str, Any]:
    """Xử lý batch tuần tự với monitoring"""
    start_time = time.time()
    start_memory = get_memory_usage()
    
    # Chia thành các batch nhỏ
    batches = [sentences[i:i + batch_size] for i in range(0, len(sentences), batch_size)]
    all_embeddings = []
    
    print(f"🔄 Xử lý {len(sentences)} câu trong {len(batches)} batches (size={batch_size})")
    
    for i, batch in enumerate(batches):
        batch_start = time.time()
        embeddings = model.encode(batch, prompt_name=prompt_name)
        all_embeddings.append(embeddings)
        
        batch_time = time.time() - batch_start
        print(f"   Batch {i+1}/{len(batches)}: {len(batch)} câu - {batch_time:.3f}s")
    
    # Kết hợp tất cả embeddings
    final_embeddings = np.vstack(all_embeddings)
    
    end_time = time.time()
    end_memory = get_memory_usage()
    
    return {
        'embeddings': final_embeddings,
        'total_time': end_time - start_time,
        'sentences_per_second': len(sentences) / (end_time - start_time),
        'memory_used_mb': end_memory['rss_mb'] - start_memory['rss_mb'],
        'batch_count': len(batches),
        'batch_size': batch_size
    }


def batch_encode_optimized(
    model: SentenceTransformer,
    sentences: List[str],
    batch_size: int = 64,
    prompt_name: str = "passage",
    show_progress: bool = True
) -> Dict[str, Any]:
    """Xử lý batch tối ưu hóa với memory management"""
    start_time = time.time()
    start_memory = get_memory_usage()
    
    print(f"🚀 Tối ưu hóa batch processing cho {len(sentences)} câu...")
    
    # Encode toàn bộ với batch size lớn hơn
    embeddings = model.encode(
        sentences, 
        prompt_name=prompt_name,
        batch_size=batch_size,
        show_progress_bar=show_progress,
        convert_to_numpy=True
    )
    
    # Force garbage collection
    gc.collect()
    
    end_time = time.time()
    end_memory = get_memory_usage()
    
    return {
        'embeddings': embeddings,
        'total_time': end_time - start_time,
        'sentences_per_second': len(sentences) / (end_time - start_time),
        'memory_used_mb': end_memory['rss_mb'] - start_memory['rss_mb'],
        'batch_size': batch_size
    }


def compare_batch_strategies(
    model: SentenceTransformer,
    sentences: List[str]
) -> Dict[str, Dict[str, Any]]:
    """So sánh các chiến lược batch khác nhau"""
    print(f"\n📊 So sánh chiến lược batch cho {len(sentences)} câu:")
    print("=" * 60)
    
    strategies = {
        'sequential_32': lambda: batch_encode_sequential(model, sentences, 32),
        'sequential_64': lambda: batch_encode_sequential(model, sentences, 64),
        'optimized_64': lambda: batch_encode_optimized(model, sentences, 64),
        'optimized_128': lambda: batch_encode_optimized(model, sentences, 128)
    }
    
    results = {}
    
    for strategy_name, strategy_func in strategies.items():
        print(f"\n🔍 Testing {strategy_name}...")
        try:
            results[strategy_name] = strategy_func()
            
            result = results[strategy_name]
            print(f"✅ {strategy_name}:")
            print(f"   Thời gian: {result['total_time']:.2f}s")
            print(f"   Tốc độ: {result['sentences_per_second']:.1f} câu/s")
            print(f"   Bộ nhớ: {result['memory_used_mb']:.1f} MB")
            
        except Exception as e:
            print(f"❌ {strategy_name} thất bại: {e}")
            results[strategy_name] = {'error': str(e)}
    
    return results


def performance_benchmark(model: SentenceTransformer) -> None:
    """Chạy benchmark toàn diện"""
    print("🏁 BẮT ĐẦU PERFORMANCE BENCHMARK")
    print("=" * 70)
    
    datasets = create_test_datasets()
    
    for dataset_name, sentences in datasets.items():
        print(f"\n📈 Dataset: {dataset_name.upper()} ({len(sentences)} câu)")
        print("-" * 50)
        
        # Chỉ test strategy tốt nhất cho dataset lớn
        if len(sentences) > 100:
            result = batch_encode_optimized(model, sentences, batch_size=128)
            print(f"✅ Optimized batch (128):")
            print(f"   Thời gian: {result['total_time']:.2f}s") 
            print(f"   Tốc độ: {result['sentences_per_second']:.1f} câu/s")
            print(f"   Bộ nhớ: {result['memory_used_mb']:.1f} MB")
        else:
            # Test đầy đủ cho dataset nhỏ
            compare_batch_strategies(model, sentences)


def analyze_embeddings(embeddings: np.ndarray, sentences: List[str]) -> None:
    """Phân tích chất lượng embeddings"""
    print(f"\n🔬 PHÂN TÍCH EMBEDDINGS")
    print("=" * 40)
    
    print(f"📊 Thống kê cơ bản:")
    print(f"   Shape: {embeddings.shape}")
    print(f"   Dtype: {embeddings.dtype}")
    print(f"   Min: {embeddings.min():.6f}")
    print(f"   Max: {embeddings.max():.6f}")
    print(f"   Mean: {embeddings.mean():.6f}")
    print(f"   Std: {embeddings.std():.6f}")
    
    # Tính similarity matrix cho sample
    if len(sentences) <= 10:
        from sklearn.metrics.pairwise import cosine_similarity
        sim_matrix = cosine_similarity(embeddings)
        
        print(f"\n🤝 Cosine Similarity Matrix:")
        for i in range(min(5, len(sentences))):
            for j in range(min(5, len(sentences))):
                print(f"{sim_matrix[i][j]:.3f}", end=" ")
            print()


def main():
    """Main function để chạy tất cả tests"""
    print("🚀 NOMIC EMBED TEXT V2 MoE - BATCH PERFORMANCE TEST")
    print("=" * 70)    # Hiển thị thông tin hệ thống
    print(f"💻 System Info:")
    print(f"   CPU cores: {psutil.cpu_count()}")
    if HAS_PSUTIL:
        print(f"   Memory: {psutil.virtual_memory().total / 1024**3:.1f} GB")
    else:
        print(f"   Memory: N/A (psutil not available)")
    print(f"   Python PID: {os.getpid()}")
    
    # Load model
    print(f"\n🤖 Loading model...")
    load_start = time.time()
    
    model = SentenceTransformer(
        "nomic-ai/nomic-embed-text-v2-moe", 
        trust_remote_code=True
    )
    
    load_time = time.time() - load_start
    print(f"✅ Model loaded in {load_time:.2f}s")
    
    # Chạy performance benchmark
    performance_benchmark(model)
    
    # Test với dataset medium để phân tích embeddings
    print(f"\n🔬 Embedding analysis on medium dataset...")
    datasets = create_test_datasets()
    result = batch_encode_optimized(model, datasets['medium'][:10])
    analyze_embeddings(result['embeddings'], datasets['medium'][:10])
    
    print(f"\n🎉 BENCHMARK HOÀN THÀNH!")


if __name__ == "__main__":
    main()
