# Generated by Copilot
"""
Test batch processing vá»›i Nomic Embed Text v2 MoE model
Tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t cho xá»­ lÃ½ lÃ´ lá»›n vÃ  Ä‘o lÆ°á»ng performance
"""

from sentence_transformers import SentenceTransformer
import time
import numpy as np
from typing import List, Dict, Any
import gc
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Handle optional dependencies gracefully
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    class MockPsutil:
        @staticmethod
        def cpu_count():
            return "N/A"
        @staticmethod  
        def virtual_memory():
            class MockMemory:
                total = 0
            return MockMemory()
        @staticmethod
        def Process(pid):
            class MockProcess:
                def memory_info(self):
                    class MockMemInfo:
                        rss = 0
                        vms = 0
                    return MockMemInfo()
                def memory_percent(self):
                    return 0.0
            return MockProcess()
    
    psutil = MockPsutil()
    HAS_PSUTIL = False
    print("âš ï¸  psutil not available, memory monitoring disabled")


def get_memory_usage() -> Dict[str, float]:
    """Láº¥y thÃ´ng tin sá»­ dá»¥ng bá»™ nhá»› hiá»‡n táº¡i"""
    try:
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,  # Resident Set Size
            'vms_mb': memory_info.vms / 1024 / 1024,  # Virtual Memory Size
            'percent': process.memory_percent()
        }
    except Exception:
        return {'rss_mb': 0.0, 'vms_mb': 0.0, 'percent': 0.0}


def create_test_datasets() -> Dict[str, List[str]]:
    """Táº¡o cÃ¡c dataset test vá»›i kÃ­ch thÆ°á»›c khÃ¡c nhau"""
    base_sentences = [
        "Hello, how are you today?",
        "Â¡Hola! Â¿CÃ³mo estÃ¡s?", 
        "Xin chÃ o! Báº¡n cÃ³ khá»e khÃ´ng?",
        "Machine learning is revolutionizing technology.",
        "La inteligencia artificial cambiarÃ¡ el mundo.",
        "TrÃ­ tuá»‡ nhÃ¢n táº¡o sáº½ thay Ä‘á»•i tÆ°Æ¡ng lai.",
        "Natural language processing enables better communication.",
        "El procesamiento de lenguaje natural mejora la comunicaciÃ³n.",
        "Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn giÃºp giao tiáº¿p hiá»‡u quáº£ hÆ¡n.",
        "Deep learning models require significant computational resources.",
        "Los modelos de aprendizaje profundo requieren muchos recursos.",
        "CÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u cáº§n tÃ i nguyÃªn tÃ­nh toÃ¡n lá»›n."
    ]
    
    return {
        'small': base_sentences[:5],  # 5 cÃ¢u
        'medium': base_sentences * 5,  # 60 cÃ¢u  
        'large': base_sentences * 20,  # 240 cÃ¢u
        'xlarge': base_sentences * 50   # 600 cÃ¢u
    }


def batch_encode_sequential(
    model: SentenceTransformer, 
    sentences: List[str], 
    batch_size: int = 32,
    prompt_name: str = "passage"
) -> Dict[str, Any]:
    """Xá»­ lÃ½ batch tuáº§n tá»± vá»›i monitoring"""
    start_time = time.time()
    start_memory = get_memory_usage()
    
    # Chia thÃ nh cÃ¡c batch nhá»
    batches = [sentences[i:i + batch_size] for i in range(0, len(sentences), batch_size)]
    all_embeddings = []
    
    print(f"ğŸ”„ Xá»­ lÃ½ {len(sentences)} cÃ¢u trong {len(batches)} batches (size={batch_size})")
    
    for i, batch in enumerate(batches):
        batch_start = time.time()
        embeddings = model.encode(batch, prompt_name=prompt_name)
        all_embeddings.append(embeddings)
        
        batch_time = time.time() - batch_start
        print(f"   Batch {i+1}/{len(batches)}: {len(batch)} cÃ¢u - {batch_time:.3f}s")
    
    # Káº¿t há»£p táº¥t cáº£ embeddings
    final_embeddings = np.vstack(all_embeddings)
    
    end_time = time.time()
    end_memory = get_memory_usage()
    
    return {
        'embeddings': final_embeddings,
        'total_time': end_time - start_time,
        'sentences_per_second': len(sentences) / (end_time - start_time),
        'memory_used_mb': end_memory['rss_mb'] - start_memory['rss_mb'],
        'batch_count': len(batches),
        'batch_size': batch_size
    }


def batch_encode_optimized(
    model: SentenceTransformer,
    sentences: List[str],
    batch_size: int = 64,
    prompt_name: str = "passage",
    show_progress: bool = True
) -> Dict[str, Any]:
    """Xá»­ lÃ½ batch tá»‘i Æ°u hÃ³a vá»›i memory management"""
    start_time = time.time()
    start_memory = get_memory_usage()
    
    print(f"ğŸš€ Tá»‘i Æ°u hÃ³a batch processing cho {len(sentences)} cÃ¢u...")
    
    # Encode toÃ n bá»™ vá»›i batch size lá»›n hÆ¡n
    embeddings = model.encode(
        sentences, 
        prompt_name=prompt_name,
        batch_size=batch_size,
        show_progress_bar=show_progress,
        convert_to_numpy=True
    )
    
    # Force garbage collection
    gc.collect()
    
    end_time = time.time()
    end_memory = get_memory_usage()
    
    return {
        'embeddings': embeddings,
        'total_time': end_time - start_time,
        'sentences_per_second': len(sentences) / (end_time - start_time),
        'memory_used_mb': end_memory['rss_mb'] - start_memory['rss_mb'],
        'batch_size': batch_size
    }


def compare_batch_strategies(
    model: SentenceTransformer,
    sentences: List[str]
) -> Dict[str, Dict[str, Any]]:
    """So sÃ¡nh cÃ¡c chiáº¿n lÆ°á»£c batch khÃ¡c nhau"""
    print(f"\nğŸ“Š So sÃ¡nh chiáº¿n lÆ°á»£c batch cho {len(sentences)} cÃ¢u:")
    print("=" * 60)
    
    strategies = {
        'sequential_32': lambda: batch_encode_sequential(model, sentences, 32),
        'sequential_64': lambda: batch_encode_sequential(model, sentences, 64),
        'optimized_64': lambda: batch_encode_optimized(model, sentences, 64),
        'optimized_128': lambda: batch_encode_optimized(model, sentences, 128)
    }
    
    results = {}
    
    for strategy_name, strategy_func in strategies.items():
        print(f"\nğŸ” Testing {strategy_name}...")
        try:
            results[strategy_name] = strategy_func()
            
            result = results[strategy_name]
            print(f"âœ… {strategy_name}:")
            print(f"   Thá»i gian: {result['total_time']:.2f}s")
            print(f"   Tá»‘c Ä‘á»™: {result['sentences_per_second']:.1f} cÃ¢u/s")
            print(f"   Bá»™ nhá»›: {result['memory_used_mb']:.1f} MB")
            
        except Exception as e:
            print(f"âŒ {strategy_name} tháº¥t báº¡i: {e}")
            results[strategy_name] = {'error': str(e)}
    
    return results


def performance_benchmark(model: SentenceTransformer) -> None:
    """Cháº¡y benchmark toÃ n diá»‡n"""
    print("ğŸ Báº®T Äáº¦U PERFORMANCE BENCHMARK")
    print("=" * 70)
    
    datasets = create_test_datasets()
    
    for dataset_name, sentences in datasets.items():
        print(f"\nğŸ“ˆ Dataset: {dataset_name.upper()} ({len(sentences)} cÃ¢u)")
        print("-" * 50)
        
        # Chá»‰ test strategy tá»‘t nháº¥t cho dataset lá»›n
        if len(sentences) > 100:
            result = batch_encode_optimized(model, sentences, batch_size=128)
            print(f"âœ… Optimized batch (128):")
            print(f"   Thá»i gian: {result['total_time']:.2f}s") 
            print(f"   Tá»‘c Ä‘á»™: {result['sentences_per_second']:.1f} cÃ¢u/s")
            print(f"   Bá»™ nhá»›: {result['memory_used_mb']:.1f} MB")
        else:
            # Test Ä‘áº§y Ä‘á»§ cho dataset nhá»
            compare_batch_strategies(model, sentences)


def analyze_embeddings(embeddings: np.ndarray, sentences: List[str]) -> None:
    """PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng embeddings"""
    print(f"\nğŸ”¬ PHÃ‚N TÃCH EMBEDDINGS")
    print("=" * 40)
    
    print(f"ğŸ“Š Thá»‘ng kÃª cÆ¡ báº£n:")
    print(f"   Shape: {embeddings.shape}")
    print(f"   Dtype: {embeddings.dtype}")
    print(f"   Min: {embeddings.min():.6f}")
    print(f"   Max: {embeddings.max():.6f}")
    print(f"   Mean: {embeddings.mean():.6f}")
    print(f"   Std: {embeddings.std():.6f}")
    
    # TÃ­nh similarity matrix cho sample
    if len(sentences) <= 10:
        from sklearn.metrics.pairwise import cosine_similarity
        sim_matrix = cosine_similarity(embeddings)
        
        print(f"\nğŸ¤ Cosine Similarity Matrix:")
        for i in range(min(5, len(sentences))):
            for j in range(min(5, len(sentences))):
                print(f"{sim_matrix[i][j]:.3f}", end=" ")
            print()


def main():
    """Main function Ä‘á»ƒ cháº¡y táº¥t cáº£ tests"""
    print("ğŸš€ NOMIC EMBED TEXT V2 MoE - BATCH PERFORMANCE TEST")
    print("=" * 70)    # Hiá»ƒn thá»‹ thÃ´ng tin há»‡ thá»‘ng
    print(f"ğŸ’» System Info:")
    print(f"   CPU cores: {psutil.cpu_count()}")
    if HAS_PSUTIL:
        print(f"   Memory: {psutil.virtual_memory().total / 1024**3:.1f} GB")
    else:
        print(f"   Memory: N/A (psutil not available)")
    print(f"   Python PID: {os.getpid()}")
    
    # Load model
    print(f"\nğŸ¤– Loading model...")
    load_start = time.time()
    
    model = SentenceTransformer(
        "nomic-ai/nomic-embed-text-v2-moe", 
        trust_remote_code=True
    )
    
    load_time = time.time() - load_start
    print(f"âœ… Model loaded in {load_time:.2f}s")
    
    # Cháº¡y performance benchmark
    performance_benchmark(model)
    
    # Test vá»›i dataset medium Ä‘á»ƒ phÃ¢n tÃ­ch embeddings
    print(f"\nğŸ”¬ Embedding analysis on medium dataset...")
    datasets = create_test_datasets()
    result = batch_encode_optimized(model, datasets['medium'][:10])
    analyze_embeddings(result['embeddings'], datasets['medium'][:10])
    
    print(f"\nğŸ‰ BENCHMARK HOÃ€N THÃ€NH!")


if __name__ == "__main__":
    main()
