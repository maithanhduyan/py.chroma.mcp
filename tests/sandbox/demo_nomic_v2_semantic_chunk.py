# Generated by Copilot
"""
Demo Nomic v2 Semantic Chunking with ChromaDB
Đọc văn bản từ docs/truyen_xuyen_khong_chapter_01.md và nhập vào ChromaDB
với semantic chunking và Nomic embedding model
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import time
import unicodedata
import re

# Add src to path để import các module trong project
current_dir = Path(__file__).parent
src_dir = current_dir.parent
sys.path.insert(0, str(src_dir))

# Import các module cần thiết
try:
    from sentence_transformers import SentenceTransformer
    import chromadb
    from chromadb.config import Settings

    HAS_REQUIRED_DEPS = True
except ImportError as e:
    print(f"❌ Thiếu dependency: {e}")
    print("🔧 Chạy: uv add sentence-transformers chromadb")
    HAS_REQUIRED_DEPS = False


def normalize_vietnamese_text(text: str) -> str:
    """Chuẩn hóa Unicode cho văn bản tiếng Việt"""
    # Normalize to NFC (Canonical Decomposition, followed by Canonical Composition)
    return unicodedata.normalize("NFC", text)


def clean_text(text: str) -> str:
    """Làm sạch văn bản và chuẩn hóa"""
    # Normalize Unicode
    text = normalize_vietnamese_text(text)

    # Remove excessive whitespace and normalize line breaks
    text = re.sub(r"\n\s*\n", "\n\n", text)  # Normalize paragraph breaks
    text = re.sub(r"\t+", " ", text)  # Replace tabs with spaces
    text = re.sub(r" +", " ", text)  # Remove multiple spaces

    # Clean up markdown headers while keeping content
    text = re.sub(r"^#{1,6}\s*", "", text, flags=re.MULTILINE)

    return text.strip()


def semantic_chunk_vietnamese(
    text: str,
    max_chunk_size: int = 500,
    overlap_size: int = 50,
    min_chunk_size: int = 100,
) -> List[str]:
    """
    Chia văn bản tiếng Việt thành chunks với semantic awareness

    Args:
        text: Văn bản cần chia
        max_chunk_size: Kích thước tối đa của chunk (ký tự)
        overlap_size: Kích thước overlap giữa chunks
        min_chunk_size: Kích thước tối thiểu của chunk

    Returns:
        List các chunks
    """
    # Clean text first
    text = clean_text(text)

    # Split by paragraphs first
    paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]

    chunks = []
    current_chunk = ""

    for paragraph in paragraphs:
        # If paragraph is too long, split by sentences
        if len(paragraph) > max_chunk_size:
            # Vietnamese sentence endings
            sentences = re.split(r"[.!?。]\s*", paragraph)

            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue

                # If adding this sentence would exceed chunk size
                if len(current_chunk) + len(sentence) > max_chunk_size:
                    if current_chunk and len(current_chunk) >= min_chunk_size:
                        chunks.append(current_chunk.strip())

                        # Create overlap with previous chunk
                        if overlap_size > 0:
                            words = current_chunk.split()
                            overlap_words = words[
                                -overlap_size // 10 :
                            ]  # Rough overlap
                            current_chunk = " ".join(overlap_words) + " " + sentence
                        else:
                            current_chunk = sentence
                    else:
                        current_chunk += " " + sentence if current_chunk else sentence
                else:
                    current_chunk += " " + sentence if current_chunk else sentence
        else:
            # If adding this paragraph would exceed chunk size
            if len(current_chunk) + len(paragraph) > max_chunk_size:
                if current_chunk and len(current_chunk) >= min_chunk_size:
                    chunks.append(current_chunk.strip())

                    # Create overlap
                    if overlap_size > 0:
                        words = current_chunk.split()
                        overlap_words = words[-overlap_size // 10 :]
                        current_chunk = " ".join(overlap_words) + " " + paragraph
                    else:
                        current_chunk = paragraph
                else:
                    current_chunk += "\n\n" + paragraph if current_chunk else paragraph
            else:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph

    # Add the last chunk
    if current_chunk and len(current_chunk) >= min_chunk_size:
        chunks.append(current_chunk.strip())

    return chunks


def create_chromadb_collection(
    collection_name: str = "truyen_xuyen_khong_chapter_01",
    chroma_db_path: str = "./chroma_db",
) -> chromadb.Collection:
    """Tạo ChromaDB collection"""

    # Initialize ChromaDB client
    client = chromadb.PersistentClient(
        path=chroma_db_path,
        settings=Settings(anonymized_telemetry=False, allow_reset=True),
    )

    # Try to get existing collection, if not create new one
    try:
        collection = client.get_collection(name=collection_name)
        print(f"📂 Sử dụng collection hiện có: {collection_name}")

        # Clear existing data for fresh start
        collection.delete()
        print(f"🗑️  Đã xóa dữ liệu cũ trong collection")

    except Exception:
        print(f"📝 Tạo collection mới: {collection_name}")

    # Create collection
    collection = client.create_collection(
        name=collection_name, metadata={"hnsw:space": "cosine"}  # Use cosine similarity
    )

    return collection


def load_nomic_model() -> SentenceTransformer:
    """Tải Nomic embedding model"""
    print("🤖 Đang tải Nomic Embed Text v2 MoE model...")
    start_time = time.time()

    model = SentenceTransformer(
        "nomic-ai/nomic-embed-text-v2-moe", trust_remote_code=True
    )

    load_time = time.time() - start_time
    print(f"✅ Model đã tải xong trong {load_time:.2f}s")

    return model


def read_document(file_path: str) -> str:
    """Đọc văn bản từ file"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        print(f"📖 Đã đọc file: {file_path}")
        print(f"📏 Độ dài văn bản: {len(content):,} ký tự")
        return content
    except Exception as e:
        print(f"❌ Lỗi đọc file {file_path}: {e}")
        return ""


def embed_and_store_chunks(
    model: SentenceTransformer,
    collection: chromadb.Collection,
    chunks: List[str],
    metadata: Optional[Dict[str, Any]] = None,
) -> None:
    """Tạo embeddings và lưu vào ChromaDB"""

    if not chunks:
        print("⚠️  Không có chunks để xử lý")
        return

    print(f"🔄 Đang tạo embeddings cho {len(chunks)} chunks...")
    start_time = time.time()

    # Create embeddings in batches for efficiency
    batch_size = 32
    all_embeddings = []

    for i in range(0, len(chunks), batch_size):
        batch_chunks = chunks[i : i + batch_size]
        batch_embeddings = model.encode(
            batch_chunks,
            prompt_name="passage",
            show_progress_bar=False,
            convert_to_numpy=True,
        )
        all_embeddings.extend(batch_embeddings)

        print(
            f"   Processed batch {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1}"
        )

    embed_time = time.time() - start_time
    print(f"✅ Embeddings hoàn thành trong {embed_time:.2f}s")
    print(f"📊 Tốc độ: {len(chunks)/embed_time:.1f} chunks/s")

    # Prepare data for ChromaDB
    ids = [f"chunk_{i:04d}" for i in range(len(chunks))]

    # Create metadata for each chunk
    metadatas = []
    for i, chunk in enumerate(chunks):
        chunk_metadata = {
            "chunk_id": i,
            "chunk_length": len(chunk),
            "word_count": len(chunk.split()),
            "chapter": "chapter_01",
            **(metadata or {}),
        }
        metadatas.append(chunk_metadata)

    # Store in ChromaDB
    print(f"💾 Đang lưu vào ChromaDB...")
    store_start = time.time()

    collection.add(
        embeddings=all_embeddings, documents=chunks, metadatas=metadatas, ids=ids
    )

    store_time = time.time() - store_start
    print(f"✅ Đã lưu vào ChromaDB trong {store_time:.2f}s")

    # Verify storage
    count = collection.count()
    print(f"📝 Total documents trong collection: {count}")


def search_demo(
    model: SentenceTransformer,
    collection: chromadb.Collection,
    query: str,
    n_results: int = 3,
) -> None:
    """Demo tìm kiếm semantic trong collection"""

    print(f"\n🔍 DEMO TÌM KIẾM: '{query}'")
    print("=" * 60)

    # Create query embedding
    query_embedding = model.encode([query], prompt_name="query")

    # Search in ChromaDB
    results = collection.query(
        query_embeddings=query_embedding.tolist(),
        n_results=n_results,
        include=["documents", "distances", "metadatas"],
    )

    if results["documents"] and results["documents"][0]:
        documents = results["documents"][0]
        distances_data = results.get("distances")
        metadatas_data = results.get("metadatas")

        distances = distances_data[0] if distances_data and distances_data[0] else []
        metadatas = metadatas_data[0] if metadatas_data and metadatas_data[0] else []

        for i, doc in enumerate(documents):
            distance = distances[i] if distances and i < len(distances) else 0.0
            metadata = metadatas[i] if metadatas and i < len(metadatas) else {}

            print(f"\n📄 Kết quả {i+1} (similarity: {1-distance:.3f}):")
            print(
                f"📋 Metadata: chunk_id={metadata.get('chunk_id', 'N/A')}, "
                f"length={metadata.get('chunk_length', 'N/A')}, words={metadata.get('word_count', 'N/A')}"
            )
            print(f"📝 Nội dung: {doc[:200]}...")
    else:
        print("❌ Không tìm thấy kết quả")


def main():
    """Main function"""
    if not HAS_REQUIRED_DEPS:
        return

    print("🚀 DEMO NOMIC V2 SEMANTIC CHUNKING WITH CHROMADB")
    print("=" * 70)

    # Define paths
    doc_path = (
        Path(__file__).parent.parent.parent
        / "docs"
        / "truyen_xuyen_khong_chapter_01.md"
    )

    if not doc_path.exists():
        print(f"❌ File không tồn tại: {doc_path}")
        return
    # Step 1: Read document
    content = read_document(str(doc_path))
    if not content:
        return

    # Step 2: Load model
    model = load_nomic_model()

    # Step 3: Semantic chunking
    print(f"\n📄 Đang thực hiện semantic chunking...")
    chunks = semantic_chunk_vietnamese(
        content,
        max_chunk_size=600,  # Larger chunks for Vietnamese
        overlap_size=100,
        min_chunk_size=150,
    )

    print(f"✅ Đã chia thành {len(chunks)} chunks")
    print(f"📊 Kích thước chunks:")
    for i, chunk in enumerate(chunks[:5]):  # Show first 5
        print(f"   Chunk {i+1}: {len(chunk)} ký tự, {len(chunk.split())} từ")
    if len(chunks) > 5:
        print(f"   ... và {len(chunks)-5} chunks khác")

    # Step 4: Create ChromaDB collection
    print(f"\n🗄️  Thiết lập ChromaDB...")
    collection = create_chromadb_collection()

    # Step 5: Embed and store
    metadata = {
        "title": "Đặc Công Xuyên Không Thành Cậu Bé Thời Nguyên Thủy",
        "chapter": "Chương 1",
        "language": "vietnamese",
        "genre": "tiểu thuyết xuyên không",
    }

    embed_and_store_chunks(model, collection, chunks, metadata)

    # Step 6: Demo searches
    test_queries = [
        "Hàn Phong bắt cá",
        "con khủng điểu",
        "thủ lĩnh bộ lạc",
        "làm giỏ đánh cá",
        "xuyên không thời nguyên thủy",
    ]

    for query in test_queries:
        search_demo(model, collection, query)

    print(f"\n🎉 DEMO HOÀN THÀNH!")
    print(f"💡 ChromaDB collection đã sẵn sàng để sử dụng")
    print(f"📍 Vị trí: ./chroma_db/")


if __name__ == "__main__":
    main()
