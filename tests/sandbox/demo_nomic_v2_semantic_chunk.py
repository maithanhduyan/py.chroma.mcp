# Generated by Copilot
"""
Demo Nomic v2 Semantic Chunking with ChromaDB
Äá»c vÄƒn báº£n tá»« docs/truyen_xuyen_khong_chapter_01.md vÃ  nháº­p vÃ o ChromaDB
vá»›i semantic chunking vÃ  Nomic embedding model
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import time
import unicodedata
import re

# Add src to path Ä‘á»ƒ import cÃ¡c module trong project
current_dir = Path(__file__).parent
src_dir = current_dir.parent
sys.path.insert(0, str(src_dir))

# Import cÃ¡c module cáº§n thiáº¿t
try:
    from sentence_transformers import SentenceTransformer
    import chromadb
    from chromadb.config import Settings

    HAS_REQUIRED_DEPS = True
except ImportError as e:
    print(f"âŒ Thiáº¿u dependency: {e}")
    print("ğŸ”§ Cháº¡y: uv add sentence-transformers chromadb")
    HAS_REQUIRED_DEPS = False


def normalize_vietnamese_text(text: str) -> str:
    """Chuáº©n hÃ³a Unicode cho vÄƒn báº£n tiáº¿ng Viá»‡t"""
    # Normalize to NFC (Canonical Decomposition, followed by Canonical Composition)
    return unicodedata.normalize("NFC", text)


def clean_text(text: str) -> str:
    """LÃ m sáº¡ch vÄƒn báº£n vÃ  chuáº©n hÃ³a"""
    # Normalize Unicode
    text = normalize_vietnamese_text(text)

    # Remove excessive whitespace and normalize line breaks
    text = re.sub(r"\n\s*\n", "\n\n", text)  # Normalize paragraph breaks
    text = re.sub(r"\t+", " ", text)  # Replace tabs with spaces
    text = re.sub(r" +", " ", text)  # Remove multiple spaces

    # Clean up markdown headers while keeping content
    text = re.sub(r"^#{1,6}\s*", "", text, flags=re.MULTILINE)

    return text.strip()


def semantic_chunk_vietnamese(
    text: str,
    max_chunk_size: int = 500,
    overlap_size: int = 50,
    min_chunk_size: int = 100,
) -> List[str]:
    """
    Chia vÄƒn báº£n tiáº¿ng Viá»‡t thÃ nh chunks vá»›i semantic awareness

    Args:
        text: VÄƒn báº£n cáº§n chia
        max_chunk_size: KÃ­ch thÆ°á»›c tá»‘i Ä‘a cá»§a chunk (kÃ½ tá»±)
        overlap_size: KÃ­ch thÆ°á»›c overlap giá»¯a chunks
        min_chunk_size: KÃ­ch thÆ°á»›c tá»‘i thiá»ƒu cá»§a chunk

    Returns:
        List cÃ¡c chunks
    """
    # Clean text first
    text = clean_text(text)

    # Split by paragraphs first
    paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]

    chunks = []
    current_chunk = ""

    for paragraph in paragraphs:
        # If paragraph is too long, split by sentences
        if len(paragraph) > max_chunk_size:
            # Vietnamese sentence endings
            sentences = re.split(r"[.!?ã€‚]\s*", paragraph)

            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue

                # If adding this sentence would exceed chunk size
                if len(current_chunk) + len(sentence) > max_chunk_size:
                    if current_chunk and len(current_chunk) >= min_chunk_size:
                        chunks.append(current_chunk.strip())

                        # Create overlap with previous chunk
                        if overlap_size > 0:
                            words = current_chunk.split()
                            overlap_words = words[
                                -overlap_size // 10 :
                            ]  # Rough overlap
                            current_chunk = " ".join(overlap_words) + " " + sentence
                        else:
                            current_chunk = sentence
                    else:
                        current_chunk += " " + sentence if current_chunk else sentence
                else:
                    current_chunk += " " + sentence if current_chunk else sentence
        else:
            # If adding this paragraph would exceed chunk size
            if len(current_chunk) + len(paragraph) > max_chunk_size:
                if current_chunk and len(current_chunk) >= min_chunk_size:
                    chunks.append(current_chunk.strip())

                    # Create overlap
                    if overlap_size > 0:
                        words = current_chunk.split()
                        overlap_words = words[-overlap_size // 10 :]
                        current_chunk = " ".join(overlap_words) + " " + paragraph
                    else:
                        current_chunk = paragraph
                else:
                    current_chunk += "\n\n" + paragraph if current_chunk else paragraph
            else:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph

    # Add the last chunk
    if current_chunk and len(current_chunk) >= min_chunk_size:
        chunks.append(current_chunk.strip())

    return chunks


def create_chromadb_collection(
    collection_name: str = "truyen_xuyen_khong_chapter_01",
    chroma_db_path: str = "./chroma_db",
) -> chromadb.Collection:
    """Táº¡o ChromaDB collection"""

    # Initialize ChromaDB client
    client = chromadb.PersistentClient(
        path=chroma_db_path,
        settings=Settings(anonymized_telemetry=False, allow_reset=True),
    )

    # Try to get existing collection, if not create new one
    try:
        collection = client.get_collection(name=collection_name)
        print(f"ğŸ“‚ Sá»­ dá»¥ng collection hiá»‡n cÃ³: {collection_name}")

        # Clear existing data for fresh start
        collection.delete()
        print(f"ğŸ—‘ï¸  ÄÃ£ xÃ³a dá»¯ liá»‡u cÅ© trong collection")

    except Exception:
        print(f"ğŸ“ Táº¡o collection má»›i: {collection_name}")

    # Create collection
    collection = client.create_collection(
        name=collection_name, metadata={"hnsw:space": "cosine"}  # Use cosine similarity
    )

    return collection


def load_nomic_model() -> SentenceTransformer:
    """Táº£i Nomic embedding model"""
    print("ğŸ¤– Äang táº£i Nomic Embed Text v2 MoE model...")
    start_time = time.time()

    model = SentenceTransformer(
        "nomic-ai/nomic-embed-text-v2-moe", trust_remote_code=True
    )

    load_time = time.time() - start_time
    print(f"âœ… Model Ä‘Ã£ táº£i xong trong {load_time:.2f}s")

    return model


def read_document(file_path: str) -> str:
    """Äá»c vÄƒn báº£n tá»« file"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        print(f"ğŸ“– ÄÃ£ Ä‘á»c file: {file_path}")
        print(f"ğŸ“ Äá»™ dÃ i vÄƒn báº£n: {len(content):,} kÃ½ tá»±")
        return content
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c file {file_path}: {e}")
        return ""


def embed_and_store_chunks(
    model: SentenceTransformer,
    collection: chromadb.Collection,
    chunks: List[str],
    metadata: Optional[Dict[str, Any]] = None,
) -> None:
    """Táº¡o embeddings vÃ  lÆ°u vÃ o ChromaDB"""

    if not chunks:
        print("âš ï¸  KhÃ´ng cÃ³ chunks Ä‘á»ƒ xá»­ lÃ½")
        return

    print(f"ğŸ”„ Äang táº¡o embeddings cho {len(chunks)} chunks...")
    start_time = time.time()

    # Create embeddings in batches for efficiency
    batch_size = 32
    all_embeddings = []

    for i in range(0, len(chunks), batch_size):
        batch_chunks = chunks[i : i + batch_size]
        batch_embeddings = model.encode(
            batch_chunks,
            prompt_name="passage",
            show_progress_bar=False,
            convert_to_numpy=True,
        )
        all_embeddings.extend(batch_embeddings)

        print(
            f"   Processed batch {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1}"
        )

    embed_time = time.time() - start_time
    print(f"âœ… Embeddings hoÃ n thÃ nh trong {embed_time:.2f}s")
    print(f"ğŸ“Š Tá»‘c Ä‘á»™: {len(chunks)/embed_time:.1f} chunks/s")

    # Prepare data for ChromaDB
    ids = [f"chunk_{i:04d}" for i in range(len(chunks))]

    # Create metadata for each chunk
    metadatas = []
    for i, chunk in enumerate(chunks):
        chunk_metadata = {
            "chunk_id": i,
            "chunk_length": len(chunk),
            "word_count": len(chunk.split()),
            "chapter": "chapter_01",
            **(metadata or {}),
        }
        metadatas.append(chunk_metadata)

    # Store in ChromaDB
    print(f"ğŸ’¾ Äang lÆ°u vÃ o ChromaDB...")
    store_start = time.time()

    collection.add(
        embeddings=all_embeddings, documents=chunks, metadatas=metadatas, ids=ids
    )

    store_time = time.time() - store_start
    print(f"âœ… ÄÃ£ lÆ°u vÃ o ChromaDB trong {store_time:.2f}s")

    # Verify storage
    count = collection.count()
    print(f"ğŸ“ Total documents trong collection: {count}")


def search_demo(
    model: SentenceTransformer,
    collection: chromadb.Collection,
    query: str,
    n_results: int = 3,
) -> None:
    """Demo tÃ¬m kiáº¿m semantic trong collection"""

    print(f"\nğŸ” DEMO TÃŒM KIáº¾M: '{query}'")
    print("=" * 60)

    # Create query embedding
    query_embedding = model.encode([query], prompt_name="query")

    # Search in ChromaDB
    results = collection.query(
        query_embeddings=query_embedding.tolist(),
        n_results=n_results,
        include=["documents", "distances", "metadatas"],
    )

    if results["documents"] and results["documents"][0]:
        documents = results["documents"][0]
        distances_data = results.get("distances")
        metadatas_data = results.get("metadatas")

        distances = distances_data[0] if distances_data and distances_data[0] else []
        metadatas = metadatas_data[0] if metadatas_data and metadatas_data[0] else []

        for i, doc in enumerate(documents):
            distance = distances[i] if distances and i < len(distances) else 0.0
            metadata = metadatas[i] if metadatas and i < len(metadatas) else {}

            print(f"\nğŸ“„ Káº¿t quáº£ {i+1} (similarity: {1-distance:.3f}):")
            print(
                f"ğŸ“‹ Metadata: chunk_id={metadata.get('chunk_id', 'N/A')}, "
                f"length={metadata.get('chunk_length', 'N/A')}, words={metadata.get('word_count', 'N/A')}"
            )
            print(f"ğŸ“ Ná»™i dung: {doc[:200]}...")
    else:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£")


def main():
    """Main function"""
    if not HAS_REQUIRED_DEPS:
        return

    print("ğŸš€ DEMO NOMIC V2 SEMANTIC CHUNKING WITH CHROMADB")
    print("=" * 70)

    # Define paths
    doc_path = (
        Path(__file__).parent.parent.parent
        / "docs"
        / "truyen_xuyen_khong_chapter_01.md"
    )

    if not doc_path.exists():
        print(f"âŒ File khÃ´ng tá»“n táº¡i: {doc_path}")
        return
    # Step 1: Read document
    content = read_document(str(doc_path))
    if not content:
        return

    # Step 2: Load model
    model = load_nomic_model()

    # Step 3: Semantic chunking
    print(f"\nğŸ“„ Äang thá»±c hiá»‡n semantic chunking...")
    chunks = semantic_chunk_vietnamese(
        content,
        max_chunk_size=600,  # Larger chunks for Vietnamese
        overlap_size=100,
        min_chunk_size=150,
    )

    print(f"âœ… ÄÃ£ chia thÃ nh {len(chunks)} chunks")
    print(f"ğŸ“Š KÃ­ch thÆ°á»›c chunks:")
    for i, chunk in enumerate(chunks[:5]):  # Show first 5
        print(f"   Chunk {i+1}: {len(chunk)} kÃ½ tá»±, {len(chunk.split())} tá»«")
    if len(chunks) > 5:
        print(f"   ... vÃ  {len(chunks)-5} chunks khÃ¡c")

    # Step 4: Create ChromaDB collection
    print(f"\nğŸ—„ï¸  Thiáº¿t láº­p ChromaDB...")
    collection = create_chromadb_collection()

    # Step 5: Embed and store
    metadata = {
        "title": "Äáº·c CÃ´ng XuyÃªn KhÃ´ng ThÃ nh Cáº­u BÃ© Thá»i NguyÃªn Thá»§y",
        "chapter": "ChÆ°Æ¡ng 1",
        "language": "vietnamese",
        "genre": "tiá»ƒu thuyáº¿t xuyÃªn khÃ´ng",
    }

    embed_and_store_chunks(model, collection, chunks, metadata)

    # Step 6: Demo searches
    test_queries = [
        "HÃ n Phong báº¯t cÃ¡",
        "con khá»§ng Ä‘iá»ƒu",
        "thá»§ lÄ©nh bá»™ láº¡c",
        "lÃ m giá» Ä‘Ã¡nh cÃ¡",
        "xuyÃªn khÃ´ng thá»i nguyÃªn thá»§y",
    ]

    for query in test_queries:
        search_demo(model, collection, query)

    print(f"\nğŸ‰ DEMO HOÃ€N THÃ€NH!")
    print(f"ğŸ’¡ ChromaDB collection Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ sá»­ dá»¥ng")
    print(f"ğŸ“ Vá»‹ trÃ­: ./chroma_db/")


if __name__ == "__main__":
    main()
