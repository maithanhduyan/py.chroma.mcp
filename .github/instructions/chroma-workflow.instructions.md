# Generated by Copilot
---
applyTo: "**"
---

# ChromaDB MCP Server Workflow Instructions

## Overview
This document provides step-by-step instructions for AI assistants to use the ChromaDB MCP server tools to store and manage large datasets efficiently.

## Available MCP Tools

### Core Operations
- `mcp_chroma_create_collection` - Tạo collection mới
- `mcp_chroma_list_collections` - Liệt kê tất cả collections
- `mcp_chroma_delete_collection` - Xóa collection
- `mcp_chroma_get_collection_info` - Lấy thông tin collection
- `mcp_chroma_get_collection_count` - Đếm số documents trong collection

### Document Management
- `mcp_chroma_add_documents` - Thêm documents vào collection
- `mcp_chroma_get_documents` - Lấy documents từ collection
- `mcp_chroma_update_documents` - Cập nhật documents
- `mcp_chroma_delete_documents` - Xóa documents
- `mcp_chroma_query_documents` - Tìm kiếm documents tương tự
- `mcp_chroma_peek_collection` - Xem preview documents

### Text Processing
- `mcp_chroma_semantic_chunking` - Chia nhỏ văn bản thành chunks
- `mcp_chroma_echo` - Test connection

## Workflow: Đưa Dữ Liệu Lớn Vào ChromaDB

### Step 1: Kiểm tra Connection
```
1. Sử dụng mcp_chroma_echo để test connection
2. Kiểm tra danh sách collections hiện có với mcp_chroma_list_collections
```

### Step 2: Tạo Collection (nếu cần)
```
1. Nếu collection chưa tồn tại, tạo mới với mcp_chroma_create_collection
2. Sử dụng tên collection có ý nghĩa (ví dụ: "vietnamese_stories", "technical_docs")
3. Cấu hình embedding function phù hợp (default, openai, cohere, etc.)
```

### Step 3: Xử Lý Văn Bản Lớn
```
1. Sử dụng mcp_chroma_semantic_chunking để chia nhỏ văn bản
2. Cấu hình chunk_size phù hợp (300-1000 ký tự cho tiếng Việt)
3. Thiết lập overlap để đảm bảo ngữ nghĩa liên tục (50-100 ký tự)
4. Xử lý từng phần của văn bản nếu quá lớn
```

### Step 4: Batch Processing
```
1. Chia documents thành các batch nhỏ (10-50 documents/batch)
2. Tạo ID duy nhất cho mỗi chunk (ví dụ: "doc1_chunk1", "doc1_chunk2")
3. Thêm metadata hữu ích (source, chapter, page, timestamp, etc.)
4. Sử dụng mcp_chroma_add_documents cho mỗi batch
```

### Step 5: Validation và Monitoring
```
1. Kiểm tra số lượng documents đã thêm với mcp_chroma_get_collection_count
2. Sử dụng mcp_chroma_peek_collection để xem preview
3. Test tìm kiếm với mcp_chroma_query_documents
4. Xử lý lỗi và retry nếu cần
```

## Best Practices

### Text Chunking Strategy
- **Tiếng Việt**: Chunk size 500-800 ký tự, overlap 50-100 ký tự
- **Tiếng Anh**: Chunk size 800-1200 ký tự, overlap 100-150 ký tự
- **Code**: Chunk theo function/class, overlap 2-3 lines
- **Markdown**: Chunk theo section, preserve headers

### Metadata Design
```json
{
  "source": "file_name.md",
  "chapter": "chapter_01",
  "language": "vietnamese",
  "content_type": "story",
  "timestamp": "2025-06-18T16:00:00Z",
  "chunk_index": 1,
  "total_chunks": 10
}
```

### Error Handling
- Kiểm tra response từ mỗi MCP call
- Retry với exponential backoff nếu fail
- Log progress để có thể resume
- Validate data trước khi add

### Performance Optimization
- Batch size: 20-50 documents tối ưu
- Sử dụng metadata filtering cho queries
- Index theo pattern thường dùng
- Monitor memory usage

## Example Workflows

### Workflow 1: Thêm Truyện Tiếng Việt
```
1. mcp_chroma_create_collection("vietnamese_stories")
2. mcp_chroma_semantic_chunking(story_text, chunk_size=600, overlap=80)
3. For each chunk:
   - Create unique ID: f"story_{story_id}_chunk_{i}"
   - Add metadata: {"source": story_name, "chapter": chapter_num}
   - mcp_chroma_add_documents(collection, [chunk], [id], [metadata])
4. mcp_chroma_get_collection_count("vietnamese_stories")
5. mcp_chroma_query_documents("vietnamese_stories", ["tìm kiếm test"])
```

### Workflow 2: Thêm Technical Documentation
```
1. mcp_chroma_create_collection("tech_docs", embedding_function_name="openai")
2. For each markdown file:
   - Split by headers (##, ###)
   - mcp_chroma_semantic_chunking for large sections
   - Add with metadata: {"doc_type": "api", "version": "v1.0"}
3. Validate with mcp_chroma_peek_collection
```

### Workflow 3: Import Large Dataset
```
1. Check existing: mcp_chroma_list_collections()
2. Create collection with HNSW settings for large scale
3. Process in batches of 1000 documents
4. Monitor progress with collection count
5. Create backup/checkpoint after each 10k documents
```

## Troubleshooting

### Common Issues
1. **Collection không tồn tại**: Tạo trước khi add documents
2. **Chunk quá lớn**: Giảm chunk_size xuống 500-800 ký tự
3. **Memory error**: Giảm batch size xuống 10-20 documents
4. **Encoding error**: Ensure UTF-8 cho tiếng Việt

### Recovery Strategies
1. **Partial failure**: Resume từ checkpoint
2. **Data corruption**: Recreate collection từ source
3. **Performance issues**: Optimize chunk size và batch size
4. **Connection timeout**: Implement retry logic

## Vietnamese Text Specific Guidelines

### Text Preprocessing
- Normalize Unicode (NFC) trước khi chunking
- Preserve dấu câu tiếng Việt
- Handle special characters properly
- Consider sentence boundaries

### Chunk Boundary Rules
- Tránh cắt giữa từ ghép
- Preserve paragraph structure
- Keep dialogue intact
- Maintain context for proper nouns

### Search Optimization
- Use Vietnamese stopwords in metadata
- Consider tone marks in similarity
- Optimize for Vietnamese query patterns
- Test with common Vietnamese phrases

## Monitoring và Maintenance

### Health Checks
```
1. Weekly: mcp_chroma_list_collections() - check collection health
2. Daily: mcp_chroma_get_collection_count() - monitor growth
3. Hourly: mcp_chroma_echo() - verify connection
```

### Performance Metrics
- Documents per second throughput
- Query response times
- Memory usage patterns
- Error rates and retry counts

### Data Integrity
- Verify chunk count matches expected
- Spot check random samples
- Test search relevance
- Monitor embedding quality

## Example Code Patterns

### Safe Document Addition
```python
def add_documents_safely(collection_name, documents, ids, metadatas):
    try:
        result = mcp_chroma_add_documents(collection_name, documents, ids, metadatas)
        if result:
            print(f"✅ Added {len(documents)} documents")
            return True
    except Exception as e:
        print(f"❌ Failed to add documents: {e}")
        return False
```

### Batch Processing Template
```python
def process_large_text(text, collection_name, source_info):
    # 1. Chunk text
    chunks = mcp_chroma_semantic_chunking(text, chunk_size=600, overlap=80)
    
    # 2. Process in batches
    batch_size = 20
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i+batch_size]
        ids = [f"{source_info['id']}_chunk_{j}" for j in range(i, min(i+batch_size, len(chunks)))]
        metadatas = [{"source": source_info['name'], "chunk_index": j} for j in range(len(batch))]
        
        # 3. Add with error handling
        add_documents_safely(collection_name, batch, ids, metadatas)
        
        # 4. Progress update
        print(f"Processed {min(i+batch_size, len(chunks))}/{len(chunks)} chunks")
```

## Integration Guidelines

### With Vietnamese Content
- Sử dụng embedding model hỗ trợ tiếng Việt tốt
- Test với content tiếng Việt thực tế
- Optimize cho query patterns tiếng Việt
- Handle mixed Vietnamese-English content

### With Large Files
- Stream processing cho files > 100MB
- Checkpoint progress every 1000 documents
- Memory management với batch processing
- Parallel processing nếu có thể

### With Real-time Updates
- Incremental addition strategy
- Conflict resolution cho duplicate IDs
- Version control cho document updates
- Efficient delta updates

This workflow ensures efficient, reliable, and scalable data ingestion into ChromaDB using the MCP server tools.
