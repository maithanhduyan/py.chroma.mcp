"""
MCP Tools with FastMCP Integration
Provides ChromaDB integration and utility tools using MCP protocol
Generated by Copilot
"""

"""
list of tools:
- echo()
- list_collections() 
- create_collection()
- delete_collection()
- add_documents()
- query_collection()
- get_embedding_model_info()
- configure_embedding_model()
- chunk_text_intelligent()
- get_model_sizes_info()
- get_performance_metrics()
"""

import logging
from typing import Dict, Any, List, Optional
import chromadb
from chromadb.config import Settings
from chromadb.types import Metadata
from mcp.server.fastmcp import FastMCP

# Import config to setup project paths automatically
import config

from embedding import EmbeddingManager
from embedding.chunker import chunk_text_intelligent as _chunk_text_intelligent
from utils.metrics import (
    MetricsCollector,
)
from utils.error_handler import (
    handle_mcp_tool_errors,
    get_error_tracker,
    OperationContext,
)

HAS_METRICS = True

logger = logging.getLogger(__name__)

# Initialize FastMCP instance
mcp = FastMCP("chroma-embedding-tools")

# Global instances
_chroma_client = None
_embedding_manager = None


def get_chroma_client():
    """Get or create the global Chroma client instance."""
    global _chroma_client
    if _chroma_client is None:
        _chroma_client = chromadb.PersistentClient(path="./chroma_db")
    return _chroma_client


def get_embedding_manager():
    """Get or create the global embedding manager instance."""
    global _embedding_manager
    if _embedding_manager is None:
        _embedding_manager = EmbeddingManager()
        # Load default small model asynchronously when needed
        logger.debug("🧠 EmbeddingManager initialized")
    return _embedding_manager


##### Core ChromaDB Tools with Embedding Integration #####


@mcp.tool()
@handle_mcp_tool_errors("echo")
async def echo(message: str) -> str:
    """Echo back the input message (useful for testing)."""
    return f"Echo: {message}"


@mcp.tool()
@handle_mcp_tool_errors("list_collections")
async def list_collections() -> List[str]:
    """List all collection names in the ChromaDB."""
    client = get_chroma_client()
    collections = client.list_collections()
    collection_names = [coll.name for coll in collections] if collections else []
    logger.info(f"📚 Found {len(collection_names)} collections")
    return collection_names


@mcp.tool()
@handle_mcp_tool_errors("create_collection")
async def create_collection(
    name: str, metadata: Optional[Dict[str, Any]] = None
) -> str:
    """Create a new ChromaDB collection."""
    if not name or not name.strip():
        raise ValueError("Collection name cannot be empty")

    name = name.strip()
    client = get_chroma_client()

    with OperationContext("create_collection", collection_name=name):
        # Use None instead of empty dict to avoid validation error
        collection = client.create_collection(name=name, metadata=metadata)
        logger.info(f"✨ Collection '{name}' created successfully")
        return f"Collection '{name}' created successfully"


@mcp.tool()
@handle_mcp_tool_errors("delete_collection")
async def delete_collection(name: str) -> str:
    """Delete a ChromaDB collection."""
    if not name or not name.strip():
        raise ValueError("Collection name cannot be empty")

    name = name.strip()
    client = get_chroma_client()

    with OperationContext("delete_collection", collection_name=name):
        client.delete_collection(name)
        logger.info(f"🗑️ Collection '{name}' deleted successfully")
        return f"Collection '{name}' deleted successfully"


@mcp.tool()
@handle_mcp_tool_errors("add_documents")
async def add_documents(
    collection_name: str,
    documents: List[str],
    ids: Optional[List[str]] = None,
    metadatas: Optional[List[Dict[str, Any]]] = None,
) -> str:
    """Add documents to a ChromaDB collection with custom embeddings."""
    if not documents:
        raise ValueError("Documents list cannot be empty")

    if not collection_name or not collection_name.strip():
        raise ValueError("Collection name cannot be empty")

    collection_name = collection_name.strip()

    # Clean and validate documents
    clean_documents = []
    for i, doc in enumerate(documents):
        if not isinstance(doc, str):
            logger.warning(f"Document {i} converted from {type(doc)} to string")
            doc = str(doc)
        doc = doc.strip()
        if doc:  # Only add non-empty documents
            clean_documents.append(doc)

    if not clean_documents:
        raise ValueError("No valid documents after cleaning")

    # Generate IDs if not provided
    if ids is None:
        ids = [f"doc_{i}" for i in range(len(clean_documents))]

    client = get_chroma_client()
    embedding_manager = get_embedding_manager()

    with OperationContext(
        "add_documents", collection_name=collection_name, doc_count=len(clean_documents)
    ):

        collection = client.get_or_create_collection(collection_name)

        # Try to use custom embeddings if available
        embeddings = None
        model_name = "ChromaDB default"

        if embedding_manager.current_model is not None:
            try:
                with OperationContext(
                    "generate_embeddings", doc_count=len(clean_documents)
                ):
                    embeddings = embedding_manager.encode_documents(clean_documents)

                model_info = embedding_manager.get_model_info()
                model_name = model_info.get("name", "unknown")
                logger.info(f"✨ Using custom embeddings: {model_name}")

            except Exception as e:
                logger.warning(
                    f"⚠️ Custom embedding failed, falling back to ChromaDB default: {e}"
                )

        # Add documents with or without custom embeddings
        if embeddings is not None:
            collection.add(
                documents=clean_documents,
                embeddings=embeddings,  # type: ignore
                ids=ids[: len(clean_documents)],
                metadatas=metadatas[: len(clean_documents)] if metadatas else None,  # type: ignore
            )
        else:
            collection.add(
                documents=clean_documents,
                ids=ids[: len(clean_documents)],
                metadatas=metadatas[: len(clean_documents)] if metadatas else None,  # type: ignore
            )

        result_message = f"Added {len(clean_documents)} documents to '{collection_name}' using {model_name} embeddings"
        logger.info(f"📄 {result_message}")
        return result_message


@mcp.tool()
@handle_mcp_tool_errors("query_collection")
async def query_collection(
    collection_name: str,
    query_texts: List[str],
    n_results: int = 10,
    where: Optional[Dict[str, Any]] = None,
) -> Any:
    """Query a ChromaDB collection with semantic search."""
    if not query_texts:
        raise ValueError("Query texts cannot be empty")

    if not collection_name or not collection_name.strip():
        raise ValueError("Collection name cannot be empty")

    if n_results <= 0:
        raise ValueError("n_results must be greater than 0")

    collection_name = collection_name.strip()
    client = get_chroma_client()
    embedding_manager = get_embedding_manager()

    with OperationContext(
        "query_collection",
        collection_name=collection_name,
        query_count=len(query_texts),
        n_results=n_results,
    ):

        collection = client.get_collection(collection_name)

        # Try to use custom query embeddings if available
        query_embeddings = None
        model_name = "ChromaDB default"

        if embedding_manager.current_model is not None and len(query_texts) > 0:
            try:
                with OperationContext(
                    "generate_query_embeddings", query_count=len(query_texts)
                ):
                    # Convert to list for encoding
                    embeddings_list = []
                    for query_text in query_texts:
                        embedding = embedding_manager.encode_query(query_text)
                        if embedding:
                            embeddings_list.append(embedding)

                    if embeddings_list:
                        query_embeddings = embeddings_list
                        model_info = embedding_manager.get_model_info()
                        model_name = model_info.get("name", "unknown")
                        logger.info(f"✨ Using custom query embeddings: {model_name}")

            except Exception as e:
                logger.warning(
                    f"⚠️ Custom query embedding failed, falling back to ChromaDB default: {e}"
                )

        # Perform query
        if query_embeddings:
            results = collection.query(
                query_embeddings=query_embeddings,  # type: ignore
                n_results=n_results,
                where=where,
            )
        else:
            results = collection.query(
                query_texts=query_texts,
                n_results=n_results,
                where=where,
            )  # Log results summary
        result_dict = dict(results)  # type: ignore
        total_results = 0
        if (
            result_dict.get("documents")
            and isinstance(result_dict["documents"], list)
            and len(result_dict["documents"]) > 0
        ):
            total_results = len(result_dict["documents"][0])
        logger.info(f"🔍 Query returned {total_results} results using {model_name}")

        return result_dict


##### Embedding Management Tools #####


@mcp.tool()
@handle_mcp_tool_errors("get_embedding_model_info")
async def get_embedding_model_info() -> Dict[str, Any]:
    """Get information about current embedding model."""
    embedding_manager = get_embedding_manager()
    model_info = embedding_manager.get_model_info()
    if model_info:
        logger.info(f"📊 Current model: {model_info.get('name', 'unknown')}")
        return model_info
    else:
        logger.warning("⚠️ No model currently loaded")
        return {"error": "No model currently loaded"}


@mcp.tool()
@handle_mcp_tool_errors("configure_embedding_model")
async def configure_embedding_model(model_name: str, force_reload: bool = False) -> str:
    """Configure the embedding model to use."""
    if not model_name or not model_name.strip():
        raise ValueError("Model name cannot be empty")

    model_name = model_name.strip()
    embedding_manager = get_embedding_manager()

    with OperationContext(
        "configure_embedding_model", model_name=model_name, force_reload=force_reload
    ):

        success = embedding_manager.load_model(model_name, force_reload)
        if success:
            model_info = embedding_manager.get_model_info()
            if model_info:
                model_name_info = model_info.get("name", model_name)
                dimension = model_info.get("embedding_dim", "unknown")
                result_message = f"Successfully configured embedding model: {model_name_info} (dim: {dimension})"
                logger.info(f"🎯 {result_message}")
                return result_message
            else:
                result_message = f"Model loaded but info unavailable: {model_name}"
                logger.warning(f"⚠️ {result_message}")
                return result_message
        else:
            error_message = (
                f"Failed to load model '{model_name}'. Using ChromaDB default."
            )
            logger.error(f"❌ {error_message}")
            raise Exception(error_message)


@mcp.tool()
@handle_mcp_tool_errors("chunk_text_intelligent")
async def chunk_text_intelligent(
    text: str, chunk_size: int = 400, overlap: int = 50
) -> List[str]:
    """Intelligently chunk text for better embedding (Vietnamese optimized)."""
    if not text or not text.strip():
        raise ValueError("Text cannot be empty")

    if chunk_size <= 0:
        raise ValueError("Chunk size must be greater than 0")

    if overlap < 0:
        raise ValueError("Overlap cannot be negative")

    if overlap >= chunk_size:
        raise ValueError("Overlap must be less than chunk size")

    # Validate input
    if not isinstance(text, str):
        text = str(text)

    text = text.strip()

    with OperationContext(
        "chunk_text_intelligent",
        text_length=len(text),
        chunk_size=chunk_size,
        overlap=overlap,
    ):

        # Ensure valid encoding
        try:
            text.encode("utf-8")
        except UnicodeEncodeError:
            text = text.encode("utf-8", errors="ignore").decode("utf-8")

        chunks = _chunk_text_intelligent(text, chunk_size, overlap)

        # Validate and clean output chunks
        cleaned_chunks = []
        for chunk in chunks:
            if isinstance(chunk, str) and chunk.strip():
                try:
                    chunk.encode("utf-8")
                    cleaned_chunks.append(chunk.strip())
                except UnicodeEncodeError:
                    cleaned_chunk = chunk.encode("utf-8", errors="ignore").decode(
                        "utf-8"
                    )
                    if cleaned_chunk.strip():
                        cleaned_chunks.append(cleaned_chunk.strip())

        logger.info(f"📝 Text chunked into {len(cleaned_chunks)} chunks")
        return cleaned_chunks


@mcp.tool()
@handle_mcp_tool_errors("get_performance_metrics")
async def get_performance_metrics() -> Dict[str, Any]:
    """Get performance metrics for embedding operations."""
    manager = get_embedding_manager()
    error_tracker = get_error_tracker()

    metrics = {}

    if hasattr(manager, "get_metrics"):
        metrics["embedding_metrics"] = manager.get_metrics()
    else:
        metrics["embedding_metrics"] = {
            "message": "Metrics not available",
            "total_operations": 0,
        }

    # Add error metrics
    metrics["error_metrics"] = error_tracker.get_error_summary()

    logger.info(
        f"📊 Retrieved performance metrics with {metrics['error_metrics']['total_errors']} total errors"
    )
    return metrics


@mcp.tool()
@handle_mcp_tool_errors("get_model_sizes_info")
async def get_model_sizes_info() -> Dict[str, Any]:
    """Get information about available embedding models and their sizes."""
    embedding_manager = get_embedding_manager()
    if hasattr(embedding_manager, "get_model_sizes_info"):
        model_info = embedding_manager.get_model_sizes_info()
        logger.info(
            f"📏 Retrieved model size info for {len(model_info.get('models', []))} models"
        )
        return model_info
    else:
        return {
            "message": "Model size information not available",
            "recommendation": "Use smaller models for faster loading",
        }


##### Utility Functions (for compatibility) #####


class MCPTools:
    """
    Legacy compatibility class for existing server.py integration.
    All actual functionality moved to @mcp.tool() functions above.
    """

    def __init__(self):
        """Initialize compatibility layer."""
        pass

    def get_tools_list(self) -> List[Dict[str, Any]]:
        """Get list of available tools for legacy compatibility."""
        # This is now handled by FastMCP automatically
        # Return empty list to maintain compatibility
        return []

    def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Any:
        """Legacy tool calling interface - deprecated."""
        logger.warning(
            f"Legacy call_tool used for {tool_name} - consider upgrading to FastMCP"
        )
        # Legacy fallback - this should not be used in production
        raise NotImplementedError("Use FastMCP @mcp.tool() functions instead")


# Export the FastMCP instance for server.py
__all__ = ["mcp", "MCPTools"]
