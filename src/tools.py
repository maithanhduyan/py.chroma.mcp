"""
MCP Tools with FastMCP Integration
Provides ChromaDB integration and utility tools using MCP protocol
Generated by Copilot
"""

"""
list of tools:
- echo()
- list_collections() 
- create_collection()
- delete_collection()
- add_documents()
- query_collection()
- get_embedding_model_info()
- configure_embedding_model()
- chunk_text_intelligent()
- get_model_sizes_info()
- get_performance_metrics()
- clear_embedding_cache()
- get_cache_stats()
- batch_process_documents()
"""

import logging
from typing import Dict, Any, List, Optional
import chromadb
from chromadb.config import Settings
from chromadb.types import Metadata
from mcp.server.fastmcp import FastMCP

# Import config to setup project paths automatically
import config

from embedding import EmbeddingManager
from embedding.chunker import chunk_text_intelligent as _chunk_text_intelligent
from utils.metrics import (
    MetricsCollector,
)
from utils.error_handler import (
    handle_mcp_tool_errors,
    get_error_tracker,
    OperationContext,
)

HAS_METRICS = True

logger = logging.getLogger(__name__)

# Initialize FastMCP instance
mcp = FastMCP("chroma-embedding-tools")

# Global instances
_chroma_client = None
_embedding_manager = None


def get_chroma_client():
    """Get or create the global Chroma client instance."""
    global _chroma_client
    if _chroma_client is None:
        _chroma_client = chromadb.PersistentClient(path="./chroma_db")
    return _chroma_client


def get_embedding_manager():
    """Get or create the global embedding manager instance."""
    global _embedding_manager
    if _embedding_manager is None:
        _embedding_manager = EmbeddingManager()
        # Ensure model is loaded for consistent embeddings
        if _embedding_manager.current_model is None:
            logger.info("üîÑ Loading embedding model for MCP tools...")
            success = _embedding_manager.load_best_available_model()
            if success:
                logger.info("‚úÖ Embedding model loaded successfully for MCP tools")
            else:
                logger.warning(
                    "‚ö†Ô∏è No custom embedding model available, using ChromaDB default"
                )
        logger.debug("üß† EmbeddingManager initialized")
    return _embedding_manager


##### Core ChromaDB Tools with Embedding Integration #####


@mcp.tool()
@handle_mcp_tool_errors("echo")
async def echo(message: str) -> str:
    """Echo back the input message (useful for testing)."""
    return f"Echo: {message}"


@mcp.tool()
@handle_mcp_tool_errors("list_collections")
async def list_collections() -> List[str]:
    """List all collection names in the ChromaDB."""
    client = get_chroma_client()
    collections = client.list_collections()
    collection_names = [coll.name for coll in collections] if collections else []
    logger.info(f"üìö Found {len(collection_names)} collections")
    return collection_names


@mcp.tool()
@handle_mcp_tool_errors("create_collection")
async def create_collection(
    name: str, metadata: Optional[Dict[str, Any]] = None
) -> str:
    """Create a new ChromaDB collection."""
    if not name or not name.strip():
        raise ValueError("Collection name cannot be empty")

    name = name.strip()
    client = get_chroma_client()

    with OperationContext("create_collection", collection_name=name):
        # Use None instead of empty dict to avoid validation error
        collection = client.create_collection(name=name, metadata=metadata)
        logger.info(f"‚ú® Collection '{name}' created successfully")
        return f"Collection '{name}' created successfully"


@mcp.tool()
@handle_mcp_tool_errors("delete_collection")
async def delete_collection(name: str) -> str:
    """Delete a ChromaDB collection."""
    if not name or not name.strip():
        raise ValueError("Collection name cannot be empty")

    name = name.strip()
    client = get_chroma_client()

    with OperationContext("delete_collection", collection_name=name):
        client.delete_collection(name)
        logger.info(f"üóëÔ∏è Collection '{name}' deleted successfully")
        return f"Collection '{name}' deleted successfully"


@mcp.tool()
@handle_mcp_tool_errors("add_documents")
async def add_documents(
    collection_name: str,
    documents: List[str],
    ids: Optional[List[str]] = None,
    metadatas: Optional[List[Dict[str, Any]]] = None,
) -> str:
    """Add documents to a ChromaDB collection with custom embeddings."""
    if not documents:
        raise ValueError("Documents list cannot be empty")

    if not collection_name or not collection_name.strip():
        raise ValueError("Collection name cannot be empty")

    collection_name = collection_name.strip()

    # Clean and validate documents
    clean_documents = []
    for i, doc in enumerate(documents):
        if not isinstance(doc, str):
            logger.warning(f"Document {i} converted from {type(doc)} to string")
            doc = str(doc)
        doc = doc.strip()
        if doc:  # Only add non-empty documents
            clean_documents.append(doc)

    if not clean_documents:
        raise ValueError("No valid documents after cleaning")

    # Generate IDs if not provided
    if ids is None:
        ids = [f"doc_{i}" for i in range(len(clean_documents))]

    client = get_chroma_client()
    embedding_manager = get_embedding_manager()

    with OperationContext(
        "add_documents", collection_name=collection_name, doc_count=len(clean_documents)
    ):

        collection = client.get_or_create_collection(
            collection_name
        )  # Force use custom embeddings if model is available
        embeddings = None
        model_name = "ChromaDB default"

        if embedding_manager.current_model is not None:
            try:
                with OperationContext(
                    "generate_embeddings", doc_count=len(clean_documents)
                ):
                    # Force batch optimization to False to use direct model encoding
                    embeddings = embedding_manager.encode_documents(
                        clean_documents, normalize=True, use_batch_optimization=False
                    )

                model_info = embedding_manager.get_model_info()
                model_name = model_info.get("name", "unknown")
                logger.info(f"‚ú® Using custom embeddings: {model_name}")

            except Exception as e:
                logger.warning(
                    f"‚ö†Ô∏è Custom embedding failed, falling back to ChromaDB default: {e}"
                )
                # Set embeddings to None to use ChromaDB default
                embeddings = None

        # Add documents with or without custom embeddings
        if embeddings is not None:
            collection.add(
                documents=clean_documents,
                embeddings=embeddings,  # type: ignore
                ids=ids[: len(clean_documents)],
                metadatas=metadatas[: len(clean_documents)] if metadatas else None,  # type: ignore
            )
        else:
            collection.add(
                documents=clean_documents,
                ids=ids[: len(clean_documents)],
                metadatas=metadatas[: len(clean_documents)] if metadatas else None,  # type: ignore
            )

        result_message = f"Added {len(clean_documents)} documents to '{collection_name}' using {model_name} embeddings"
        logger.info(f"üìÑ {result_message}")
        return result_message


@mcp.tool()
@handle_mcp_tool_errors("query_collection")
async def query_collection(
    collection_name: str,
    query_texts: List[str],
    n_results: int = 10,
    where: Optional[Dict[str, Any]] = None,
) -> Any:
    """Query a ChromaDB collection with semantic search."""
    if not query_texts:
        raise ValueError("Query texts cannot be empty")

    if not collection_name or not collection_name.strip():
        raise ValueError("Collection name cannot be empty")

    if n_results <= 0:
        raise ValueError("n_results must be greater than 0")

    collection_name = collection_name.strip()
    client = get_chroma_client()
    embedding_manager = get_embedding_manager()

    with OperationContext(
        "query_collection",
        collection_name=collection_name,
        query_count=len(query_texts),
        n_results=n_results,
    ):

        collection = client.get_collection(
            collection_name
        )  # Force use custom query embeddings if available
        query_embeddings = None
        model_name = "ChromaDB default"

        if embedding_manager.current_model is not None and len(query_texts) > 0:
            try:
                with OperationContext(
                    "generate_query_embeddings", query_count=len(query_texts)
                ):
                    # Convert to list for encoding
                    embeddings_list = []
                    for query_text in query_texts:
                        # Force use direct encoding without cache for testing
                        embedding = embedding_manager.encode_query(
                            query_text, normalize=True, use_cache=False
                        )
                        if embedding:
                            embeddings_list.append(embedding)

                    if embeddings_list:
                        query_embeddings = embeddings_list
                        model_info = embedding_manager.get_model_info()
                        model_name = model_info.get("name", "unknown")
                        logger.info(f"‚ú® Using custom query embeddings: {model_name}")
                    else:
                        logger.warning("‚ö†Ô∏è No valid query embeddings generated")

            except Exception as e:
                logger.warning(
                    f"‚ö†Ô∏è Custom query embedding failed, falling back to ChromaDB default: {e}"
                )
                query_embeddings = None

        # Perform query
        if query_embeddings:
            results = collection.query(
                query_embeddings=query_embeddings,  # type: ignore
                n_results=n_results,
                where=where,
            )
        else:
            results = collection.query(
                query_texts=query_texts,
                n_results=n_results,
                where=where,
            )  # Log results summary
        result_dict = dict(results)  # type: ignore
        total_results = 0
        if (
            result_dict.get("documents")
            and isinstance(result_dict["documents"], list)
            and len(result_dict["documents"]) > 0
        ):
            total_results = len(result_dict["documents"][0])
        logger.info(f"üîç Query returned {total_results} results using {model_name}")

        return result_dict


##### Embedding Management Tools #####


@mcp.tool()
@handle_mcp_tool_errors("get_embedding_model_info")
async def get_embedding_model_info() -> Dict[str, Any]:
    """Get information about current embedding model."""
    embedding_manager = get_embedding_manager()
    model_info = embedding_manager.get_model_info()
    if model_info:
        logger.info(f"üìä Current model: {model_info.get('name', 'unknown')}")
        return model_info
    else:
        logger.warning("‚ö†Ô∏è No model currently loaded")
        return {"error": "No model currently loaded"}


@mcp.tool()
@handle_mcp_tool_errors("configure_embedding_model")
async def configure_embedding_model(model_name: str, force_reload: bool = False) -> str:
    """Configure the embedding model to use."""
    if not model_name or not model_name.strip():
        raise ValueError("Model name cannot be empty")

    model_name = model_name.strip()
    embedding_manager = get_embedding_manager()

    with OperationContext(
        "configure_embedding_model", model_name=model_name, force_reload=force_reload
    ):

        success = embedding_manager.load_model(model_name, force_reload)
        if success:
            model_info = embedding_manager.get_model_info()
            if model_info:
                model_name_info = model_info.get("name", model_name)
                dimension = model_info.get("embedding_dim", "unknown")
                result_message = f"Successfully configured embedding model: {model_name_info} (dim: {dimension})"
                logger.info(f"üéØ {result_message}")
                return result_message
            else:
                result_message = f"Model loaded but info unavailable: {model_name}"
                logger.warning(f"‚ö†Ô∏è {result_message}")
                return result_message
        else:
            error_message = (
                f"Failed to load model '{model_name}'. Using ChromaDB default."
            )
            logger.error(f"‚ùå {error_message}")
            raise Exception(error_message)


@mcp.tool()
@handle_mcp_tool_errors("chunk_text_intelligent")
async def chunk_text_intelligent(
    text: str, chunk_size: int = 400, overlap: int = 50
) -> List[str]:
    """Intelligently chunk text for better embedding (Vietnamese optimized)."""
    if not text or not text.strip():
        raise ValueError("Text cannot be empty")

    if chunk_size <= 0:
        raise ValueError("Chunk size must be greater than 0")

    if overlap < 0:
        raise ValueError("Overlap cannot be negative")

    if overlap >= chunk_size:
        raise ValueError("Overlap must be less than chunk size")

    # Validate input
    if not isinstance(text, str):
        text = str(text)

    text = text.strip()

    with OperationContext(
        "chunk_text_intelligent",
        text_length=len(text),
        chunk_size=chunk_size,
        overlap=overlap,
    ):

        # Ensure valid encoding
        try:
            text.encode("utf-8")
        except UnicodeEncodeError:
            text = text.encode("utf-8", errors="ignore").decode("utf-8")

        chunks = _chunk_text_intelligent(text, chunk_size, overlap)

        # Validate and clean output chunks
        cleaned_chunks = []
        for chunk in chunks:
            if isinstance(chunk, str) and chunk.strip():
                try:
                    chunk.encode("utf-8")
                    cleaned_chunks.append(chunk.strip())
                except UnicodeEncodeError:
                    cleaned_chunk = chunk.encode("utf-8", errors="ignore").decode(
                        "utf-8"
                    )
                    if cleaned_chunk.strip():
                        cleaned_chunks.append(cleaned_chunk.strip())

        logger.info(f"üìù Text chunked into {len(cleaned_chunks)} chunks")
        return cleaned_chunks


@mcp.tool()
@handle_mcp_tool_errors("get_performance_metrics")
async def get_performance_metrics() -> Any:
    """Get performance metrics for embedding operations."""
    embedding_manager = get_embedding_manager()

    with OperationContext("get_performance_metrics"):
        metrics = embedding_manager.get_metrics()

        logger.info("üìä Retrieved performance metrics")
        return {
            "message": "Performance metrics retrieved successfully",
            "metrics": metrics,
        }


@mcp.tool()
@handle_mcp_tool_errors("clear_embedding_cache")
async def clear_embedding_cache() -> str:
    """Clear the embedding cache to free memory."""
    embedding_manager = get_embedding_manager()

    with OperationContext("clear_embedding_cache"):
        cache_stats = embedding_manager.clear_embedding_cache()

        if cache_stats is not None:
            message = f"Embedding cache cleared: {cache_stats['cleared_entries']} entries, freed {cache_stats['freed_memory_mb']} MB"
            logger.info(f"üßπ {message}")
            return message
        else:
            message = (
                "No embedding cache available to clear (batch processing not enabled)"
            )
            logger.info(f"‚ÑπÔ∏è {message}")
            return message


@mcp.tool()
@handle_mcp_tool_errors("get_cache_stats")
async def get_cache_stats() -> Any:
    """Get embedding cache statistics."""
    embedding_manager = get_embedding_manager()

    with OperationContext("get_cache_stats"):
        cache_stats = embedding_manager.get_cache_stats()

        if cache_stats is not None:
            logger.info("üìä Retrieved cache statistics")
            return {
                "message": "Cache statistics retrieved successfully",
                "cache_stats": cache_stats,
            }
        else:
            return {
                "message": "No cache available (batch processing not enabled)",
                "cache_stats": None,
            }


@mcp.tool()
@handle_mcp_tool_errors("batch_process_documents")
async def batch_process_documents(
    texts: List[str], normalize: bool = True, show_progress: bool = True
) -> Any:
    """Process multiple documents with optimized batch processing."""
    if not texts:
        raise ValueError("Documents list cannot be empty")

    embedding_manager = get_embedding_manager()

    with OperationContext("batch_process_documents", doc_count=len(texts)):

        if embedding_manager.batch_processor is None:
            raise ValueError(
                "Batch processing not available - no embedding model loaded"
            )

        logger.info(f"üöÄ Starting batch processing for {len(texts)} documents")
        # Use the optimized batch processor directly
        embeddings = await embedding_manager.batch_processor.encode_documents_optimized(
            texts, normalize, show_progress
        )

        if embeddings is None:
            raise RuntimeError("Batch processing failed")

        result_message = (
            f"Successfully processed {len(texts)} documents with batch optimization"
        )
        logger.info(f"‚úÖ {result_message}")

        return {
            "message": result_message,
            "document_count": len(texts),
            "embedding_dimensions": len(embeddings[0]) if embeddings else 0,
            "processing_optimized": True,
        }
