# Generated by Copilot
"""
Intelligent text chunking for optimal embedding generation.

This module provides advanced text chunking strategies optimized for Vietnamese text
and embedding models. It focuses on semantic coherence and embedding quality.

Features:
- Multiple chunking strategies (fixed, semantic, sentence-based)
- Vietnamese text optimization with proper tokenization
- Adaptive chunk sizing based on content type
- Overlap handling for context preservation
- Quality metrics for chunk assessment
"""

import re
import logging
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass
from enum import Enum
import unicodedata

# Vietnamese text processing - enhanced regex approach
HAS_UNDERTHESEA = False  # Fallback to enhanced regex due to build issues

logger = logging.getLogger(__name__)

# Vietnamese-specific text patterns
VIETNAMESE_SENTENCE_PATTERNS = [
    r"[.!?]+\s+[A-Z√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√ç√å·ªàƒ®·ªä√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥ƒê]",
    r"[.!?]+\s+[0-9]",
    r"[.!?]+$",
]

VIETNAMESE_PUNCTUATION = '.,;:!?()[]{}""' '""‚Ä¶‚Äì‚Äî'
VIETNAMESE_STOP_WORDS = {
    "v√†",
    "c·ªßa",
    "trong",
    "v·ªõi",
    "cho",
    "t·ª´",
    "ƒë·ªÉ",
    "v·ªÅ",
    "c√≥",
    "ƒë∆∞·ª£c",
    "n√†y",
    "ƒë√≥",
    "l√†",
    "m·ªôt",
    "c√°c",
    "nh·ªØng",
    "khi",
    "nh∆∞",
    "theo",
    "tr√™n",
    "d∆∞·ªõi",
    "sau",
    "tr∆∞·ªõc",
}


class ChunkingStrategy(Enum):
    """Available text chunking strategies."""

    FIXED = "fixed"
    SEMANTIC = "semantic"
    SENTENCE = "sentence"
    PARAGRAPH = "paragraph"
    INTELLIGENT = "intelligent"


@dataclass
class ChunkInfo:
    """Information about a text chunk."""

    text: str
    start_pos: int
    end_pos: int
    chunk_id: int
    word_count: int
    char_count: int
    overlap_prev: int = 0
    overlap_next: int = 0
    quality_score: float = 0.0
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class ChunkingConfig:
    """Configuration for text chunking."""

    strategy: ChunkingStrategy = ChunkingStrategy.INTELLIGENT
    max_chunk_size: int = 400
    min_chunk_size: int = 50
    overlap_size: int = 50
    preserve_sentences: bool = True
    preserve_paragraphs: bool = True
    optimize_for_vietnamese: bool = True
    quality_threshold: float = 0.7


class TextChunker:
    """
    Advanced text chunker with multiple strategies and Vietnamese optimization.

    Provides intelligent chunking that maintains semantic coherence while
    optimizing for embedding model performance.
    """

    def __init__(self, config: Optional[ChunkingConfig] = None) -> None:
        """
        Initialize text chunker.

        Args:
            config: Optional chunking configuration
        """
        self.config = config or ChunkingConfig()
        self.chunk_cache: Dict[str, List[ChunkInfo]] = {}
        logger.debug(
            f"üîß TextChunker initialized with strategy: {self.config.strategy}"
        )

    def chunk_text(
        self, text: str, strategy: Optional[ChunkingStrategy] = None
    ) -> List[ChunkInfo]:
        """
        Chunk text using specified or default strategy.

        Args:
            text: Text to chunk
            strategy: Optional chunking strategy override

        Returns:
            List of ChunkInfo objects
        """
        if not text or not text.strip():
            logger.warning("‚ö†Ô∏è Empty text provided for chunking")
            return []

        strategy = strategy or self.config.strategy
        cache_key = f"{hash(text)}_{strategy.value}_{self.config.max_chunk_size}"

        if cache_key in self.chunk_cache:
            logger.debug(f"üì¶ Using cached chunks for text (hash: {hash(text)})")
            return self.chunk_cache[cache_key]

        # Clean and normalize text
        cleaned_text = self._preprocess_text(text)

        # Apply chunking strategy
        if strategy == ChunkingStrategy.FIXED:
            chunks = self._chunk_fixed(cleaned_text)
        elif strategy == ChunkingStrategy.SEMANTIC:
            chunks = self._chunk_semantic(cleaned_text)
        elif strategy == ChunkingStrategy.SENTENCE:
            chunks = self._chunk_by_sentences(cleaned_text)
        elif strategy == ChunkingStrategy.PARAGRAPH:
            chunks = self._chunk_by_paragraphs(cleaned_text)
        elif strategy == ChunkingStrategy.INTELLIGENT:
            chunks = self._chunk_intelligent(cleaned_text)
        else:
            logger.warning(
                f"‚ö†Ô∏è Unknown chunking strategy: {strategy}, falling back to intelligent"
            )
            chunks = self._chunk_intelligent(cleaned_text)

        # Calculate quality scores
        chunks = self._calculate_quality_scores(chunks)

        # Cache results
        self.chunk_cache[cache_key] = chunks

        logger.info(
            f"‚úÖ Chunked text into {len(chunks)} chunks using {strategy.value} strategy"
        )
        return chunks

    def _preprocess_text(self, text: str) -> str:
        """
        Preprocess text for chunking.

        Args:
            text: Raw text to preprocess

        Returns:
            Cleaned and normalized text
        """
        # Normalize Unicode (important for Vietnamese)
        text = unicodedata.normalize("NFC", text)

        # Remove excessive whitespace
        text = re.sub(r"\s+", " ", text)

        # Fix common Vietnamese text issues
        if self.config.optimize_for_vietnamese:
            text = self._fix_vietnamese_text(text)

        return text.strip()

    def _fix_vietnamese_text(self, text: str) -> str:
        """
        Fix common Vietnamese text formatting issues.

        Args:
            text: Text to fix

        Returns:
            Fixed text
        """
        # Fix spacing around punctuation
        text = re.sub(r"\s+([.,;:!?])", r"\1", text)
        text = re.sub(
            r"([.,;:!?])\s*([a-z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë])",
            r"\1 \2",
            text,
            flags=re.IGNORECASE,
        )

        # Fix quotation marks
        text = re.sub(r'"([^"]*)"', r'"\1"', text)

        # Fix dash usage
        text = re.sub(r"\s*-\s*", " - ", text)

        return text

    def _chunk_fixed(self, text: str) -> List[ChunkInfo]:
        """
        Chunk text using fixed-size strategy.

        Args:
            text: Text to chunk

        Returns:
            List of fixed-size chunks
        """
        chunks = []
        words = text.split()

        if not words:
            return chunks

        current_chunk = []
        current_size = 0
        chunk_id = 0

        for word in words:
            if (
                current_size + len(word) + 1 > self.config.max_chunk_size
                and current_chunk
            ):
                # Create chunk
                chunk_text = " ".join(current_chunk)
                chunk_info = ChunkInfo(
                    text=chunk_text,
                    start_pos=0,  # Will be calculated later
                    end_pos=len(chunk_text),
                    chunk_id=chunk_id,
                    word_count=len(current_chunk),
                    char_count=len(chunk_text),
                )
                chunks.append(chunk_info)

                # Handle overlap
                if self.config.overlap_size > 0:
                    overlap_words = current_chunk[-self.config.overlap_size :]
                    current_chunk = overlap_words + [word]
                    current_size = (
                        sum(len(w) for w in current_chunk) + len(current_chunk) - 1
                    )
                else:
                    current_chunk = [word]
                    current_size = len(word)

                chunk_id += 1
            else:
                current_chunk.append(word)
                current_size += len(word) + (1 if current_chunk else 0)

        # Add final chunk
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            chunk_info = ChunkInfo(
                text=chunk_text,
                start_pos=0,
                end_pos=len(chunk_text),
                chunk_id=chunk_id,
                word_count=len(current_chunk),
                char_count=len(chunk_text),
            )
            chunks.append(chunk_info)

        return chunks

    def _chunk_by_sentences(self, text: str) -> List[ChunkInfo]:
        """
        Chunk text by sentences, respecting max chunk size.

        Args:
            text: Text to chunk

        Returns:
            List of sentence-based chunks
        """
        # Split by sentences (Vietnamese-optimized)
        sentences = self._split_sentences(text)

        if not sentences:
            return []

        chunks = []
        current_chunk = []
        current_size = 0
        chunk_id = 0

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            sentence_size = len(sentence)

            if (
                current_size + sentence_size > self.config.max_chunk_size
                and current_chunk
            ):
                # Create chunk from current sentences
                chunk_text = " ".join(current_chunk)
                chunk_info = ChunkInfo(
                    text=chunk_text,
                    start_pos=0,
                    end_pos=len(chunk_text),
                    chunk_id=chunk_id,
                    word_count=len(chunk_text.split()),
                    char_count=len(chunk_text),
                )
                chunks.append(chunk_info)

                current_chunk = [sentence]
                current_size = sentence_size
                chunk_id += 1
            else:
                current_chunk.append(sentence)
                current_size += sentence_size + (1 if len(current_chunk) > 1 else 0)

        # Add final chunk
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            chunk_info = ChunkInfo(
                text=chunk_text,
                start_pos=0,
                end_pos=len(chunk_text),
                chunk_id=chunk_id,
                word_count=len(chunk_text.split()),
                char_count=len(chunk_text),
            )
            chunks.append(chunk_info)

        return chunks

    def _chunk_by_paragraphs(self, text: str) -> List[ChunkInfo]:
        """
        Chunk text by paragraphs, splitting large paragraphs if needed.

        Args:
            text: Text to chunk

        Returns:
            List of paragraph-based chunks
        """
        paragraphs = text.split("\n\n")
        chunks = []
        chunk_id = 0

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            if len(paragraph) <= self.config.max_chunk_size:
                # Paragraph fits in one chunk
                chunk_info = ChunkInfo(
                    text=paragraph,
                    start_pos=0,
                    end_pos=len(paragraph),
                    chunk_id=chunk_id,
                    word_count=len(paragraph.split()),
                    char_count=len(paragraph),
                )
                chunks.append(chunk_info)
                chunk_id += 1
            else:
                # Split large paragraph
                para_chunks = self._chunk_by_sentences(paragraph)
                for chunk in para_chunks:
                    chunk.chunk_id = chunk_id
                    chunk_id += 1
                chunks.extend(para_chunks)

        return chunks

    def _chunk_semantic(self, text: str) -> List[ChunkInfo]:
        """
        Chunk text using semantic analysis (basic implementation).

        Args:
            text: Text to chunk

        Returns:
            List of semantically coherent chunks
        """
        # For now, use sentence-based chunking with semantic hints
        # Could be enhanced with actual semantic analysis
        sentences = self._split_sentences(text)

        if not sentences:
            return []

        chunks = []
        current_chunk = []
        current_size = 0
        chunk_id = 0

        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue

            sentence_size = len(sentence)

            # Simple semantic hint: if sentence starts with certain words, prefer to start new chunk
            semantic_break = self._is_semantic_break(sentence, i > 0)

            if (
                current_size + sentence_size > self.config.max_chunk_size
                and current_chunk
            ) or (semantic_break and current_size > self.config.min_chunk_size):
                # Create chunk
                chunk_text = " ".join(current_chunk)
                chunk_info = ChunkInfo(
                    text=chunk_text,
                    start_pos=0,
                    end_pos=len(chunk_text),
                    chunk_id=chunk_id,
                    word_count=len(chunk_text.split()),
                    char_count=len(chunk_text),
                )
                chunks.append(chunk_info)

                current_chunk = [sentence]
                current_size = sentence_size
                chunk_id += 1
            else:
                current_chunk.append(sentence)
                current_size += sentence_size + (1 if len(current_chunk) > 1 else 0)

        # Add final chunk
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            chunk_info = ChunkInfo(
                text=chunk_text,
                start_pos=0,
                end_pos=len(chunk_text),
                chunk_id=chunk_id,
                word_count=len(chunk_text.split()),
                char_count=len(chunk_text),
            )
            chunks.append(chunk_info)

        return chunks

    def _chunk_intelligent(self, text: str) -> List[ChunkInfo]:
        """
        Intelligent chunking combining multiple strategies.

        Args:
            text: Text to chunk

        Returns:
            List of intelligently chunked pieces
        """
        # Start with paragraph-based chunking
        para_chunks = self._chunk_by_paragraphs(text)

        # Refine with sentence analysis
        refined_chunks = []
        for chunk in para_chunks:
            if chunk.char_count > self.config.max_chunk_size:
                # Re-chunk large chunks using sentences
                sub_chunks = self._chunk_by_sentences(chunk.text)
                refined_chunks.extend(sub_chunks)
            elif chunk.char_count < self.config.min_chunk_size:
                # Try to merge small chunks
                if (
                    refined_chunks
                    and refined_chunks[-1].char_count + chunk.char_count
                    <= self.config.max_chunk_size
                ):
                    # Merge with previous chunk
                    prev_chunk = refined_chunks[-1]
                    merged_text = prev_chunk.text + " " + chunk.text
                    merged_chunk = ChunkInfo(
                        text=merged_text,
                        start_pos=prev_chunk.start_pos,
                        end_pos=prev_chunk.end_pos + chunk.char_count + 1,
                        chunk_id=prev_chunk.chunk_id,
                        word_count=prev_chunk.word_count + chunk.word_count,
                        char_count=len(merged_text),
                    )
                    refined_chunks[-1] = merged_chunk
                else:
                    refined_chunks.append(chunk)
            else:
                refined_chunks.append(chunk)

        # Re-number chunks
        for i, chunk in enumerate(refined_chunks):
            chunk.chunk_id = i

        return refined_chunks

    def _split_sentences(self, text: str) -> List[str]:
        """
        Split text into sentences using enhanced Vietnamese-optimized patterns.
        Uses improved regex-based approach for accurate Vietnamese sentence tokenization.

        Args:
            text: Text to split

        Returns:
            List of sentences
        """
        if not text or not text.strip():
            return []

        # Enhanced Vietnamese sentence splitting
        if self.config.optimize_for_vietnamese:
            return self._split_vietnamese_sentences(text)
        else:
            # Simple split for non-Vietnamese text
            sentences = re.split(r"[.!?]+\s+", text)
            return [s.strip() for s in sentences if s.strip() and len(s.strip()) > 3]

    def _split_vietnamese_sentences(self, text: str) -> List[str]:
        """
        Enhanced Vietnamese sentence splitting with better patterns.
        
        Args:
            text: Vietnamese text to split
            
        Returns:
            List of sentences
        """
        # Normalize text first
        text = unicodedata.normalize('NFC', text)
        
        # Enhanced Vietnamese sentence patterns
        # Pattern 1: Standard punctuation followed by capital letter or number
        pattern1 = r'([.!?]+)\s+([A-Z√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√ç√å·ªàƒ®·ªä√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥ƒê])'
        
        # Pattern 2: End of sentences
        pattern2 = r'([.!?]+)(\s*$)'
        
        # Pattern 3: Numbering (1. 2. etc.)
        pattern3 = r'([.!?]+)\s+(\d+\.)'
        
        # Common Vietnamese discourse markers
        discourse_markers = [
            'Tuy nhi√™n', 'Nh∆∞ng', 'M·∫∑t kh√°c', 'B√™n c·∫°nh ƒë√≥', 'Ngo√†i ra',
            'ƒê·∫ßu ti√™n', 'Th·ª© hai', 'Th·ª© ba', 'Cu·ªëi c√πng', 'K·∫øt lu·∫≠n',
            'T√≥m l·∫°i', 'V√≠ d·ª•', 'Ch·∫≥ng h·∫°n', 'C·ª• th·ªÉ', 'Theo ƒë√≥', 'Do ƒë√≥'
        ]
        
        # Apply main patterns with proper group references
        # Add sentence breaks before discourse markers
        for marker in discourse_markers:
            marker_pattern = rf'([.!?]+)\s+({re.escape(marker)})'
            text = re.sub(marker_pattern, r'\1|SENT_BREAK|\2', text)
        
        # Apply main sentence patterns
        text = re.sub(pattern1, r'\1|SENT_BREAK|\2', text)
        text = re.sub(pattern2, r'\1|SENT_BREAK|', text)  # No group 2 for end pattern
        text = re.sub(pattern3, r'\1|SENT_BREAK|\2', text)
        
        # Split and clean
        parts = text.split('|SENT_BREAK|')
        sentences = []
        current_sentence = ""
        
        for part in parts:
            part = part.strip()
            if not part:
                continue
                
            # Accumulate very short fragments with previous sentence
            if len(part) < 15 and current_sentence and not any(p in part for p in ['.', '!', '?']):
                current_sentence += " " + part
            else:
                if current_sentence:
                    sentences.append(current_sentence.strip())
                current_sentence = part
        
        # Add the last sentence
        if current_sentence and current_sentence.strip():
            sentences.append(current_sentence.strip())
        
        # Final filtering and cleaning
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            # Remove very short fragments and ensure meaningful content
            if sentence and len(sentence) > 10 and any(c.isalpha() for c in sentence):
                cleaned_sentences.append(sentence)
        
        logger.debug(f"üáªüá≥ Enhanced Vietnamese sentence splitting: {len(cleaned_sentences)} sentences")
        return cleaned_sentences

    def _is_semantic_break(self, sentence: str, has_previous: bool) -> bool:
        """
        Determine if sentence represents a semantic break.

        Args:
            sentence: Sentence to analyze
            has_previous: Whether there are previous sentences

        Returns:
            True if this sentence should start a new chunk
        """
        if not has_previous:
            return False

        # Vietnamese semantic break indicators
        break_indicators = [
            "Tuy nhi√™n",
            "Nh∆∞ng",
            "M·∫∑t kh√°c",
            "B√™n c·∫°nh ƒë√≥",
            "Ngo√†i ra",
            "ƒê·∫ßu ti√™n",
            "Th·ª© hai",
            "Cu·ªëi c√πng",
            "K·∫øt lu·∫≠n",
            "T√≥m l·∫°i",
            "V√≠ d·ª•",
            "Ch·∫≥ng h·∫°n",
            "C·ª• th·ªÉ",
            "Theo ƒë√≥",
            "Do ƒë√≥",
        ]

        sentence_start = sentence[:50].lower()
        return any(
            indicator.lower() in sentence_start for indicator in break_indicators
        )

    def _calculate_quality_scores(self, chunks: List[ChunkInfo]) -> List[ChunkInfo]:
        """
        Calculate quality scores for chunks.

        Args:
            chunks: List of chunks to score

        Returns:
            Chunks with quality scores
        """
        for chunk in chunks:
            score = 0.0

            # Size score (prefer chunks near optimal size)
            optimal_size = self.config.max_chunk_size * 0.7
            size_ratio = min(chunk.char_count / optimal_size, 1.0)
            score += size_ratio * 0.3

            # Completeness score (prefer complete sentences/paragraphs)
            if chunk.text.endswith((".", "!", "?")):
                score += 0.2

            # Coherence score (basic word repetition analysis)
            words = chunk.text.lower().split()
            unique_words = set(words) - VIETNAMESE_STOP_WORDS
            if words:
                coherence = len(unique_words) / len(words)
                score += coherence * 0.3

            # Readability score (sentence length variance)
            sentences = self._split_sentences(chunk.text)
            if sentences:
                lengths = [len(s.split()) for s in sentences]
                if len(lengths) > 1:
                    avg_length = sum(lengths) / len(lengths)
                    variance = sum((l - avg_length) ** 2 for l in lengths) / len(
                        lengths
                    )
                    readability = 1.0 / (1.0 + variance / 100)  # Normalize variance
                    score += readability * 0.2

            chunk.quality_score = min(score, 1.0)

        return chunks

    def get_chunking_stats(self) -> Dict[str, Any]:
        """Get chunking statistics."""
        total_cached = len(self.chunk_cache)
        total_chunks = sum(len(chunks) for chunks in self.chunk_cache.values())

        return {
            "cached_texts": total_cached,
            "total_chunks_generated": total_chunks,
            "average_chunks_per_text": total_chunks / max(total_cached, 1),
            "config": {
                "strategy": self.config.strategy.value,
                "max_chunk_size": self.config.max_chunk_size,
                "min_chunk_size": self.config.min_chunk_size,
                "overlap_size": self.config.overlap_size,
            },
        }


# Convenience functions
def chunk_text_intelligent(
    text: str, max_chunk_size: int = 400, overlap: int = 50
) -> List[str]:
    """
    Quick intelligent chunking function.

    Args:
        text: Text to chunk
        max_chunk_size: Maximum chunk size in characters
        overlap: Overlap size in characters

    Returns:
        List of chunk texts
    """
    config = ChunkingConfig(
        strategy=ChunkingStrategy.INTELLIGENT,
        max_chunk_size=max_chunk_size,
        overlap_size=overlap,
    )
    chunker = TextChunker(config)
    chunks = chunker.chunk_text(text)
    return [chunk.text for chunk in chunks]


def chunk_text_semantic(text: str, max_chunk_size: int = 400) -> List[str]:
    """
    Quick semantic chunking function.

    Args:
        text: Text to chunk
        max_chunk_size: Maximum chunk size in characters

    Returns:
        List of chunk texts
    """
    config = ChunkingConfig(
        strategy=ChunkingStrategy.SEMANTIC, max_chunk_size=max_chunk_size
    )
    chunker = TextChunker(config)
    chunks = chunker.chunk_text(text)
    return [chunk.text for chunk in chunks]


def optimize_chunk_size(text: str, target_chunks: int) -> int:
    """
    Optimize chunk size for target number of chunks.

    Args:
        text: Sample text to analyze
        target_chunks: Target number of chunks

    Returns:
        Recommended chunk size
    """
    text_length = len(text)
    base_size = text_length // target_chunks

    # Test different sizes around the base
    sizes_to_test = [
        int(base_size * 0.8),
        base_size,
        int(base_size * 1.2),
        int(base_size * 1.5),
    ]

    best_size = base_size
    best_score = float("inf")

    for size in sizes_to_test:
        if size < 50:  # Minimum reasonable size
            continue

        chunker = TextChunker(ChunkingConfig(max_chunk_size=size))
        chunks = chunker.chunk_text(text)

        # Score based on how close we are to target and chunk quality
        chunk_count_diff = abs(len(chunks) - target_chunks)
        avg_quality = (
            sum(chunk.quality_score for chunk in chunks) / len(chunks) if chunks else 0
        )

        score = chunk_count_diff * 10 + (1.0 - avg_quality) * 5

        if score < best_score:
            best_score = score
            best_size = size

    logger.info(
        f"üéØ Optimized chunk size: {best_size} (targeting {target_chunks} chunks)"
    )
    return best_size


# Export symbols
__all__ = [
    "ChunkingStrategy",
    "ChunkInfo",
    "ChunkingConfig",
    "TextChunker",
    "chunk_text_intelligent",
    "chunk_text_semantic",
    "optimize_chunk_size",
]
