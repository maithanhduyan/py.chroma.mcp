# Generated by Copilot
"""
Intelligent text chunking for optimal embedding generation.
Optimized for Vietnamese text and embedding models.
"""

import re
from typing import List, Dict, Any, Optional
import unicodedata

from utils.logger import get_logger

logger = get_logger(__name__)

# Vietnamese-specific text patterns
VIETNAMESE_SENTENCE_PATTERNS = [
    r"[.!?]+\s+[A-ZÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴĐ]",
    r"[.!?]+\s+[0-9]",
    r"[.!?]+$",
]

VIETNAMESE_PUNCTUATION = '.,;:!?()[]{}""' '""…–—'
VIETNAMESE_STOP_WORDS = {
    "và", "của", "trong", "với", "cho", "từ", "để", "về", "có", "được",
    "này", "đó", "là", "một", "các", "những", "khi", "như", "theo",
    "trên", "dưới", "sau", "trước",
}


def normalize_vietnamese_text(text: str) -> str:
    """Normalize Vietnamese text using NFC normalization."""
    return unicodedata.normalize("NFC", text)


def split_sentences_vietnamese(text: str) -> List[str]:
    """Split text into sentences with Vietnamese-specific patterns."""
    sentences = []
    current_sentence = ""
    
    for pattern in VIETNAMESE_SENTENCE_PATTERNS:
        if re.search(pattern, text):
            parts = re.split(pattern, text)
            for part in parts:
                if part.strip():
                    sentences.append(part.strip())
            break
    
    if not sentences:
        # Fallback to simple sentence splitting
        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
    
    return sentences


def calculate_chunk_quality(text: str) -> float:
    """Calculate quality score for a text chunk."""
    if not text or len(text.strip()) < 10:
        return 0.0
    
    # Basic quality metrics
    word_count = len(text.split())
    char_count = len(text)
    
    # Sentence completeness
    ends_with_punctuation = text.strip()[-1] in '.!?'
    
    # Length appropriateness (prefer chunks around 300-500 chars)
    length_score = 1.0 - abs(char_count - 400) / 400
    length_score = max(0.0, min(1.0, length_score))
    
    # Basic quality score
    quality = (length_score * 0.6 + 
               (0.4 if ends_with_punctuation else 0.2) + 
               min(word_count / 50, 0.3))
    
    return min(1.0, quality)


def chunk_text_intelligent(text: str, max_chunk_size: int = 400, overlap: int = 50) -> List[str]:
    """
    Intelligent text chunking optimized for Vietnamese text and embeddings.
    
    Args:
        text: Text to chunk
        max_chunk_size: Maximum chunk size in characters
        overlap: Overlap size in characters
    
    Returns:
        List of chunk texts
    """
    if not text or not text.strip():
        return []
    
    # Normalize text
    text = normalize_vietnamese_text(text.strip())
    
    # If text is shorter than max_chunk_size, return as single chunk
    if len(text) <= max_chunk_size:
        return [text]
    
    chunks = []
    
    try:
        # Try sentence-based chunking first
        sentences = split_sentences_vietnamese(text)
        
        if len(sentences) > 1:
            current_chunk = ""
            
            for sentence in sentences:
                # If adding this sentence would exceed max_chunk_size
                if len(current_chunk) + len(sentence) + 1 > max_chunk_size:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                        # Add overlap for context preservation
                        if overlap > 0 and len(current_chunk) > overlap:
                            current_chunk = current_chunk[-overlap:] + " " + sentence
                        else:
                            current_chunk = sentence
                    else:
                        # Single sentence too long, split it
                        if len(sentence) > max_chunk_size:
                            # Split long sentence by words
                            words = sentence.split()
                            temp_chunk = ""
                            for word in words:
                                if len(temp_chunk) + len(word) + 1 <= max_chunk_size:
                                    temp_chunk += (" " if temp_chunk else "") + word
                                else:
                                    if temp_chunk:
                                        chunks.append(temp_chunk.strip())
                                    temp_chunk = word
                            if temp_chunk:
                                current_chunk = temp_chunk
                        else:
                            current_chunk = sentence
                else:
                    current_chunk += (" " if current_chunk else "") + sentence
            
            # Add remaining chunk
            if current_chunk and current_chunk.strip():
                chunks.append(current_chunk.strip())
        
        else:
            # Single sentence or no sentence boundaries found
            # Fall back to word-based chunking
            words = text.split()
            current_chunk = ""
            
            for word in words:
                if len(current_chunk) + len(word) + 1 <= max_chunk_size:
                    current_chunk += (" " if current_chunk else "") + word
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                        # Add overlap
                        if overlap > 0:
                            overlap_words = current_chunk.split()[-min(5, len(current_chunk.split())):]
                            current_chunk = " ".join(overlap_words) + " " + word
                        else:
                            current_chunk = word
                    else:
                        current_chunk = word
            
            if current_chunk and current_chunk.strip():
                chunks.append(current_chunk.strip())
    
    except Exception as e:
        logger.warning(f"Intelligent chunking failed, using simple chunking: {e}")
        # Fallback to simple character-based chunking
        chunks = []
        for i in range(0, len(text), max_chunk_size - overlap):
            chunk = text[i:i + max_chunk_size]
            if chunk.strip():
                chunks.append(chunk.strip())
    
    # Filter out empty chunks and validate
    final_chunks = []
    for chunk in chunks:
        if chunk and chunk.strip() and len(chunk.strip()) >= 10:
            final_chunks.append(chunk.strip())
    
    logger.debug(f"Chunked text of {len(text)} chars into {len(final_chunks)} chunks")
    
    return final_chunks if final_chunks else [text]


# Backward compatibility
def chunk_text_semantic(text: str, max_chunk_size: int = 400) -> List[str]:
    """Semantic chunking (alias for intelligent chunking)."""
    return chunk_text_intelligent(text, max_chunk_size, 50)


def chunk_text_simple(text: str, max_chunk_size: int = 400, overlap: int = 50) -> List[str]:
    """Simple character-based chunking."""
    if not text:
        return []
    
    chunks = []
    for i in range(0, len(text), max_chunk_size - overlap):
        chunk = text[i:i + max_chunk_size].strip()
        if chunk:
            chunks.append(chunk)
    
    return chunks


# Export main function
__all__ = ["chunk_text_intelligent", "chunk_text_semantic", "chunk_text_simple"]
