# Generated by Copilot
"""
Embedding manager for MCP server.
Handles loading and managing different embedding models.
"""

import os
from typing import Dict, List, Optional, Any
import numpy as np

# Import config to setup project paths automatically
import config
from config import get_embedding_config

from utils.metrics import (
    track_execution_time,
    measure_memory_usage,
    MetricsCollector,
)
from utils.logger import get_logger

# Import batch processor for optimization (will be imported when needed)
# from .batch_processor import BatchProcessor

logger = get_logger(__name__)


class EmbeddingManager:
    """
    Manages embedding models for the MCP server.
    Supports model loading, caching, and fallback strategies.
    """

    def __init__(self):
        """Initialize the embedding manager."""
        self.models: Dict[str, Any] = {}  # Cache loaded models
        self.current_model = None
        self.current_model_name = (
            "chromadb-default"  # PRIORITY 1: Environment Variables (cao nh·∫•t)
        )
        env_model = self._get_model_from_env()

        if env_model:
            self.default_model = env_model
            # Use env-based fallback priority
            self.model_priority = self._get_fallback_models_from_env()
        else:
            # PRIORITY 2: Fallback to config.py if env not set
            try:
                embedding_config = get_embedding_config()
                self.default_model = embedding_config["default_model"]
                self.model_priority = embedding_config["fallback_models"]
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Config load failed: {e}, using hardcoded defaults")
                # PRIORITY 3: Hardcoded defaults as final fallback
                self.default_model = "sentence-transformers/all-MiniLM-L6-v2"
                self.model_priority = [
                    "sentence-transformers/all-MiniLM-L6-v2",
                    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                ]

        # Initialize metrics collector
        self.metrics = MetricsCollector()
        # Simple operation counters for immediate metrics
        self.operation_counts = {
            "load_model": 0,
            "encode_documents": 0,
            "encode_query": 0,
        }
        # Initialize batch processor for optimization
        self.batch_processor: Optional[Any] = None

    def load_model(self, model_name: str, force_reload: bool = False) -> bool:
        """
        Load an embedding model.        Args:
            model_name: Name of the model to load
            force_reload: Whether to reload if already cached        Returns:
            True if model loaded successfully
        """
        with track_execution_time("load_model"):
            result = self._do_load_model(model_name, force_reload)
            if result:
                self.operation_counts["load_model"] += 1
            return result

    def _detect_optimal_device(self) -> str:
        """
        Detect the optimal device for running embedding models.

        Returns:
            Device string: "cuda", "mps" (Mac), or "cpu"
        """
        try:
            # Try to import torch to check for GPU availability
            import torch

            if torch.cuda.is_available():
                device = "cuda"
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                logger.info(f"üöÄ CUDA available: {gpu_count} GPU(s) - {gpu_name}")
                return device
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                logger.info("üçé MPS (Apple Silicon) available")
                return "mps"
            else:
                logger.info("üíª Using CPU for embeddings")
                return "cpu"

        except ImportError:
            logger.warning("‚ö†Ô∏è PyTorch not available, using CPU")
            return "cpu"
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Device detection failed: {e}, using CPU")
            return "cpu"

    def _do_load_model(self, model_name: str, force_reload: bool = False) -> bool:
        """Internal method for loading model without metrics tracking."""
        # Check if already loaded and cached
        if not force_reload and model_name in self.models:
            self.current_model = self.models[model_name]
            self.current_model_name = model_name
            logger.info(
                f"‚úÖ Using cached model: {model_name}"
            )  # Initialize batch processor with current model
            self._initialize_batch_processor()
            return True

        try:
            logger.info(f"üîÑ Loading embedding model: {model_name}")
            logger.info("‚è≥ This may take a few minutes for first-time downloads...")

            # Try sentence-transformers first
            try:
                from sentence_transformers import (
                    SentenceTransformer,
                )  # PRIORITY 1: Get device from environment variables

                device = self._get_device_from_env()

                # Load the model with appropriate device and progress indication
                logger.info(f"üì± Loading model on device: {device}")
                logger.info(
                    "üì• Downloading model files (if not cached)..."
                )  # PRIORITY 1: Get cache directory from environment variables
                cache_dir = self._get_cache_dir_from_env()

                model = SentenceTransformer(
                    model_name,
                    device=device,
                    cache_folder=cache_dir,
                    trust_remote_code=True,
                )

                # Cache the model
                self.models[model_name] = model
                self.current_model = model
                self.current_model_name = model_name

                # Initialize batch processor with new model
                self._initialize_batch_processor()

                logger.info(f"‚úÖ Successfully loaded model: {model_name} on {device}")
                logger.info(f"üíæ Model cached to: {cache_dir}")
                return True

            except Exception as sentence_transformers_error:
                logger.warning(
                    f"‚ö†Ô∏è SentenceTransformers failed: {sentence_transformers_error}"
                )
                logger.info("üîÑ Trying fallback with transformers AutoModel...")

                # Fallback to transformers AutoModel
                from transformers import AutoModel, AutoTokenizer
                import torch  # PRIORITY 1: Get device from environment variables

                device = self._get_device_from_env()
                logger.info(f"üì± Loading fallback model on device: {device}")

                # Load tokenizer and model
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name, trust_remote_code=True
                )
                model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
                model.to(device)
                model.eval()

                # Create a wrapper to make it compatible with our interface
                class TransformersModelWrapper:
                    def __init__(self, model, tokenizer, device):
                        self.model = model
                        self.tokenizer = tokenizer
                        self.device = device

                    def encode(self, texts, **kwargs):
                        """Encode texts to embeddings using transformers model."""
                        if isinstance(texts, str):
                            texts = [texts]

                        # Tokenize
                        inputs = self.tokenizer(
                            texts,
                            padding=True,
                            truncation=True,
                            return_tensors="pt",
                            max_length=512,
                        )
                        inputs = {k: v.to(self.device) for k, v in inputs.items()}

                        # Get embeddings
                        with torch.no_grad():
                            outputs = self.model(**inputs)
                            # Use [CLS] token embedding or mean pooling
                            embeddings = outputs.last_hidden_state.mean(dim=1)

                        # Normalize if requested
                        if kwargs.get("normalize_embeddings", True):
                            embeddings = torch.nn.functional.normalize(
                                embeddings, p=2, dim=1
                            )

                        return embeddings.cpu().numpy()

                wrapped_model = TransformersModelWrapper(model, tokenizer, device)

                # Cache the model
                self.models[model_name] = wrapped_model
                self.current_model = wrapped_model
                self.current_model_name = model_name

                # Note: batch processor may not work with fallback model
                self.batch_processor = None

                logger.info(
                    f"‚úÖ Successfully loaded fallback model: {model_name} on {device}"
                )
                return True

        except Exception as e:
            logger.error(f"‚ùå Failed to load model {model_name}: {e}")
            return False

    def _initialize_batch_processor(self) -> None:
        """Initialize the batch processor with the current model."""
        if self.current_model is not None:
            # Import here to avoid circular dependency
            from .batch_processor import BatchProcessor

            self.batch_processor = BatchProcessor(self, cache_size=10000)
            logger.debug(
                "üöÄ Batch processor initialized for optimized embedding processing"
            )

    def load_default_model(self) -> bool:
        """
        Load the default embedding model from configuration.

        Returns:
            True if default model loaded successfully
        """
        logger.info(f"üîÑ Loading default model: {self.default_model}")
        return self.load_model(self.default_model)

    def load_best_available_model(self) -> bool:
        """
        Load the best available embedding model from priority list.
        First tries the default model, then fallback models.

        Returns:
            True if any model was loaded successfully
        """
        logger.info("üîÑ Loading best available embedding model...")

        # First try the default model
        logger.info(f"üîÑ Trying default model: {self.default_model}")
        if self.load_model(self.default_model):
            logger.info(f"‚úÖ Successfully loaded default model: {self.default_model}")
            return True
        else:
            logger.warning(f"‚ùå Failed to load default model: {self.default_model}")

        # Then try fallback models
        for model_name in self.model_priority:
            if model_name == self.default_model:
                continue  # Skip already tried default model

            logger.info(f"üîÑ Trying fallback model: {model_name}")

            if self.load_model(model_name):
                logger.info(f"‚úÖ Successfully loaded fallback model: {model_name}")
                return True
            else:
                logger.warning(f"‚ùå Failed to load: {model_name}")
                continue

        # If all models fail, use ChromaDB default
        logger.warning("‚ùå All embedding models failed to load")
        logger.info("üîÑ Falling back to ChromaDB default embedding...")
        self.current_model = None
        self.current_model_name = "chromadb-default"
        return False

    def encode_documents(
        self,
        texts: List[str],
        normalize: bool = True,
        use_batch_optimization: bool = True,
    ) -> Optional[List[List[float]]]:
        """
        Encode a list of documents into embeddings.

        Args:
            texts: List of texts to encode
            normalize: Whether to normalize embeddings
            use_batch_optimization: Whether to use batch processing optimization

        Returns:
            List of embeddings or None if no model available"""
        with track_execution_time("encode_documents"):
            # Use batch processor if available and requested
            if use_batch_optimization and self.batch_processor is not None:
                logger.debug(
                    f"üöÄ Using optimized batch processing for {len(texts)} documents"
                )
                # Note: batch processor is async, but we'll call it synchronously for now
                # In a real async environment, this would be awaited
                import asyncio

                try:
                    loop = asyncio.get_event_loop()
                    result = loop.run_until_complete(
                        self.batch_processor.encode_documents_optimized(
                            texts, normalize
                        )
                    )
                except RuntimeError:
                    # No event loop running, create one
                    result = asyncio.run(
                        self.batch_processor.encode_documents_optimized(
                            texts, normalize
                        )
                    )

                if result is not None:
                    self.operation_counts["encode_documents"] += 1
                return result
            else:
                # Fall back to original implementation
                result = self._do_encode_documents(texts, normalize)
                if result is not None:
                    self.operation_counts["encode_documents"] += 1
                return result

    def _do_encode_documents(
        self, texts: List[str], normalize: bool = True
    ) -> Optional[List[List[float]]]:
        """Internal method for encoding documents."""
        if not self.current_model:
            logger.warning("No embedding model available, returning None")
            return None

        try:
            logger.debug(
                f"üß† Encoding {len(texts)} documents with {self.current_model_name}"
            )
            embeddings = self.current_model.encode(
                texts, normalize_embeddings=normalize
            )

            # Convert numpy array to list for ChromaDB compatibility
            if isinstance(embeddings, np.ndarray):
                embeddings_list = embeddings.tolist()
            else:
                embeddings_list = embeddings

            logger.debug(
                f"‚úÖ Generated embeddings: {len(embeddings_list)} x {len(embeddings_list[0])}"
            )
            return embeddings_list

        except Exception as e:
            logger.error(f"‚ùå Failed to encode documents: {e}")
            return None

    def encode_query(
        self, query: str, normalize: bool = True, use_cache: bool = True
    ) -> Optional[List[float]]:
        """
        Encode a single query into embedding.

        Args:
            query: Query text to encode
            normalize: Whether to normalize embedding
            use_cache: Whether to use cache for this query

        Returns:
            Query embedding or None if no model available
        """
        with track_execution_time("encode_query"):
            # Try cache first if batch processor is available
            if use_cache and self.batch_processor is not None:
                cached_embedding = self.batch_processor.cache.get(query)
                if cached_embedding is not None:
                    logger.debug(f"üéØ Cache hit for query: '{query[:50]}...'")
                    self.operation_counts["encode_query"] += 1
                    return cached_embedding

            # Encode the query
            result = self._do_encode_query(
                query, normalize
            )  # Cache the result if batch processor is available
            if result is not None and use_cache and self.batch_processor is not None:
                self.batch_processor.cache.set(query, result)

            if result is not None:
                self.operation_counts["encode_query"] += 1
            return result

    def _do_encode_query(
        self, query: str, normalize: bool = True
    ) -> Optional[List[float]]:
        """Internal method for encoding query."""
        if not self.current_model:
            logger.warning("No embedding model available, returning None")
            return None

        try:
            logger.debug(
                f"üß† Encoding query with {self.current_model_name}: '{query[:50]}...'"
            )
            embedding = self.current_model.encode(
                [query], normalize_embeddings=normalize
            )

            # Convert to list for ChromaDB compatibility
            if isinstance(embedding, np.ndarray):
                embedding_list = embedding[0].tolist()
            else:
                embedding_list = embedding[0]

            logger.debug(
                f"‚úÖ Generated query embedding: {len(embedding_list)} dimensions"
            )
            return embedding_list

        except Exception as e:
            logger.error(f"‚ùå Failed to encode query: {e}")
            return None

    def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the current model.

        Returns:
            Model information dictionary including environment variables
        """
        # Get environment variable configuration
        env_model = self._get_model_from_env()
        env_device = os.getenv("EMBEDDING_DEVICE", "not set")
        env_cache_dir = os.getenv("EMBEDDING_CACHE_DIR", "not set")

        base_info = {
            "name": self.current_model_name,
            "configured_model": self.default_model,
            "environment_variables": {
                "EMBEDDING_MODEL": env_model or "not set",
                "EMBEDDING_DEVICE": env_device,
                "EMBEDDING_CACHE_DIR": env_cache_dir,
                "priority": "Environment Variables ‚Üí Config ‚Üí Defaults",
            },
            "fallback_models": self.model_priority,
        }

        if not self.current_model:
            base_info.update(
                {
                    "status": "No custom model loaded (using ChromaDB default)",
                    "embedding_dim": "Unknown",
                    "type": "ChromaDB default",
                    "device": "N/A",
                    "note": "Use configure_embedding_model() to load custom model",
                }
            )
            return base_info

        try:
            # Try to get embedding dimension
            test_embedding = self.current_model.encode(
                ["test"], normalize_embeddings=False
            )
            embedding_dim = (
                len(test_embedding[0]) if len(test_embedding) > 0 else "Unknown"
            )

            # Get device information
            device = "Unknown"
            try:
                if hasattr(self.current_model, "device"):
                    device = str(self.current_model.device)
                else:
                    device = self._detect_optimal_device()
            except:
                device = "Unknown"

        except:
            embedding_dim = "Unknown"
            device = "Unknown"

        base_info.update(
            {
                "status": "Loaded and ready",
                "embedding_dim": embedding_dim,
                "type": "SentenceTransformer",
                "device": device,
                "cached_models": list(self.models.keys()),
            }
        )

        return base_info

    def get_metrics(self) -> Dict[str, Any]:
        """
        Get performance metrics for embedding operations.

        Returns:
            Dictionary containing performance metrics
        """
        base_metrics = self.metrics.get_performance_summary()
        # Add our operation counts
        base_metrics.update(
            {
                "operation_counts": self.operation_counts,
                "total_custom_operations": sum(self.operation_counts.values()),
            }
        )

        # Add batch processing metrics if available
        if self.batch_processor is not None:
            batch_metrics = self.batch_processor.get_performance_metrics()
            base_metrics.update({"batch_processing_optimization": batch_metrics})

        return base_metrics

    def clear_embedding_cache(self) -> Optional[Dict[str, int]]:
        """
        Clear the embedding cache if batch processor is available.

        Returns:
            Cache statistics before clearing, or None if not available
        """
        if self.batch_processor is not None:
            return self.batch_processor.clear_cache()
        return None

    def get_cache_stats(self) -> Optional[Dict[str, Any]]:
        """
        Get embedding cache statistics.

        Returns:
            Cache statistics or None if not available
        """
        if self.batch_processor is not None:
            return self.batch_processor.cache.get_stats()
        return None

    def intelligent_chunk_text(
        self, text: str, chunk_size: int = 400, overlap: int = 50
    ) -> List[str]:
        """
        Intelligent text chunking for Vietnamese text.

        Args:
            text: Text to chunk
            chunk_size: Maximum chunk size
            overlap: Overlap between chunks

        Returns:
            List of text chunks
        """
        # T√°ch theo ƒëo·∫°n vƒÉn tr∆∞·ªõc
        paragraphs = [p.strip() for p in text.split("\\n\\n") if p.strip()]

        chunks = []
        current_chunk = ""

        for para in paragraphs:
            # N·∫øu ƒëo·∫°n vƒÉn qu√° d√†i, chia nh·ªè h∆°n
            if len(para) > chunk_size:
                # T√°ch theo c√¢u
                sentences = []
                temp_sentence = ""

                for char in para:
                    temp_sentence += char
                    if char in ".!?":
                        # Ki·ªÉm tra kh√¥ng ph·∫£i s·ªë th·∫≠p ph√¢n
                        remaining_text = para[
                            para.find(temp_sentence) + len(temp_sentence) :
                        ]
                        next_chars = (
                            remaining_text[:3]
                            if len(remaining_text) >= 3
                            else remaining_text
                        )
                        if not any(c.isdigit() for c in next_chars):
                            sentences.append(temp_sentence.strip())
                            temp_sentence = ""

                # Th√™m c√¢u cu·ªëi n·∫øu c√≤n
                if temp_sentence.strip():
                    sentences.append(temp_sentence.strip())

                # Combine sentences to chunks
                for sentence in sentences:
                    if len(current_chunk) + len(sentence) <= chunk_size:
                        current_chunk += " " + sentence if current_chunk else sentence
                    else:
                        if current_chunk.strip():
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence
            else:
                # ƒêo·∫°n vƒÉn ng·∫Øn, th√™m tr·ª±c ti·∫øp
                if len(current_chunk) + len(para) <= chunk_size:
                    current_chunk += "\\n" + para if current_chunk else para
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    current_chunk = para

        # Th√™m chunk cu·ªëi
        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return [chunk for chunk in chunks if len(chunk.strip()) > 20]

    def get_model_sizes_info(self) -> Dict[str, Any]:
        """
        Get information about model sizes for user to choose.

        Returns:
            Dictionary with model names and their approximate sizes
        """
        size_info = {
            "nomic-ai/nomic-embed-text-v2-moe": "305M active/475M total - Latest MoE version, SoTA multilingual, 768D embeddings, no token needed",
            # "nomic-ai/nomic-embed-text-v1.5": "137MB - Previous version, stable fallback, 768D embeddings",
            # "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "177MB - Fast, multilingual",
            # "sentence-transformers/paraphrase-multilingual-mpnet-base-v2": "1.1GB - High quality, multilingual",
        }

        return {
            "available_models": size_info,
            "recommendation": "For best multilingual performance: nomic-ai/nomic-embed-text-v2-moe",
            "note": "All models are publicly accessible, no HF token required",
        }

    def _get_model_from_env(self) -> Optional[str]:
        """
        Get embedding model from environment variables (PRIORITY 1).

        Environment Variables checked in order:
        1. EMBEDDING_MODEL - Primary model name
        2. MCP_EMBEDDING_MODEL - Alternative naming
        3. CHROMA_EMBEDDING_MODEL - ChromaDB specific

        Returns:
            Model name if found in env, None otherwise
        """
        env_vars = ["EMBEDDING_MODEL", "MCP_EMBEDDING_MODEL", "CHROMA_EMBEDDING_MODEL"]

        for env_var in env_vars:
            model_name = os.getenv(env_var)
            if model_name:
                logger.info(f"üåç Using model from {env_var}: {model_name}")
                return model_name.strip()

        logger.info("üîç No embedding model found in environment variables")
        return None

    def _get_fallback_models_from_env(self) -> List[str]:
        """
        Get fallback models from environment variables.

        Environment Variables:
        - EMBEDDING_FALLBACK_MODELS: comma-separated list

        Returns:
            List of fallback model names
        """
        fallback_env = os.getenv("EMBEDDING_FALLBACK_MODELS", "")
        if fallback_env:
            models = [m.strip() for m in fallback_env.split(",") if m.strip()]
            logger.info(f"üîÑ Fallback models from env: {models}")
            return models

        # Default fallback models if not in env
        default_fallbacks = [
            "sentence-transformers/all-MiniLM-L6-v2",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        ]
        logger.info(f"üìã Using default fallback models: {default_fallbacks}")
        return default_fallbacks

    def _get_device_from_env(self) -> str:
        """
        Get device preference from environment variables.

        Environment Variables:
        - EMBEDDING_DEVICE: cpu, cuda, mps, auto

        Returns:
            Device string
        """
        device_env = os.getenv("EMBEDDING_DEVICE", "auto").lower()

        if device_env == "auto":
            return self._detect_optimal_device()
        elif device_env in ["cpu", "cuda", "mps"]:
            logger.info(f"üéØ Using device from env: {device_env}")
            return device_env
        else:
            logger.warning(f"‚ö†Ô∏è Invalid device in env: {device_env}, using auto")
            return self._detect_optimal_device()

    def _get_cache_dir_from_env(self) -> str:
        """
        Get cache directory from environment variables.

        Environment Variables checked in order:
        1. EMBEDDING_CACHE_DIR
        2. SENTENCE_TRANSFORMERS_HOME
        3. MODEL_CACHE_DIR

        Returns:
            Cache directory path
        """
        env_vars = [
            "EMBEDDING_CACHE_DIR",
            "SENTENCE_TRANSFORMERS_HOME",
            "MODEL_CACHE_DIR",
        ]

        for env_var in env_vars:
            cache_dir = os.getenv(env_var)
            if cache_dir:
                logger.info(f"üìÅ Using cache dir from {env_var}: {cache_dir}")
                return cache_dir

        default_cache = "./models_cache"
        logger.info(f"üìÅ Using default cache dir: {default_cache}")
        return default_cache
