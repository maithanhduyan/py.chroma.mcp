# Generated by Copilot
"""
Embedding manager for MCP server.
Handles loading and managing different embedding models.
"""

import logging
import os
from typing import Dict, List, Optional, Any
import numpy as np

# Import config to setup project paths automatically
import config

from utils.metrics import (
    track_execution_time,
    measure_memory_usage,
    MetricsCollector,
)

# Import batch processor for optimization (will be imported when needed)
# from .batch_processor import BatchProcessor

logger = logging.getLogger(__name__)


class EmbeddingManager:
    """
    Manages embedding models for the MCP server.
    Supports model loading, caching, and fallback strategies.
    """

    def __init__(self):
        """Initialize the embedding manager."""
        self.models: Dict[str, Any] = {}  # Cache loaded models
        self.current_model = None
        self.current_model_name = "chromadb-default"

        # Model priority list (fastest to slowest, no HF token required, multilingual preferred)
        self.model_priority = [
            "nomic-ai/nomic-embed-text-v1.5",  # Very lightweight (137MB), no token needed, good performance
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",  # Small (177MB), multilingual
            "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",  # High quality (1.1GB), multilingual
        ]

        # Initialize metrics collector
        self.metrics = MetricsCollector()
        # Simple operation counters for immediate metrics
        self.operation_counts = {
            "load_model": 0,
            "encode_documents": 0,
            "encode_query": 0,
        }
        # Initialize batch processor for optimization
        self.batch_processor: Optional[Any] = None

    def load_model(self, model_name: str, force_reload: bool = False) -> bool:
        """
        Load an embedding model.        Args:
            model_name: Name of the model to load
            force_reload: Whether to reload if already cached        Returns:
            True if model loaded successfully
        """
        with track_execution_time("load_model"):
            result = self._do_load_model(model_name, force_reload)
            if result:
                self.operation_counts["load_model"] += 1
            return result

    def _detect_optimal_device(self) -> str:
        """
        Detect the optimal device for running embedding models.

        Returns:
            Device string: "cuda", "mps" (Mac), or "cpu"
        """
        try:
            # Try to import torch to check for GPU availability
            import torch

            if torch.cuda.is_available():
                device = "cuda"
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                logger.info(f"🚀 CUDA available: {gpu_count} GPU(s) - {gpu_name}")
                return device
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                logger.info("🍎 MPS (Apple Silicon) available")
                return "mps"
            else:
                logger.info("💻 Using CPU for embeddings")
                return "cpu"

        except ImportError:
            logger.warning("⚠️ PyTorch not available, using CPU")
            return "cpu"
        except Exception as e:
            logger.warning(f"⚠️ Device detection failed: {e}, using CPU")
            return "cpu"

    def _do_load_model(self, model_name: str, force_reload: bool = False) -> bool:
        """Internal method for loading model without metrics tracking."""
        # Check if already loaded and cached
        if not force_reload and model_name in self.models:
            self.current_model = self.models[model_name]
            self.current_model_name = model_name
            logger.info(f"✅ Using cached model: {model_name}")

            # Initialize batch processor with current model
            self._initialize_batch_processor()
            return True

        try:
            logger.info(f"🔄 Loading embedding model: {model_name}")
            logger.info("⏳ This may take a few minutes for first-time downloads...")

            from sentence_transformers import SentenceTransformer

            # Detect optimal device (GPU/CPU)
            device = self._detect_optimal_device()

            # Load the model with appropriate device and progress indication
            logger.info(f"📱 Loading model on device: {device}")
            logger.info(
                "📥 Downloading model files (if not cached)..."
            )  # Set cache directory for faster subsequent loads
            cache_dir = os.getenv("SENTENCE_TRANSFORMERS_HOME", "./models_cache")

            model = SentenceTransformer(
                model_name,
                device=device,
                cache_folder=cache_dir,
                trust_remote_code=True,
            )

            # Cache the model
            self.models[model_name] = model
            self.current_model = model
            self.current_model_name = model_name

            # Initialize batch processor with new model
            self._initialize_batch_processor()

            logger.info(f"✅ Successfully loaded model: {model_name} on {device}")
            logger.info(f"💾 Model cached to: {cache_dir}")
            return True

        except Exception as e:
            logger.error(f"❌ Failed to load model {model_name}: {e}")
            return False

    def _initialize_batch_processor(self) -> None:
        """Initialize the batch processor with the current model."""
        if self.current_model is not None:
            # Import here to avoid circular dependency
            from .batch_processor import BatchProcessor

            self.batch_processor = BatchProcessor(self, cache_size=10000)
            logger.debug(
                "🚀 Batch processor initialized for optimized embedding processing"
            )

    def load_best_available_model(self) -> bool:
        """
        Load the best available embedding model from priority list.

        Returns:
            True if any model was loaded successfully
        """
        logger.info("🔄 Loading best available embedding model...")

        for model_name in self.model_priority:
            logger.info(f"🔄 Trying to load: {model_name}")

            if self.load_model(model_name):
                logger.info(f"✅ Successfully loaded: {model_name}")
                return True
            else:
                logger.warning(f"❌ Failed to load: {model_name}")
                continue  # If all models fail, use ChromaDB default
        logger.warning("❌ All embedding models failed to load")
        logger.info("🔄 Falling back to ChromaDB default embedding...")
        self.current_model = None
        self.current_model_name = "chromadb-default"
        return False

    def encode_documents(
        self,
        texts: List[str],
        normalize: bool = True,
        use_batch_optimization: bool = True,
    ) -> Optional[List[List[float]]]:
        """
        Encode a list of documents into embeddings.

        Args:
            texts: List of texts to encode
            normalize: Whether to normalize embeddings
            use_batch_optimization: Whether to use batch processing optimization

        Returns:
            List of embeddings or None if no model available"""
        with track_execution_time("encode_documents"):
            # Use batch processor if available and requested
            if use_batch_optimization and self.batch_processor is not None:
                logger.debug(
                    f"🚀 Using optimized batch processing for {len(texts)} documents"
                )
                # Note: batch processor is async, but we'll call it synchronously for now
                # In a real async environment, this would be awaited
                import asyncio

                try:
                    loop = asyncio.get_event_loop()
                    result = loop.run_until_complete(
                        self.batch_processor.encode_documents_optimized(
                            texts, normalize
                        )
                    )
                except RuntimeError:
                    # No event loop running, create one
                    result = asyncio.run(
                        self.batch_processor.encode_documents_optimized(
                            texts, normalize
                        )
                    )

                if result is not None:
                    self.operation_counts["encode_documents"] += 1
                return result
            else:
                # Fall back to original implementation
                result = self._do_encode_documents(texts, normalize)
                if result is not None:
                    self.operation_counts["encode_documents"] += 1
                return result

    def _do_encode_documents(
        self, texts: List[str], normalize: bool = True
    ) -> Optional[List[List[float]]]:
        """Internal method for encoding documents."""
        if not self.current_model:
            logger.warning("No embedding model available, returning None")
            return None

        try:
            logger.debug(
                f"🧠 Encoding {len(texts)} documents with {self.current_model_name}"
            )
            embeddings = self.current_model.encode(
                texts, normalize_embeddings=normalize
            )

            # Convert numpy array to list for ChromaDB compatibility
            if isinstance(embeddings, np.ndarray):
                embeddings_list = embeddings.tolist()
            else:
                embeddings_list = embeddings

            logger.debug(
                f"✅ Generated embeddings: {len(embeddings_list)} x {len(embeddings_list[0])}"
            )
            return embeddings_list

        except Exception as e:
            logger.error(f"❌ Failed to encode documents: {e}")
            return None

    def encode_query(
        self, query: str, normalize: bool = True, use_cache: bool = True
    ) -> Optional[List[float]]:
        """
        Encode a single query into embedding.

        Args:
            query: Query text to encode
            normalize: Whether to normalize embedding
            use_cache: Whether to use cache for this query

        Returns:
            Query embedding or None if no model available
        """
        with track_execution_time("encode_query"):
            # Try cache first if batch processor is available
            if use_cache and self.batch_processor is not None:
                cached_embedding = self.batch_processor.cache.get(query)
                if cached_embedding is not None:
                    logger.debug(f"🎯 Cache hit for query: '{query[:50]}...'")
                    self.operation_counts["encode_query"] += 1
                    return cached_embedding

            # Encode the query
            result = self._do_encode_query(query, normalize)

            # Cache the result if batch processor is available
            if result is not None and use_cache and self.batch_processor is not None:
                self.batch_processor.cache.set(query, result)

            if result is not None:
                self.operation_counts["encode_query"] += 1
            return result

    def _do_encode_query(
        self, query: str, normalize: bool = True
    ) -> Optional[List[float]]:
        """Internal method for encoding query."""
        if not self.current_model:
            logger.warning("No embedding model available, returning None")
            return None

        try:
            logger.debug(
                f"🧠 Encoding query with {self.current_model_name}: '{query[:50]}...'"
            )
            embedding = self.current_model.encode(
                [query], normalize_embeddings=normalize
            )  # Convert to list for ChromaDB compatibility
            if isinstance(embedding, np.ndarray):
                embedding_list = embedding[0].tolist()
            else:
                embedding_list = embedding[0]

            logger.debug(
                f"✅ Generated query embedding: {len(embedding_list)} dimensions"
            )
            return embedding_list

        except Exception as e:
            logger.error(f"❌ Failed to encode query: {e}")
            return None

    def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the current model.

        Returns:
            Model information dictionary
        """
        if not self.current_model:
            return {
                "name": self.current_model_name,
                "status": "No custom model loaded",
                "embedding_dim": "Unknown",
                "type": "ChromaDB default",
                "device": "N/A",
            }

        try:
            # Try to get embedding dimension
            test_embedding = self.current_model.encode(
                ["test"], normalize_embeddings=False
            )
            embedding_dim = (
                len(test_embedding[0]) if len(test_embedding) > 0 else "Unknown"
            )

            # Get device information
            device = "Unknown"
            try:
                if hasattr(self.current_model, "_target_device"):
                    device = str(self.current_model._target_device)
                elif hasattr(self.current_model, "device"):
                    device = str(self.current_model.device)
                else:
                    device = self._detect_optimal_device()
            except:
                device = "Unknown"

        except:
            embedding_dim = "Unknown"
            device = "Unknown"

        return {
            "name": self.current_model_name,
            "status": "Loaded and ready",
            "embedding_dim": embedding_dim,
            "type": "SentenceTransformer",
            "device": device,
            "cached_models": list(self.models.keys()),
        }

    def get_metrics(self) -> Dict[str, Any]:
        """
        Get performance metrics for embedding operations.

        Returns:
            Dictionary containing performance metrics
        """
        base_metrics = self.metrics.get_performance_summary()
        # Add our operation counts
        base_metrics.update(
            {
                "operation_counts": self.operation_counts,
                "total_custom_operations": sum(self.operation_counts.values()),
            }
        )

        # Add batch processing metrics if available
        if self.batch_processor is not None:
            batch_metrics = self.batch_processor.get_performance_metrics()
            base_metrics.update({"batch_processing_optimization": batch_metrics})

        return base_metrics

    def clear_embedding_cache(self) -> Optional[Dict[str, int]]:
        """
        Clear the embedding cache if batch processor is available.

        Returns:
            Cache statistics before clearing, or None if not available
        """
        if self.batch_processor is not None:
            return self.batch_processor.clear_cache()
        return None

    def get_cache_stats(self) -> Optional[Dict[str, Any]]:
        """
        Get embedding cache statistics.

        Returns:
            Cache statistics or None if not available
        """
        if self.batch_processor is not None:
            return self.batch_processor.cache.get_stats()
        return None

    def intelligent_chunk_text(
        self, text: str, chunk_size: int = 400, overlap: int = 50
    ) -> List[str]:
        """
        Intelligent text chunking for Vietnamese text.

        Args:
            text: Text to chunk
            chunk_size: Maximum chunk size
            overlap: Overlap between chunks

        Returns:
            List of text chunks
        """
        # Tách theo đoạn văn trước
        paragraphs = [p.strip() for p in text.split("\\n\\n") if p.strip()]

        chunks = []
        current_chunk = ""

        for para in paragraphs:
            # Nếu đoạn văn quá dài, chia nhỏ hơn
            if len(para) > chunk_size:
                # Tách theo câu
                sentences = []
                temp_sentence = ""

                for char in para:
                    temp_sentence += char
                    if char in ".!?":
                        # Kiểm tra không phải số thập phân
                        remaining_text = para[
                            para.find(temp_sentence) + len(temp_sentence) :
                        ]
                        next_chars = (
                            remaining_text[:3]
                            if len(remaining_text) >= 3
                            else remaining_text
                        )
                        if not any(c.isdigit() for c in next_chars):
                            sentences.append(temp_sentence.strip())
                            temp_sentence = ""

                # Thêm câu cuối nếu còn
                if temp_sentence.strip():
                    sentences.append(temp_sentence.strip())

                # Combine sentences to chunks
                for sentence in sentences:
                    if len(current_chunk) + len(sentence) <= chunk_size:
                        current_chunk += " " + sentence if current_chunk else sentence
                    else:
                        if current_chunk.strip():
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence
            else:
                # Đoạn văn ngắn, thêm trực tiếp
                if len(current_chunk) + len(para) <= chunk_size:
                    current_chunk += "\\n" + para if current_chunk else para
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    current_chunk = para

        # Thêm chunk cuối
        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return [chunk for chunk in chunks if len(chunk.strip()) > 20]

    def get_model_sizes_info(self) -> Dict[str, Any]:
        """
        Get information about model sizes for user to choose.

        Returns:
            Dictionary with model names and their approximate sizes
        """

        size_info = {
            "nomic-ai/nomic-embed-text-v1.5": "137MB - Lightweight, good performance, no token needed",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "177MB - Fast, multilingual",
            "sentence-transformers/paraphrase-multilingual-mpnet-base-v2": "1.1GB - High quality, multilingual",
        }

        return {
            "available_models": size_info,
            "recommendation": "For fast testing: nomic-ai/nomic-embed-text-v1.5",
            "note": "All models are publicly accessible, no HF token required",
        }
