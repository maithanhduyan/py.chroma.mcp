# Generated by Copilot
"""
Embedding manager for MCP server.
Handles loading and managing different embedding models.
"""

import logging
from typing import Dict, List, Optional, Any
import numpy as np
from .auth import HuggingFaceAuth

logger = logging.getLogger(__name__)


class EmbeddingManager:
    """
    Manages embedding models for the MCP server.
    Supports model loading, caching, and fallback strategies.
    """

    def __init__(self):
        """Initialize the embedding manager."""
        self.models: Dict[str, Any] = {}  # Cache loaded models
        self.current_model = None
        self.current_model_name = "chromadb-default"

        # Model priority list (best to fallback)
        self.model_priority = [
            "mixedbread-ai/mxbai-embed-large-v1",  # SOTA, requires HF token
            "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",  # Good multilingual
            "sentence-transformers/distiluse-base-multilingual-cased",  # Fast multilingual
            "sentence-transformers/all-MiniLM-L12-v2",  # Balanced performance
        ]

    def load_model(self, model_name: str, force_reload: bool = False) -> bool:
        """
        Load an embedding model.

        Args:
            model_name: Name of the model to load
            force_reload: Whether to reload if already cached

        Returns:
            True if model loaded successfully
        """
        # Check if already loaded and cached
        if not force_reload and model_name in self.models:
            self.current_model = self.models[model_name]
            self.current_model_name = model_name
            logger.info(f"‚úÖ Using cached model: {model_name}")
            return True

        # Setup HF authentication first
        token = HuggingFaceAuth.setup_token()
        if not token and model_name.startswith("mixedbread-ai"):
            logger.warning(f"‚ö†Ô∏è No HF_TOKEN found, may not be able to load {model_name}")

        try:
            logger.info(f"üîÑ Loading embedding model: {model_name}")

            from sentence_transformers import SentenceTransformer

            # Load the model
            model = SentenceTransformer(model_name)

            # Cache the model
            self.models[model_name] = model
            self.current_model = model
            self.current_model_name = model_name

            logger.info(f"‚úÖ Successfully loaded model: {model_name}")
            return True

        except Exception as e:
            logger.error(f"‚ùå Failed to load model {model_name}: {e}")
            return False

    def load_best_available_model(self) -> bool:
        """
        Load the best available embedding model from priority list.

        Returns:
            True if any model was loaded successfully
        """
        logger.info("üîÑ Loading best available embedding model...")

        for model_name in self.model_priority:
            logger.info(f"üîÑ Trying to load: {model_name}")

            if self.load_model(model_name):
                logger.info(f"‚úÖ Successfully loaded: {model_name}")
                return True
            else:
                logger.warning(f"‚ùå Failed to load: {model_name}")
                continue

        # If all models fail, use ChromaDB default
        logger.warning("‚ùå All embedding models failed to load")
        logger.info("üîÑ Falling back to ChromaDB default embedding...")
        self.current_model = None
        self.current_model_name = "chromadb-default"
        return False

    def encode_documents(
        self, texts: List[str], normalize: bool = True
    ) -> Optional[List[List[float]]]:
        """
        Encode a list of documents into embeddings.

        Args:
            texts: List of texts to encode
            normalize: Whether to normalize embeddings

        Returns:
            List of embeddings or None if no model available
        """
        if not self.current_model:
            logger.warning("No embedding model available, returning None")
            return None

        try:
            logger.debug(
                f"üß† Encoding {len(texts)} documents with {self.current_model_name}"
            )
            embeddings = self.current_model.encode(
                texts, normalize_embeddings=normalize
            )

            # Convert numpy array to list for ChromaDB compatibility
            if isinstance(embeddings, np.ndarray):
                embeddings_list = embeddings.tolist()
            else:
                embeddings_list = embeddings

            logger.debug(
                f"‚úÖ Generated embeddings: {len(embeddings_list)} x {len(embeddings_list[0])}"
            )
            return embeddings_list

        except Exception as e:
            logger.error(f"‚ùå Failed to encode documents: {e}")
            return None

    def encode_query(self, query: str, normalize: bool = True) -> Optional[List[float]]:
        """
        Encode a single query into embedding.

        Args:
            query: Query text to encode
            normalize: Whether to normalize embedding

        Returns:
            Query embedding or None if no model available
        """
        if not self.current_model:
            logger.warning("No embedding model available, returning None")
            return None

        try:
            logger.debug(
                f"üß† Encoding query with {self.current_model_name}: '{query[:50]}...'"
            )
            embedding = self.current_model.encode(
                [query], normalize_embeddings=normalize
            )

            # Convert to list for ChromaDB compatibility
            if isinstance(embedding, np.ndarray):
                embedding_list = embedding[0].tolist()
            else:
                embedding_list = embedding[0]

            logger.debug(
                f"‚úÖ Generated query embedding: {len(embedding_list)} dimensions"
            )
            return embedding_list

        except Exception as e:
            logger.error(f"‚ùå Failed to encode query: {e}")
            return None

    def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the current model.

        Returns:
            Model information dictionary
        """
        if not self.current_model:
            return {
                "name": self.current_model_name,
                "status": "No custom model loaded",
                "embedding_dim": "Unknown",
                "type": "ChromaDB default",
            }

        try:
            # Try to get embedding dimension
            test_embedding = self.current_model.encode(
                ["test"], normalize_embeddings=False
            )
            embedding_dim = (
                len(test_embedding[0]) if len(test_embedding) > 0 else "Unknown"
            )
        except:
            embedding_dim = "Unknown"

        return {
            "name": self.current_model_name,
            "status": "Loaded and ready",
            "embedding_dim": embedding_dim,
            "type": "SentenceTransformer",
            "cached_models": list(self.models.keys()),
        }

    def intelligent_chunk_text(
        self, text: str, chunk_size: int = 400, overlap: int = 50
    ) -> List[str]:
        """
        Intelligent text chunking for Vietnamese text.

        Args:
            text: Text to chunk
            chunk_size: Maximum chunk size
            overlap: Overlap between chunks

        Returns:
            List of text chunks
        """
        # T√°ch theo ƒëo·∫°n vƒÉn tr∆∞·ªõc
        paragraphs = [p.strip() for p in text.split("\\n\\n") if p.strip()]

        chunks = []
        current_chunk = ""

        for para in paragraphs:
            # N·∫øu ƒëo·∫°n vƒÉn qu√° d√†i, chia nh·ªè h∆°n
            if len(para) > chunk_size:
                # T√°ch theo c√¢u
                sentences = []
                temp_sentence = ""

                for char in para:
                    temp_sentence += char
                    if char in ".!?":
                        # Ki·ªÉm tra kh√¥ng ph·∫£i s·ªë th·∫≠p ph√¢n
                        remaining_text = para[
                            para.find(temp_sentence) + len(temp_sentence) :
                        ]
                        next_chars = (
                            remaining_text[:3]
                            if len(remaining_text) >= 3
                            else remaining_text
                        )
                        if not any(c.isdigit() for c in next_chars):
                            sentences.append(temp_sentence.strip())
                            temp_sentence = ""

                # Th√™m c√¢u cu·ªëi n·∫øu c√≤n
                if temp_sentence.strip():
                    sentences.append(temp_sentence.strip())

                # Combine sentences to chunks
                for sentence in sentences:
                    if len(current_chunk) + len(sentence) <= chunk_size:
                        current_chunk += " " + sentence if current_chunk else sentence
                    else:
                        if current_chunk.strip():
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence
            else:
                # ƒêo·∫°n vƒÉn ng·∫Øn, th√™m tr·ª±c ti·∫øp
                if len(current_chunk) + len(para) <= chunk_size:
                    current_chunk += "\\n" + para if current_chunk else para
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    current_chunk = para

        # Th√™m chunk cu·ªëi
        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return [chunk for chunk in chunks if len(chunk.strip()) > 20]
