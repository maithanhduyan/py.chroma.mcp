# Generated by Copilot
"""
Embedding manager for MCP server.
Handles loading and managing different embedding models.
"""

import logging
from typing import Dict, List, Optional, Any
import numpy as np
from .auth import HuggingFaceAuth

logger = logging.getLogger(__name__)


class EmbeddingManager:
    """
    Manages embedding models for the MCP server.
    Supports model loading, caching, and fallback strategies.
    """

    def __init__(self):
        """Initialize the embedding manager."""
        self.models: Dict[str, Any] = {}  # Cache loaded models
        self.current_model = None
        self.current_model_name = "chromadb-default"
        
        # Model priority list (best to fallback)
        self.model_priority = [
            "mixedbread-ai/mxbai-embed-large-v1",  # SOTA, requires HF token
            "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",  # Good multilingual
            "sentence-transformers/distiluse-base-multilingual-cased",  # Fast multilingual
            "sentence-transformers/all-MiniLM-L12-v2",  # Balanced performance
        ]

    def load_model(self, model_name: str, force_reload: bool = False) -> bool:
        """
        Load an embedding model.
        
        Args:
            model_name: Name of the model to load
            force_reload: Whether to reload if already cached
            
        Returns:
            True if model loaded successfully
        """
        # Check if already loaded and cached
        if not force_reload and model_name in self.models:
            self.current_model = self.models[model_name]
            self.current_model_name = model_name
            logger.info(f"âœ… Using cached model: {model_name}")
            return True

        # Setup HF authentication first
        token = HuggingFaceAuth.setup_token()
        if not token and model_name.startswith("mixedbread-ai"):
            logger.warning(f"âš ï¸ No HF_TOKEN found, may not be able to load {model_name}")

        try:
            logger.info(f"ğŸ”„ Loading embedding model: {model_name}")
            
            from sentence_transformers import SentenceTransformer
            
            # Load the model
            model = SentenceTransformer(model_name)
            
            # Cache the model
            self.models[model_name] = model
            self.current_model = model
            self.current_model_name = model_name
            
            logger.info(f"âœ… Successfully loaded model: {model_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to load model {model_name}: {e}")
            return False

    def load_best_available_model(self) -> bool:
        """
        Load the best available embedding model from priority list.
        
        Returns:
            True if any model was loaded successfully
        """
        logger.info("ğŸ”„ Loading best available embedding model...")
        
        for model_name in self.model_priority:
            logger.info(f"ğŸ”„ Trying to load: {model_name}")
            
            if self.load_model(model_name):
                logger.info(f"âœ… Successfully loaded: {model_name}")
                return True
            else:
                logger.warning(f"âŒ Failed to load: {model_name}")
                continue
        
        # If all models fail, use ChromaDB default
        logger.warning("âŒ All embedding models failed to load")
        logger.info("ğŸ”„ Falling back to ChromaDB default embedding...")
        self.current_model = None
        self.current_model_name = "chromadb-default"
        return False

    def encode_documents(self, texts: List[str], normalize: bool = True) -> Optional[List[List[float]]]:
        """
        Encode a list of documents into embeddings.
        
        Args:
            texts: List of texts to encode
            normalize: Whether to normalize embeddings
            
        Returns:
            List of embeddings or None if no model available
        """
        if not self.current_model:
            logger.warning("No embedding model available, returning None")
            return None
            
        try:
            logger.debug(f"ğŸ§  Encoding {len(texts)} documents with {self.current_model_name}")
            embeddings = self.current_model.encode(texts, normalize_embeddings=normalize)
            
            # Convert numpy array to list for ChromaDB compatibility
            if isinstance(embeddings, np.ndarray):
                embeddings_list = embeddings.tolist()
            else:
                embeddings_list = embeddings
                
            logger.debug(f"âœ… Generated embeddings: {len(embeddings_list)} x {len(embeddings_list[0])}")
            return embeddings_list
            
        except Exception as e:
            logger.error(f"âŒ Failed to encode documents: {e}")
            return None

    def encode_query(self, query: str, normalize: bool = True) -> Optional[List[float]]:
        """
        Encode a single query into embedding.
        
        Args:
            query: Query text to encode
            normalize: Whether to normalize embedding
            
        Returns:
            Query embedding or None if no model available
        """
        if not self.current_model:
            logger.warning("No embedding model available, returning None")
            return None
            
        try:
            logger.debug(f"ğŸ§  Encoding query with {self.current_model_name}: '{query[:50]}...'")
            embedding = self.current_model.encode([query], normalize_embeddings=normalize)
            
            # Convert to list for ChromaDB compatibility
            if isinstance(embedding, np.ndarray):
                embedding_list = embedding[0].tolist()
            else:
                embedding_list = embedding[0]
                
            logger.debug(f"âœ… Generated query embedding: {len(embedding_list)} dimensions")
            return embedding_list
            
        except Exception as e:
            logger.error(f"âŒ Failed to encode query: {e}")
            return None

    def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the current model.
        
        Returns:
            Model information dictionary
        """
        if not self.current_model:
            return {
                "name": self.current_model_name,
                "status": "No custom model loaded",
                "embedding_dim": "Unknown",
                "type": "ChromaDB default"
            }
        
        try:
            # Try to get embedding dimension
            test_embedding = self.current_model.encode(["test"], normalize_embeddings=False)
            embedding_dim = len(test_embedding[0]) if len(test_embedding) > 0 else "Unknown"
        except:
            embedding_dim = "Unknown"
            
        return {
            "name": self.current_model_name,
            "status": "Loaded and ready",
            "embedding_dim": embedding_dim,
            "type": "SentenceTransformer",
            "cached_models": list(self.models.keys())
        }

    def intelligent_chunk_text(self, text: str, chunk_size: int = 400, overlap: int = 50) -> List[str]:
        """
        Intelligent text chunking for Vietnamese text.
        
        Args:
            text: Text to chunk
            chunk_size: Maximum chunk size
            overlap: Overlap between chunks
            
        Returns:
            List of text chunks
        """
        # TÃ¡ch theo Ä‘oáº¡n vÄƒn trÆ°á»›c
        paragraphs = [p.strip() for p in text.split("\\n\\n") if p.strip()]

        chunks = []
        current_chunk = ""

        for para in paragraphs:
            # Náº¿u Ä‘oáº¡n vÄƒn quÃ¡ dÃ i, chia nhá» hÆ¡n
            if len(para) > chunk_size:
                # TÃ¡ch theo cÃ¢u
                sentences = []
                temp_sentence = ""

                for char in para:
                    temp_sentence += char
                    if char in ".!?":
                        # Kiá»ƒm tra khÃ´ng pháº£i sá»‘ tháº­p phÃ¢n
                        remaining_text = para[para.find(temp_sentence) + len(temp_sentence):]
                        next_chars = remaining_text[:3] if len(remaining_text) >= 3 else remaining_text
                        if not any(c.isdigit() for c in next_chars):
                            sentences.append(temp_sentence.strip())
                            temp_sentence = ""

                # ThÃªm cÃ¢u cuá»‘i náº¿u cÃ²n
                if temp_sentence.strip():
                    sentences.append(temp_sentence.strip())

                # Combine sentences to chunks
                for sentence in sentences:
                    if len(current_chunk) + len(sentence) <= chunk_size:
                        current_chunk += " " + sentence if current_chunk else sentence
                    else:
                        if current_chunk.strip():
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence
            else:
                # Äoáº¡n vÄƒn ngáº¯n, thÃªm trá»±c tiáº¿p
                if len(current_chunk) + len(para) <= chunk_size:
                    current_chunk += "\\n" + para if current_chunk else para
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    current_chunk = para

        # ThÃªm chunk cuá»‘i
        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return [chunk for chunk in chunks if len(chunk.strip()) > 20]
