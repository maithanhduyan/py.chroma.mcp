# Generated by Copilot
"""
Batch processing optimization for embeddings.
Handles adaptive batching, caching, and memory management.
"""

import asyncio
import hashlib
import logging
import os
from typing import Dict, List, Optional, Any, Tuple, Generator, Union
from functools import lru_cache
import json
from concurrent.futures import ThreadPoolExecutor
import time

# Try to import psutil, fall back to basic memory estimation
try:
    import psutil

    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

logger = logging.getLogger(__name__)


class EmbeddingCache:
    """LRU cache for embeddings with hash-based lookup."""

    def __init__(self, max_size: int = 10000):
        self.max_size = max_size
        self.cache: Dict[str, List[float]] = {}
        self.access_order: List[str] = []
        self.cache_hits = 0
        self.cache_misses = 0

    def _hash_text(self, text: str) -> str:
        """Generate hash for text content."""
        return hashlib.md5(text.encode("utf-8")).hexdigest()

    def get(self, text: str) -> Optional[List[float]]:
        """Get embedding from cache."""
        text_hash = self._hash_text(text)
        if text_hash in self.cache:
            # Move to end (most recently used)
            self.access_order.remove(text_hash)
            self.access_order.append(text_hash)
            self.cache_hits += 1
            return self.cache[text_hash]

        self.cache_misses += 1
        return None

    def set(self, text: str, embedding: List[float]) -> None:
        """Store embedding in cache."""
        text_hash = self._hash_text(text)

        # Remove if already exists
        if text_hash in self.cache:
            self.access_order.remove(text_hash)

        # Add to cache
        self.cache[text_hash] = embedding
        self.access_order.append(text_hash)

        # Evict oldest if over limit
        while len(self.cache) > self.max_size:
            oldest_hash = self.access_order.pop(0)
            del self.cache[oldest_hash]

    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0

        return {
            "cache_size": len(self.cache),
            "max_size": self.max_size,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "hit_rate": round(hit_rate, 3),
            "memory_usage_mb": self._estimate_memory_usage(),
        }

    def _estimate_memory_usage(self) -> float:
        """Estimate cache memory usage in MB."""
        if not self.cache:
            return 0.0

        # Estimate: hash (32 bytes) + embedding (768 * 4 bytes typical)
        avg_embedding_size = 768 * 4  # 4 bytes per float
        estimated_bytes = len(self.cache) * (32 + avg_embedding_size)
        return estimated_bytes / (1024 * 1024)


class BatchProcessor:
    """Optimized batch processor for embeddings."""

    def __init__(self, embedding_manager, cache_size: int = 10000):
        self.embedding_manager = embedding_manager
        self.cache = EmbeddingCache(cache_size)
        self.batch_stats = {
            "total_batches": 0,
            "total_documents": 0,
            "cache_hits": 0,
            "processing_time": 0.0,
        }

        # Adaptive batch sizing parameters
        self.min_batch_size = 1
        self.max_batch_size = 1000
        self.default_batch_size = 100
        self.current_batch_size = self.default_batch_size

    def _get_available_memory_gb(self) -> float:
        """Get available system memory in GB."""
        try:
            if HAS_PSUTIL:
                memory = psutil.virtual_memory()
                return memory.available / (1024**3)
            else:
                # Fallback estimation
                return 4.0  # Conservative default
        except Exception:
            return 4.0  # Default assumption

    def _estimate_optimal_batch_size(self, text_lengths: List[int]) -> int:
        """Estimate optimal batch size based on memory and text characteristics."""
        available_memory_gb = self._get_available_memory_gb()
        avg_text_length = sum(text_lengths) / len(text_lengths) if text_lengths else 100

        # Conservative memory estimation:
        # - Text processing: ~2x text size
        # - Model computation: ~1KB per token
        # - Embedding storage: ~3KB per document (768 dims * 4 bytes)
        estimated_memory_per_doc = (avg_text_length * 2 + avg_text_length + 3072) / (
            1024**3
        )

        # Use 50% of available memory for safety
        safe_memory_gb = available_memory_gb * 0.5
        optimal_size = (
            int(safe_memory_gb / estimated_memory_per_doc)
            if estimated_memory_per_doc > 0
            else self.default_batch_size
        )

        # Clamp to reasonable bounds
        optimal_size = max(self.min_batch_size, min(optimal_size, self.max_batch_size))

        logger.debug(
            f"ðŸ§  Estimated optimal batch size: {optimal_size} "
            f"(mem: {available_memory_gb:.1f}GB, avg_len: {avg_text_length:.0f})"
        )

        return optimal_size

    def _create_batches(
        self, texts: List[str], batch_size: int
    ) -> Generator[List[str], None, None]:
        """Create batches of texts."""
        for i in range(0, len(texts), batch_size):
            yield texts[i : i + batch_size]

    def _check_cache_batch(
        self, texts: List[str]
    ) -> Tuple[List[str], List[int], List[Optional[List[float]]]]:
        """Check cache for batch of texts and return uncached texts, indices, and cached embeddings."""
        uncached_texts = []
        uncached_indices = []
        embeddings: List[Optional[List[float]]] = [None] * len(texts)

        for i, text in enumerate(texts):
            cached_embedding = self.cache.get(text)
            if cached_embedding is not None:
                embeddings[i] = cached_embedding
                self.batch_stats["cache_hits"] += 1
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)

        return uncached_texts, uncached_indices, embeddings

    async def encode_documents_optimized(
        self, texts: List[str], normalize: bool = True, show_progress: bool = True
    ) -> Optional[List[List[float]]]:
        """
        Optimized batch processing for document encoding.

        Args:
            texts: List of texts to encode
            normalize: Whether to normalize embeddings
            show_progress: Whether to show progress for large batches

        Returns:
            List of embeddings or None if failed
        """
        if not texts:
            return []

        start_time = time.time()

        # Estimate optimal batch size
        text_lengths = [len(text) for text in texts]
        optimal_batch_size = self._estimate_optimal_batch_size(text_lengths)

        logger.info(
            f"ðŸš€ Processing {len(texts)} documents with batch size {optimal_batch_size}"
        )

        all_embeddings: List[Optional[List[float]]] = [None] * len(texts)
        total_uncached = 0

        try:
            # Process in batches
            batches = list(self._create_batches(texts, optimal_batch_size))

            for batch_idx, batch_texts in enumerate(batches):
                batch_start_idx = batch_idx * optimal_batch_size

                # Check cache for this batch
                uncached_texts, uncached_indices, batch_embeddings = (
                    self._check_cache_batch(batch_texts)
                )
                total_uncached += len(uncached_texts)

                # Process uncached texts
                if uncached_texts:
                    logger.debug(
                        f"ðŸ”„ Processing batch {batch_idx + 1}/{len(batches)}: "
                        f"{len(uncached_texts)}/{len(batch_texts)} uncached"
                    )

                    # Use the original embedding manager for actual computation
                    uncached_embeddings = self.embedding_manager._do_encode_documents(
                        uncached_texts, normalize
                    )

                    if uncached_embeddings is None:
                        logger.error(f"âŒ Failed to encode batch {batch_idx + 1}")
                        return None

                    # Cache and assign the new embeddings
                    for i, embedding in enumerate(uncached_embeddings):
                        original_idx = batch_start_idx + uncached_indices[i]
                        all_embeddings[original_idx] = embedding
                        self.cache.set(uncached_texts[i], embedding)

                # Assign cached embeddings
                for i, embedding in enumerate(batch_embeddings):
                    if embedding is not None:
                        original_idx = batch_start_idx + i
                        all_embeddings[original_idx] = embedding

                # Progress reporting
                if show_progress and len(batches) > 1:
                    progress = (batch_idx + 1) / len(batches) * 100
                    logger.info(
                        f"ðŸ“Š Progress: {progress:.1f}% ({batch_idx + 1}/{len(batches)} batches)"
                    )

            # Update statistics
            processing_time = time.time() - start_time
            self.batch_stats["total_batches"] += len(batches)
            self.batch_stats["total_documents"] += len(texts)
            self.batch_stats["processing_time"] += processing_time

            # Calculate efficiency metrics
            cache_hit_rate = (
                (len(texts) - total_uncached) / len(texts) if len(texts) > 0 else 0
            )
            throughput = len(texts) / processing_time if processing_time > 0 else 0

            logger.info(
                f"âœ… Processed {len(texts)} documents in {processing_time:.2f}s "
                f"(cache hit rate: {cache_hit_rate:.1%}, throughput: {throughput:.1f} docs/s)"
            )

            # Convert to final format, filtering out None values
            final_embeddings: List[List[float]] = []
            for embedding in all_embeddings:
                if embedding is not None:
                    final_embeddings.append(embedding)
                else:
                    logger.error("âŒ Found None embedding in final results")
                    return None

            return final_embeddings

        except Exception as e:
            logger.error(f"âŒ Batch processing failed: {e}")
            return None

    async def encode_queries_parallel(
        self, queries: List[str], normalize: bool = True, max_workers: int = 4
    ) -> Optional[List[List[float]]]:
        """
        Process multiple queries in parallel for improved throughput.

        Args:
            queries: List of query texts
            normalize: Whether to normalize embeddings
            max_workers: Maximum number of parallel workers

        Returns:
            List of query embeddings or None if failed
        """
        if not queries:
            return []

        start_time = time.time()

        # For small numbers of queries, use sequential processing
        if len(queries) <= 2:
            return await self.encode_documents_optimized(
                queries, normalize, show_progress=False
            )

        logger.info(
            f"ðŸ”„ Processing {len(queries)} queries in parallel (max workers: {max_workers})"
        )

        try:
            # Use ThreadPoolExecutor for parallel processing
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Create tasks for each query
                loop = asyncio.get_event_loop()
                tasks = []

                for query in queries:
                    # Check cache first
                    cached_embedding = self.cache.get(query)
                    if cached_embedding is not None:
                        tasks.append(
                            asyncio.create_task(self._async_return(cached_embedding))
                        )
                        self.batch_stats["cache_hits"] += 1
                    else:
                        # Submit to thread pool
                        future = executor.submit(
                            self.embedding_manager._do_encode_query, query, normalize
                        )
                        task = loop.run_in_executor(None, lambda f=future: f.result())
                        tasks.append(task)

                # Wait for all tasks to complete
                embeddings = await asyncio.gather(*tasks, return_exceptions=True)
                # Process results and update cache
                final_embeddings = []
                for i, embedding in enumerate(embeddings):
                    if isinstance(embedding, Exception):
                        logger.error(f"âŒ Failed to process query {i}: {embedding}")
                        return None

                    # Ensure embedding is the right type
                    if not isinstance(embedding, list):
                        logger.error(
                            f"âŒ Invalid embedding type for query {i}: {type(embedding)}"
                        )
                        return None

                    # Cache the result if it's not from cache
                    if not self.cache.get(queries[i]):
                        self.cache.set(queries[i], embedding)

                    final_embeddings.append(embedding)

                processing_time = time.time() - start_time
                throughput = (
                    len(queries) / processing_time if processing_time > 0 else 0
                )

                logger.info(
                    f"âœ… Processed {len(queries)} queries in {processing_time:.2f}s "
                    f"(throughput: {throughput:.1f} queries/s)"
                )

                return final_embeddings

        except Exception as e:
            logger.error(f"âŒ Parallel query processing failed: {e}")
            return None

    async def _async_return(self, value):
        """Helper to return a value asynchronously."""
        return value

    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get comprehensive performance metrics."""
        cache_stats = self.cache.get_stats()

        # Calculate derived metrics
        total_docs = self.batch_stats["total_documents"]
        total_time = self.batch_stats["processing_time"]
        avg_throughput = total_docs / total_time if total_time > 0 else 0

        return {
            "batch_processing": {
                "total_batches_processed": self.batch_stats["total_batches"],
                "total_documents_processed": total_docs,
                "total_processing_time_seconds": round(total_time, 2),
                "average_throughput_docs_per_second": round(avg_throughput, 2),
                "current_batch_size": self.current_batch_size,
                "available_memory_gb": round(self._get_available_memory_gb(), 2),
            },
            "caching": cache_stats,
            "optimization_status": {
                "adaptive_batching": True,
                "embedding_caching": True,
                "memory_monitoring": True,
                "parallel_queries": True,
            },
        }

    def clear_cache(self) -> Dict[str, int]:
        """Clear the embedding cache."""
        cache_stats = self.cache.get_stats()
        self.cache = EmbeddingCache(self.cache.max_size)

        logger.info("ðŸ§¹ Embedding cache cleared")
        return {
            "cleared_entries": cache_stats["cache_size"],
            "freed_memory_mb": int(cache_stats["memory_usage_mb"]),
        }
