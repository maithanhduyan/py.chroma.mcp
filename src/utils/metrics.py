# Generated by Copilot
"""
Performance and quality metrics for ChromaDB MCP server.

Provides comprehensive monitoring and measurement capabilities for
vector database operations and system performance.
"""

import time
import logging
from typing import Dict, List, Any, Optional, Callable, TypeVar, Union, TYPE_CHECKING
from functools import wraps
from dataclasses import dataclass, field
from contextlib import contextmanager
import asyncio
from pathlib import Path
import json

# Optional import for psutil
try:
    import psutil

    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False
    psutil = None

logger = logging.getLogger(__name__)

# Type variables for generic functions
F = TypeVar("F", bound=Callable[..., Any])


@dataclass
class PerformanceMetrics:
    """
    Container for performance measurement data.

    Tracks execution time, memory usage, and system resource utilization.
    """

    operation_name: str
    execution_time: float
    memory_usage_mb: float
    cpu_usage_percent: float
    peak_memory_mb: float = 0.0
    start_time: float = field(default_factory=time.time)
    end_time: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert metrics to dictionary format.

        Returns:
            Dictionary representation of metrics
        """
        return {
            "operation_name": self.operation_name,
            "execution_time": self.execution_time,
            "memory_usage_mb": self.memory_usage_mb,
            "cpu_usage_percent": self.cpu_usage_percent,
            "peak_memory_mb": self.peak_memory_mb,
            "start_time": self.start_time,
            "end_time": self.end_time,
            "metadata": self.metadata,
        }


@dataclass
class QualityMetrics:
    """
    Container for data quality and accuracy metrics.

    Tracks embedding quality, retrieval accuracy, and system reliability.
    """

    embedding_quality_score: float = 0.0
    retrieval_accuracy: float = 0.0
    relevance_score: float = 0.0
    diversity_score: float = 0.0
    coherence_score: float = 0.0
    error_rate: float = 0.0
    success_rate: float = 0.0

    def calculate_overall_score(self) -> float:
        """
        Calculate overall quality score from individual metrics.

        Returns:
            Weighted average of quality metrics
        """
        weights = {
            "embedding_quality": 0.25,
            "retrieval_accuracy": 0.25,
            "relevance_score": 0.20,
            "diversity_score": 0.10,
            "coherence_score": 0.10,
            "success_rate": 0.10,
        }

        score = (
            self.embedding_quality_score * weights["embedding_quality"]
            + self.retrieval_accuracy * weights["retrieval_accuracy"]
            + self.relevance_score * weights["relevance_score"]
            + self.diversity_score * weights["diversity_score"]
            + self.coherence_score * weights["coherence_score"]
            + self.success_rate * weights["success_rate"]
        )

        return min(max(score, 0.0), 1.0)  # Clamp between 0 and 1


class MetricsCollector:
    """
    Centralized metrics collection and storage system.

    Provides thread-safe collection, aggregation, and persistence of metrics.
    """

    def __init__(self, storage_path: Optional[Path] = None) -> None:
        """
        Initialize metrics collector.

        Args:
            storage_path: Optional path for persisting metrics
        """
        self.performance_metrics: List[PerformanceMetrics] = []
        self.quality_metrics: List[QualityMetrics] = []
        self.storage_path = storage_path
        self._lock = asyncio.Lock()

        logger.info("ðŸ”¥ MetricsCollector initialized")

    async def add_performance_metric(self, metric: PerformanceMetrics) -> None:
        """
        Add performance metric to collection.

        Args:
            metric: Performance metric to add
        """
        async with self._lock:
            self.performance_metrics.append(metric)
            logger.debug(f"ðŸ“Š Added performance metric: {metric.operation_name}")

    async def add_quality_metric(self, metric: QualityMetrics) -> None:
        """
        Add quality metric to collection.

        Args:
            metric: Quality metric to add
        """
        async with self._lock:
            self.quality_metrics.append(metric)
            logger.debug("âœ¨ Added quality metric")

    def get_performance_summary(self) -> Dict[str, Any]:
        """
        Get summary statistics for performance metrics.

        Returns:
            Dictionary with performance summary statistics
        """
        if not self.performance_metrics:
            return {"total_operations": 0}

        execution_times = [m.execution_time for m in self.performance_metrics]
        memory_usages = [m.memory_usage_mb for m in self.performance_metrics]

        summary = {
            "total_operations": len(self.performance_metrics),
            "avg_execution_time": sum(execution_times) / len(execution_times),
            "max_execution_time": max(execution_times),
            "min_execution_time": min(execution_times),
            "avg_memory_usage": sum(memory_usages) / len(memory_usages),
            "max_memory_usage": max(memory_usages),
            "operations_by_type": self._group_by_operation(),
        }

        return summary

    def _group_by_operation(self) -> Dict[str, int]:
        """Group metrics by operation type."""
        operations = {}
        for metric in self.performance_metrics:
            op_name = metric.operation_name
            operations[op_name] = operations.get(op_name, 0) + 1
        return operations

    async def save_metrics(self) -> None:
        """
        Persist metrics to storage if storage path is configured.
        """
        if not self.storage_path:
            logger.warning("âš ï¸ No storage path configured for metrics")
            return

        try:
            metrics_data = {
                "performance_metrics": [m.to_dict() for m in self.performance_metrics],
                "quality_metrics": [
                    {
                        "embedding_quality": m.embedding_quality_score,
                        "retrieval_accuracy": m.retrieval_accuracy,
                        "relevance_score": m.relevance_score,
                        "overall_score": m.calculate_overall_score(),
                    }
                    for m in self.quality_metrics
                ],
                "timestamp": time.time(),
            }

            with open(self.storage_path, "w", encoding="utf-8") as f:
                json.dump(metrics_data, f, indent=2, ensure_ascii=False)

            logger.info(f"ðŸ’¾ Metrics saved to {self.storage_path}")

        except Exception as e:
            logger.error(f"âŒ Failed to save metrics: {e}")


def benchmark_function(func: F) -> F:
    """
    Decorator Ä‘á»ƒ benchmark function execution.

    Args:
        func: Function to benchmark

    Returns:
        Wrapped function with performance monitoring
    """

    @wraps(func)
    async def async_wrapper(*args, **kwargs):
        """Async wrapper for benchmarking."""
        start_time = time.time()
        start_memory = 0.0

        if HAS_PSUTIL and psutil:
            try:
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            except Exception:
                logger.warning("âš ï¸ Could not measure memory usage")

        try:
            if asyncio.iscoroutinefunction(func):
                result = await func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)

            execution_time = time.time() - start_time
            end_memory = 0.0

            if HAS_PSUTIL and psutil and start_memory > 0:
                try:
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                except Exception:
                    pass

            memory_usage = end_memory - start_memory if end_memory > 0 else 0.0

            logger.info(
                f"âš¡ {func.__name__} completed in {execution_time:.3f}s"
                + (f", memory delta: {memory_usage:.2f}MB" if memory_usage != 0 else "")
            )

            return result

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ {func.__name__} failed after {execution_time:.3f}s: {e}")
            raise

    @wraps(func)
    def sync_wrapper(*args, **kwargs):
        """Sync wrapper for benchmarking."""
        start_time = time.time()
        start_memory = 0.0

        if HAS_PSUTIL and psutil:
            try:
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            except Exception:
                logger.warning("âš ï¸ Could not measure memory usage")

        try:
            result = func(*args, **kwargs)

            execution_time = time.time() - start_time
            end_memory = 0.0

            if HAS_PSUTIL and psutil and start_memory > 0:
                try:
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                except Exception:
                    pass

            memory_usage = end_memory - start_memory if end_memory > 0 else 0.0

            logger.info(
                f"âš¡ {func.__name__} completed in {execution_time:.3f}s"
                + (f", memory delta: {memory_usage:.2f}MB" if memory_usage != 0 else "")
            )

            return result

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ {func.__name__} failed after {execution_time:.3f}s: {e}")
            raise

    # Return appropriate wrapper based on function type
    if asyncio.iscoroutinefunction(func):
        return async_wrapper  # type: ignore
    else:
        return sync_wrapper  # type: ignore


@contextmanager
def measure_memory_usage(operation_name: str):
    """
    Context manager Ä‘á»ƒ measure memory usage cá»§a operation.

    Args:
        operation_name: Name of operation being measured

    Yields:
        Dictionary with memory usage information
    """
    if not HAS_PSUTIL or psutil is None:
        # Fallback when psutil is not available
        memory_info = {
            "start_memory_mb": 0.0,
            "peak_memory_mb": 0.0,
            "operation_name": operation_name,
            "memory_tracking_available": False,
        }
        yield memory_info
        memory_info.update(
            {"end_memory_mb": 0.0, "memory_delta_mb": 0.0, "peak_memory_mb": 0.0}
        )
        logger.debug(f"ðŸ’¾ {operation_name} completed (memory tracking unavailable)")
        return

    try:
        process = psutil.Process()
        start_memory = process.memory_info().rss / 1024 / 1024
        peak_memory = start_memory

        memory_info = {
            "start_memory_mb": start_memory,
            "peak_memory_mb": peak_memory,
            "operation_name": operation_name,
            "memory_tracking_available": True,
        }

        try:
            yield memory_info

        finally:
            end_memory = process.memory_info().rss / 1024 / 1024
            memory_delta = end_memory - start_memory

            memory_info.update(
                {
                    "end_memory_mb": end_memory,
                    "memory_delta_mb": memory_delta,
                    "peak_memory_mb": max(peak_memory, end_memory),
                }
            )

            logger.info(
                f"ðŸ’¾ {operation_name} memory usage: "
                f"{memory_delta:+.2f}MB (peak: {memory_info['peak_memory_mb']:.2f}MB)"
            )

    except Exception as e:
        logger.warning(f"âš ï¸ Could not measure memory for {operation_name}: {e}")
        memory_info = {
            "start_memory_mb": 0.0,
            "end_memory_mb": 0.0,
            "memory_delta_mb": 0.0,
            "peak_memory_mb": 0.0,
            "operation_name": operation_name,
            "memory_tracking_available": False,
        }
        yield memory_info


@contextmanager
def track_execution_time(operation_name: str):
    """
    Context manager Ä‘á»ƒ track execution time cá»§a operation.

    Args:
        operation_name: Name of operation being tracked

    Yields:
        Dictionary with timing information
    """
    start_time = time.time()
    timing_info = {"operation_name": operation_name, "start_time": start_time}

    try:
        yield timing_info

    finally:
        end_time = time.time()
        execution_time = end_time - start_time

        timing_info.update({"end_time": end_time, "execution_time": execution_time})

        logger.info(f"â±ï¸ {operation_name} executed in {execution_time:.3f}s")


def calculate_embedding_similarity(
    embedding1: List[float], embedding2: List[float]
) -> float:
    """
    Calculate cosine similarity between two embeddings.

    Args:
        embedding1: First embedding vector
        embedding2: Second embedding vector

    Returns:
        Cosine similarity score between 0 and 1
    """
    import numpy as np

    if len(embedding1) != len(embedding2):
        raise ValueError("Embeddings must have same dimension")

    # Convert to numpy arrays
    vec1 = np.array(embedding1)
    vec2 = np.array(embedding2)

    # Calculate cosine similarity
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)

    if norm1 == 0 or norm2 == 0:
        return 0.0

    similarity = dot_product / (norm1 * norm2)

    # Normalize to 0-1 range (cosine similarity is -1 to 1)
    return (similarity + 1.0) / 2.0


def analyze_retrieval_quality(
    query: str,
    retrieved_documents: List[str],
    expected_documents: Optional[List[str]] = None,
) -> QualityMetrics:
    """
    Analyze quality of document retrieval results.

    Args:
        query: Original query string
        retrieved_documents: Documents returned by retrieval
        expected_documents: Optional expected/ground truth documents

    Returns:
        QualityMetrics with calculated scores
    """
    metrics = QualityMetrics()

    if not retrieved_documents:
        logger.warning("âš ï¸ No documents retrieved for quality analysis")
        return metrics

    # Basic quality checks
    metrics.success_rate = 1.0 if retrieved_documents else 0.0

    # Calculate diversity (unique content ratio)
    unique_docs = set(retrieved_documents)
    metrics.diversity_score = len(unique_docs) / len(retrieved_documents)

    # Calculate relevance based on query term presence
    query_terms = set(query.lower().split())
    relevance_scores = []

    for doc in retrieved_documents:
        doc_terms = set(doc.lower().split())
        term_overlap = len(query_terms.intersection(doc_terms))
        doc_relevance = term_overlap / len(query_terms) if query_terms else 0.0
        relevance_scores.append(doc_relevance)

    metrics.relevance_score = sum(relevance_scores) / len(relevance_scores)

    # Calculate accuracy if expected documents provided
    if expected_documents:
        expected_set = set(expected_documents)
        retrieved_set = set(retrieved_documents)

        intersection = expected_set.intersection(retrieved_set)
        union = expected_set.union(retrieved_set)

        metrics.retrieval_accuracy = len(intersection) / len(union) if union else 0.0

    # Coherence score (average document length as proxy)
    avg_doc_length = sum(len(doc.split()) for doc in retrieved_documents) / len(
        retrieved_documents
    )
    metrics.coherence_score = min(avg_doc_length / 100.0, 1.0)  # Normalize

    logger.info(
        f"ðŸ“ˆ Quality analysis: relevance={metrics.relevance_score:.3f}, "
        f"diversity={metrics.diversity_score:.3f}"
    )

    return metrics


# Global metrics collector instance
_global_collector: Optional[MetricsCollector] = None


def get_metrics_collector() -> MetricsCollector:
    """
    Get global metrics collector instance.

    Returns:
        Global MetricsCollector instance
    """
    global _global_collector

    if _global_collector is None:
        _global_collector = MetricsCollector()
        logger.info("ðŸ”§ Global metrics collector initialized")

    return _global_collector


def reset_metrics_collector() -> None:
    """Reset global metrics collector."""
    global _global_collector
    _global_collector = None
    logger.info("ðŸ”„ Global metrics collector reset")
