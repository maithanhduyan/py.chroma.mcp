# Cáº¥u trÃºc Dá»± Ã¡n nhÆ° sau:

```
..\py.chroma.mcp
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ embedding
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ batch_processor.py
â”‚   â”‚   â”œâ”€â”€ chunker.py
â”‚   â”‚   â””â”€â”€ manager.py
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ tools.py
â”‚   â””â”€â”€ utils
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ error_handler.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ metrics.py
â”‚       â”œâ”€â”€ metrics_simple.py
â”‚       â”œâ”€â”€ model_loader.py
â”‚       â””â”€â”€ validators.py
â””â”€â”€ uv.lock
```

# Danh sÃ¡ch chi tiáº¿t cÃ¡c file:

## File ..\py.chroma.mcp\src\config.py:
```python
"""
Simple configuration module for the MCP server.
Generated by Copilot

ASYNC HANDLING NOTES:
- FastMCP manages its own asyncio event loop internally
- Do NOT wrap mcp.run() in asyncio.run() - this causes conflicts
- All @mcp.tool() functions are async by design
- Server initialization should be synchronous, letting FastMCP handle async
- This prevents "cannot be called from a running event loop" errors

GPU/CPU DEVICE HANDLING:
- EmbeddingManager automatically detects best available device (CUDA, MPS, CPU)
- Models are loaded with appropriate device settings for optimal performance
- Fallback to CPU if GPU not available or fails
- Device information shown in model info for transparency

MODEL SELECTION OPTIMIZATION:
- Single unified model: nomic-ai/nomic-embed-text-v2-moe (Latest MoE, SoTA multilingual)
- No complex fallback chain - simplified configuration
- ChromaDB default embedding as ultimate fallback
- Progress indication for model downloads
- Local caching to speed up subsequent loads
"""

import os
import sys
from pathlib import Path
from typing import Dict, Any

# Flag to prevent multiple path setup
_paths_setup = False


def setup_project_paths() -> Path:
    """
    Setup Python path for the project to enable absolute imports.
    Only runs once to prevent circular imports.

    Returns:
        Path to the project root directory
    """
    global _paths_setup

    if _paths_setup:
        return Path(__file__).resolve().parent.parent

    # Get the project root (parent of src directory)
    current_file = Path(__file__).resolve()
    src_dir = current_file.parent  # src directory
    project_root = src_dir.parent  # project root directory

    # Add src directory to Python path if not already there
    src_path = str(src_dir)
    if src_path not in sys.path:
        sys.path.insert(0, src_path)

    _paths_setup = True
    return project_root


# Initialize paths when module is imported (only once)
PROJECT_ROOT = setup_project_paths()


# Default configuration constants
DEFAULT_EMBEDDING_MODEL = "nomic-ai/nomic-embed-text-v2-moe"  # Latest MoE version, SoTA multilingual (305M active/475M total), 768D embeddings
FALLBACK_EMBEDDING_MODELS = [
    "nomic-ai/nomic-embed-text-v2-moe",  # Primary model - Latest MoE version, SoTA multilingual
]
DEFAULT_CHUNK_SIZE = 400
DEFAULT_CHUNK_OVERLAP = 50
DEFAULT_CHROMA_DB_PATH = "./chroma_db"


def get_embedding_config() -> Dict[str, Any]:
    """
    Get embedding configuration with default values and environment overrides.

    Returns:
        Dictionary containing embedding configuration:
        - default_model: Default embedding model to use (nomic-ai/nomic-embed-text-v2-moe)
        - fallback_models: Single model in list format for consistency
        - chunk_size: Size of text chunks for embedding
        - chunk_overlap: Overlap between chunks
        - enable_custom_embeddings: Whether to allow custom embedding models
    """
    return {
        "default_model": os.getenv("DEFAULT_EMBEDDING_MODEL", DEFAULT_EMBEDDING_MODEL),
        "fallback_models": FALLBACK_EMBEDDING_MODELS,
        "chunk_size": int(os.getenv("EMBEDDING_CHUNK_SIZE", str(DEFAULT_CHUNK_SIZE))),
        "chunk_overlap": int(
            os.getenv("EMBEDDING_CHUNK_OVERLAP", str(DEFAULT_CHUNK_OVERLAP))
        ),
        "enable_custom_embeddings": os.getenv(
            "ENABLE_CUSTOM_EMBEDDINGS", "true"
        ).lower()
        == "true",
    }


def get_chroma_config() -> Dict[str, Any]:
    """
    Get ChromaDB configuration with default values.

    Returns:
        Dictionary containing ChromaDB configuration:
        - database_path: Path to ChromaDB database
        - host: ChromaDB host (for client mode)
        - port: ChromaDB port (for client mode)
    """
    return {
        "database_path": os.getenv("CHROMA_DB_PATH", DEFAULT_CHROMA_DB_PATH),
        "host": os.getenv("CHROMA_HOST", "localhost"),
        "port": int(os.getenv("CHROMA_PORT", "8000")),
    }

```

## File ..\py.chroma.mcp\src\server.py:
```python
#!/usr/bin/env python3
"""
FastMCP ChromaDB Server with Advanced Embedding Integration
Supports ChromaDB, custom embeddings, Vietnamese text processing, and semantic search
Uses asyncio for optimal performance
Generated by Copilot
"""

import sys
import asyncio
import time
from typing import Dict, Any, Optional

# Import config to setup project paths automatically (main entry point)
import config
from config import get_embedding_config

# Import centralized logger
from utils.logger import get_logger
from utils.model_loader import get_model_loading_manager

# Import FastMCP and tools
from tools import mcp

logger = get_logger(__name__)

# Ensure UTF-8 encoding for stdout
try:
    if hasattr(sys.stdout, "reconfigure"):
        getattr(sys.stdout, "reconfigure")(encoding="utf-8")
except (AttributeError, TypeError):
    import codecs

    if hasattr(sys.stdout, "buffer"):
        sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer)


def log_server_startup():
    """Log server startup information."""
    startup_info = {
        "server": "ChromaDB FastMCP Server",
        "version": "2.0.0",
        "embedding_support": True,
        "vietnamese_support": True,
        "features": [
            "ChromaDB Integration",
            "Vietnamese Text Processing",
            "Intelligent Chunking",
            "Semantic Search",
            "Cross-lingual Support",
        ],
        "tools": [
            "echo",
            "list_collections",
            "create_collection",
            "delete_collection",
            "add_documents",
            "query_collection",
            "get_embedding_model_info",
            "configure_embedding_model",
            "chunk_text_intelligent",
            "get_performance_metrics",  # New metrics tool
        ],
        "startup_time": time.strftime("%Y-%m-%d %H:%M:%S"),
    }

    logger.info("=" * 60)
    logger.info("ðŸš€ FastMCP ChromaDB Server Starting")
    logger.info("=" * 60)
    logger.info(f"ðŸ“‹ Server: {startup_info['server']} v{startup_info['version']}")
    logger.info(
        f"ðŸ¤– Embedding: nomic-ai/nomic-embed-text-v2-moe (SoTA multilingual MoE)"
    )
    logger.info(f"ðŸ‡»ðŸ‡³ Vietnamese: Fully Supported")
    logger.info(f"ðŸ› ï¸ Tools: {len(startup_info['tools'])} available")
    logger.info(f"â° Started: {startup_info['startup_time']}")
    logger.info("=" * 60)


def initialize_systems_sync() -> Dict[str, Any]:
    """
    Initialize ChromaDB and embedding systems synchronously.
    FastMCP handles its own async operations internally.

    Returns:
        Dictionary with initialization results"""
    logger.info("ðŸ”§ Initializing embedding systems...")

    # Import to trigger initialization
    from tools import get_chroma_client, get_embedding_manager

    # Initialize ChromaDB
    chroma_client = get_chroma_client()
    logger.info(f"âœ… ChromaDB initialized: {type(chroma_client).__name__}")

    # Initialize embeddings and try to load a fast model
    embedding_manager = get_embedding_manager()

    # Try to load a lightweight model for faster development
    logger.info("ðŸš€ Pre-loading embedding model for better user experience...")

    # Check if any model is already loaded (from cache)
    model_info = embedding_manager.get_model_info()
    if model_info and model_info.get("name") != "chromadb-default":
        model_name = model_info.get("name", "unknown")
        model_dim = model_info.get("embedding_dim", "unknown")
        device = model_info.get("device", "unknown")
        logger.info(
            f"âœ… Using cached embedding model: {model_name} ({model_dim}D) on {device}"
        )
    else:
        # Try to load the fastest model from our priority list
        logger.info("âš¡ Loading lightweight model for fast development...")
        success = embedding_manager.load_best_available_model()

        if success:
            model_info = embedding_manager.get_model_info()
            model_name = model_info.get("name", "unknown")
            model_dim = model_info.get("embedding_dim", "unknown")
            device = model_info.get("device", "unknown")
            logger.info(
                f"âœ… Pre-loaded embedding model: {model_name} ({model_dim}D) on {device}"
            )
        else:
            logger.info("âš ï¸ No custom model loaded, will use ChromaDB default")

    return {
        "chroma_client": chroma_client,
        "embedding_manager": embedding_manager,
        "model_info": model_info,
    }


def preload_embedding_model() -> bool:
    """
    Preload an embedding model for faster development.
    Tries to use cached models first for speed.

    Returns:
        True if a model was successfully loaded
    """
    logger.info("ðŸš€ Pre-loading embedding model...")

    try:
        from tools import get_embedding_manager

        embedding_manager = get_embedding_manager()

        # Check if we already have a model loaded from cache
        model_info = embedding_manager.get_model_info()
        if model_info and model_info.get("name") != "chromadb-default":
            logger.info(f"âœ… Model already loaded: {model_info.get('name')}")
            return True

        # Try to load the best available model
        success = embedding_manager.load_best_available_model()
        if success:
            model_info = embedding_manager.get_model_info()
            logger.info(
                f"âœ… Pre-loaded: {model_info.get('name')} ({model_info.get('embedding_dim')}D)"
            )
            return True
        else:
            logger.warning("âš ï¸ No custom models could be loaded")
            return False

    except Exception as e:
        logger.error(f"âŒ Failed to preload model: {e}")
        return False


async def async_preload_embedding_model() -> bool:
    """
    Async preload embedding model using model_loader for proper synchronization.

    Returns:
        True if a model was successfully loaded
    """
    logger.info("ðŸš€ Async pre-loading embedding model with proper synchronization...")

    try:
        from tools import get_embedding_manager

        embedding_manager = get_embedding_manager()
        model_loader = get_model_loading_manager()

        # Use async model loading with proper synchronization
        success = await model_loader.ensure_model_loaded(
            embedding_manager, force_reload=False
        )

        if success:
            model_info = embedding_manager.get_model_info()
            model_name = model_info.get("name", "unknown")
            model_dim = model_info.get("embedding_dim", "unknown")
            device = model_info.get("device", "unknown")
            logger.info(f"âœ… Async pre-loaded: {model_name} ({model_dim}D) on {device}")
            return True
        else:
            logger.warning("âš ï¸ Async model loading failed, will use ChromaDB default")
            return False

    except Exception as e:
        logger.error(f"âŒ Async model preload failed: {e}")
        return False


async def async_main():
    """
    Async main server entry point vá»›i proper model loading.
    """
    try:
        # Log startup information
        log_server_startup()

        # Async preload embedding model with proper synchronization
        logger.info("ðŸ”¥ Async pre-loading embedding model for faster development...")
        preload_success = await async_preload_embedding_model()

        if preload_success:
            logger.info("âš¡ Model pre-loaded successfully with async loading!")
        else:
            logger.info("âš ï¸ Model pre-loading failed - will use ChromaDB default")

        # Initialize remaining systems
        systems = initialize_systems_sync()

        logger.info("ðŸŽ¯ FastMCP server ready for connections")
        logger.info("=" * 60)

        # Note: FastMCP mcp.run() will handle its own async context
        # We'll run it in a separate thread to avoid blocking
        import threading

        def run_mcp_server():
            from tools import mcp

            mcp.run()

        # Start MCP server in background thread
        mcp_thread = threading.Thread(target=run_mcp_server, daemon=True)
        mcp_thread.start()

        # Keep main thread alive
        logger.info("ðŸš€ Server running... Press Ctrl+C to stop")
        while True:
            await asyncio.sleep(1)

    except KeyboardInterrupt:
        logger.info("\nðŸ›‘ Server shutdown requested by user")
    except Exception as e:
        logger.error(f"ðŸ’¥ Server startup failed: {e}")
        logger.exception("Server startup error details:")
        raise
    finally:
        logger.info("ðŸ”š FastMCP ChromaDB Server stopped")


def main():
    """
    Main server entry point - runs async main.
    """
    try:
        # Run async main
        asyncio.run(async_main())
    except KeyboardInterrupt:
        logger.info("\nðŸ›‘ Server shutdown by user")
    except Exception as e:
        logger.error(f"ðŸ’¥ Failed to start async server: {e}")
        # Fallback to sync mode
        logger.info("ðŸ”„ Falling back to sync mode...")
        sync_main()


def sync_main():
    """
    Sync fallback main function (original implementation).
    """
    try:
        # Log startup information
        log_server_startup()

        # Preload embedding model first for faster user experience
        logger.info("ðŸ”¥ Pre-loading embedding model for faster development...")
        preload_success = preload_embedding_model()

        if preload_success:
            logger.info("âš¡ Model pre-loaded successfully - development ready!")
        else:
            logger.info("âš ï¸ Model pre-loading failed - will use ChromaDB default")

        # Initialize remaining systems
        systems = initialize_systems_sync()

        logger.info("ðŸŽ¯ FastMCP server ready for connections")
        logger.info("=" * 60)

        # Import and start FastMCP server - it handles its own event loop
        from tools import mcp

        # FastMCP manages its own asyncio context
        mcp.run()

    except KeyboardInterrupt:
        logger.info("\nðŸ›‘ Server shutdown requested by user")
    except Exception as e:
        logger.error(f"ðŸ’¥ Server startup failed: {e}")
        logger.exception("Server startup error details:")
        raise
    finally:
        logger.info("ðŸ”š FastMCP ChromaDB Server stopped")


if __name__ == "__main__":
    main()

```

## File ..\py.chroma.mcp\src\tools.py:
```python
"""
MCP Tools with FastMCP Integration - Cleaned Version
Provides ChromaDB integration and utility tools using MCP protocol
Generated by Copilot
"""

"""
Essential MCP Tools for ChromaDB:
BASIC TOOLS FOR PRODUCTION:
- echo() - Test tool
- list_collections() - List all collections
- chunk_text_intelligent() - Smart text chunking
- create_collection() - Create new collection
- add_documents() - Add documents with embeddings
- query_collection() - Query with semantic search
- delete_collection() - Delete collection

ADVANCED TOOLS FOR DEVELOPMENT:
- get_embedding_model_info() - Get current model info
- configure_embedding_model() - Change embedding model
- load_default_embedding_model() - Load default model from config
- get_performance_metrics() - Performance stats
- clear_embedding_cache() - Clear embedding cache
- get_cache_stats() - Cache statistics
- batch_process_documents() - Batch processing
"""

import logging
from typing import Dict, Any, List, Optional
import chromadb
from chromadb.config import Settings
from chromadb.types import Metadata
from mcp.server.fastmcp import FastMCP

# Import config to setup project paths automatically
import config
from config import get_embedding_config

from embedding import EmbeddingManager
from embedding.chunker import chunk_text_intelligent as _chunk_text_intelligent
from utils.metrics import (
    MetricsCollector,
)
from utils.error_handler import (
    handle_mcp_tool_errors,
    get_error_tracker,
    OperationContext,
)
from utils.logger import get_logger
from utils.model_loader import get_model_loading_manager

HAS_METRICS = True

logger = get_logger(__name__)

# Initialize FastMCP instance
mcp = FastMCP("chroma-embedding-tools")

# Global instances
_chroma_client = None
_embedding_manager = None


def get_chroma_client():
    """Get or create the global Chroma client instance."""
    global _chroma_client
    if _chroma_client is None:
        _chroma_client = chromadb.PersistentClient(path="./chroma_db")
    return _chroma_client


def get_embedding_manager():
    """Get or create the global embedding manager instance with single model configuration."""
    global _embedding_manager
    if _embedding_manager is None:
        _embedding_manager = EmbeddingManager()
        logger.debug("ðŸ§  EmbeddingManager initialized (model will be loaded async)")
    return _embedding_manager


async def ensure_embedding_model_loaded(force_reload: bool = False) -> bool:
    """Ensure embedding model is loaded with proper async synchronization.

    Args:
        force_reload: Whether to force reload the model

    Returns:
        True if model loaded successfully
    """
    embedding_manager = get_embedding_manager()
    model_loader = get_model_loading_manager()

    return await model_loader.ensure_model_loaded(embedding_manager, force_reload)


##### Core ChromaDB Tools with Embedding Integration #####


@mcp.tool()
@handle_mcp_tool_errors("echo")
async def echo(message: str) -> str:
    """Echo back the input message (useful for testing)."""
    return f"Echo: {message}"


@mcp.tool()
@handle_mcp_tool_errors("list_collections")
async def list_collections() -> List[str]:
    """List all collection names in the ChromaDB."""
    client = get_chroma_client()
    collections = client.list_collections()
    collection_names = [coll.name for coll in collections] if collections else []
    logger.info(f"ðŸ“š Found {len(collection_names)} collections")
    return collection_names


@mcp.tool()
@handle_mcp_tool_errors("create_collection")
async def create_collection(
    name: str, metadata: Optional[Dict[str, Any]] = None
) -> str:
    """Create a new ChromaDB collection."""
    if not name or not name.strip():
        raise ValueError("Collection name cannot be empty")

    name = name.strip()
    client = get_chroma_client()

    with OperationContext("create_collection", collection_name=name):
        collection = client.create_collection(name=name, metadata=metadata)
        logger.info(f"âœ¨ Collection '{name}' created successfully")
        return f"Collection '{name}' created successfully"


@mcp.tool()
@handle_mcp_tool_errors("delete_collection")
async def delete_collection(name: str) -> str:
    """Delete a ChromaDB collection."""
    if not name or not name.strip():
        raise ValueError("Collection name cannot be empty")

    name = name.strip()
    client = get_chroma_client()

    with OperationContext("delete_collection", collection_name=name):
        client.delete_collection(name)
        logger.info(f"ðŸ—‘ï¸ Collection '{name}' deleted successfully")
        return f"Collection '{name}' deleted successfully"


@mcp.tool()
@handle_mcp_tool_errors("add_documents")
async def add_documents(
    collection_name: str,
    documents: List[str],
    ids: Optional[List[str]] = None,
    metadatas: Optional[List[Dict[str, Any]]] = None,
) -> str:
    """Add documents to a ChromaDB collection with custom embeddings."""
    if not documents:
        raise ValueError("Documents list cannot be empty")

    if not collection_name or not collection_name.strip():
        raise ValueError("Collection name cannot be empty")

    collection_name = collection_name.strip()

    # Clean and validate documents
    clean_documents = []
    for i, doc in enumerate(documents):
        if not isinstance(doc, str):
            logger.warning(f"Document {i} converted from {type(doc)} to string")
            doc = str(doc)
        doc = doc.strip()
        if doc:  # Only add non-empty documents
            clean_documents.append(doc)

    if not clean_documents:
        raise ValueError("No valid documents after cleaning")

    # Generate IDs if not provided
    if ids is None:
        ids = [f"doc_{i}" for i in range(len(clean_documents))]

    client = get_chroma_client()

    # Ensure model is loaded before proceeding
    await ensure_embedding_model_loaded()
    embedding_manager = get_embedding_manager()

    with OperationContext(
        "add_documents", collection_name=collection_name, doc_count=len(clean_documents)
    ):
        collection = client.get_or_create_collection(collection_name)

        # Try using custom embedding model from config
        embeddings = None
        model_name = "ChromaDB default"

        if embedding_manager.current_model is not None:
            try:
                with OperationContext(
                    "generate_embeddings", doc_count=len(clean_documents)
                ):
                    # Use direct model encoding for single model config
                    embeddings = embedding_manager.encode_documents(
                        clean_documents, normalize=True, use_batch_optimization=False
                    )

                model_info = embedding_manager.get_model_info()
                model_name = model_info.get("name", "unknown")
                logger.info(f"âœ¨ Using custom embeddings: {model_name}")

            except Exception as e:
                logger.warning(
                    f"âš ï¸ Custom embedding failed, using ChromaDB default: {e}"
                )
                embeddings = None

        # Add documents with or without custom embeddings
        if embeddings is not None:
            collection.add(
                documents=clean_documents,
                embeddings=embeddings,  # type: ignore
                ids=ids[: len(clean_documents)],
                metadatas=metadatas[: len(clean_documents)] if metadatas else None,  # type: ignore
            )
        else:
            collection.add(
                documents=clean_documents,
                ids=ids[: len(clean_documents)],
                metadatas=metadatas[: len(clean_documents)] if metadatas else None,  # type: ignore
            )

        result_message = f"Added {len(clean_documents)} documents to '{collection_name}' using {model_name} embeddings"
        logger.info(f"ðŸ“„ {result_message}")
        return result_message


@mcp.tool()
@handle_mcp_tool_errors("query_collection")
async def query_collection(
    collection_name: str,
    query_texts: List[str],
    n_results: int = 10,
    where: Optional[Dict[str, Any]] = None,
) -> Any:
    """Query a ChromaDB collection with semantic search."""
    if not query_texts:
        raise ValueError("Query texts cannot be empty")

    if not collection_name or not collection_name.strip():
        raise ValueError("Collection name cannot be empty")

    if n_results <= 0:
        raise ValueError("n_results must be greater than 0")

    collection_name = collection_name.strip()
    client = get_chroma_client()

    # Ensure model is loaded before proceeding
    await ensure_embedding_model_loaded()
    embedding_manager = get_embedding_manager()

    with OperationContext(
        "query_collection",
        collection_name=collection_name,
        query_count=len(query_texts),
        n_results=n_results,
    ):
        collection = client.get_collection(collection_name)

        # Try using custom query embeddings from config
        query_embeddings = None
        model_name = "ChromaDB default"

        if embedding_manager.current_model is not None and len(query_texts) > 0:
            try:
                with OperationContext(
                    "generate_query_embeddings", query_count=len(query_texts)
                ):
                    # Generate embeddings for each query text
                    embeddings_list = []
                    for query_text in query_texts:
                        embedding = embedding_manager.encode_query(
                            query_text, normalize=True, use_cache=False
                        )
                        if embedding:
                            embeddings_list.append(embedding)

                    if embeddings_list:
                        query_embeddings = embeddings_list
                        model_info = embedding_manager.get_model_info()
                        model_name = model_info.get("name", "unknown")
                        logger.info(f"âœ¨ Using custom query embeddings: {model_name}")
                    else:
                        logger.warning("âš ï¸ No valid query embeddings generated")

            except Exception as e:
                logger.warning(
                    f"âš ï¸ Custom query embedding failed, using ChromaDB default: {e}"
                )
                query_embeddings = None

        # Perform query
        if query_embeddings:
            results = collection.query(
                query_embeddings=query_embeddings,  # type: ignore
                n_results=n_results,
                where=where,
            )
        else:
            results = collection.query(
                query_texts=query_texts,
                n_results=n_results,
                where=where,
            )

        # Log results summary
        result_dict = dict(results)  # type: ignore
        total_results = 0
        if (
            result_dict.get("documents")
            and isinstance(result_dict["documents"], list)
            and len(result_dict["documents"]) > 0
        ):
            total_results = len(result_dict["documents"][0])
        logger.info(f"ðŸ” Query returned {total_results} results using {model_name}")

        return result_dict


##### Embedding Management Tools #####


@mcp.tool()
@handle_mcp_tool_errors("get_embedding_model_info")
async def get_embedding_model_info() -> Dict[str, Any]:
    """Get information about current embedding model."""
    # Ensure model is loaded before getting info
    await ensure_embedding_model_loaded()
    embedding_manager = get_embedding_manager()
    model_info = embedding_manager.get_model_info()
    if model_info:
        logger.info(f"ðŸ“Š Current model: {model_info.get('name', 'unknown')}")
        return model_info
    else:
        logger.warning("âš ï¸ No model currently loaded")
        return {"error": "No model currently loaded"}


@mcp.tool()
@handle_mcp_tool_errors("configure_embedding_model")
async def configure_embedding_model(model_name: str, force_reload: bool = False) -> str:
    """Configure the embedding model to use."""
    if not model_name or not model_name.strip():
        raise ValueError("Model name cannot be empty")

    model_name = model_name.strip()
    embedding_manager = get_embedding_manager()

    with OperationContext(
        "configure_embedding_model", model_name=model_name, force_reload=force_reload
    ):
        # Use async loading vá»›i proper synchronization
        model_loader = get_model_loading_manager()
        success = await model_loader.ensure_model_loaded(
            embedding_manager, force_reload
        )
        if success:
            model_info = embedding_manager.get_model_info()
            if model_info:
                model_name_info = model_info.get("name", model_name)
                dimension = model_info.get("embedding_dim", "unknown")
                result_message = f"Successfully configured embedding model: {model_name_info} (dim: {dimension})"
                logger.info(f"ðŸŽ¯ {result_message}")
                return result_message
            else:
                result_message = f"Model loaded but info unavailable: {model_name}"
                logger.warning(f"âš ï¸ {result_message}")
                return result_message
        else:
            error_message = (
                f"Failed to load model '{model_name}'. Using ChromaDB default."
            )
            logger.error(f"âŒ {error_message}")
            raise Exception(error_message)


@mcp.tool()
@handle_mcp_tool_errors("load_default_embedding_model")
async def load_default_embedding_model() -> str:
    """Load the default embedding model from configuration."""
    embedding_manager = get_embedding_manager()
    embedding_config = get_embedding_config()

    default_model = embedding_config["default_model"]
    logger.info(f"ðŸ”„ Loading default model from config: {default_model}")

    # Use async loading Ä‘á»ƒ Ä‘áº£m báº£o proper synchronization
    success = await ensure_embedding_model_loaded(force_reload=True)

    if success:
        model_info = embedding_manager.get_model_info()
        if model_info and model_info.get("status") == "Loaded and ready":
            result_message = f"Successfully loaded default model: {default_model}"
            logger.info(f"âœ… {result_message}")
            return result_message
        else:
            result_message = (
                f"Default model loaded but info unavailable: {default_model}"
            )
            logger.warning(f"âš ï¸ {result_message}")
            return result_message
    else:
        error_message = (
            f"Failed to load default model '{default_model}'. Using ChromaDB default."
        )
        logger.error(f"âŒ {error_message}")
        raise Exception(error_message)


@mcp.tool()
@handle_mcp_tool_errors("chunk_text_intelligent")
async def chunk_text_intelligent(
    text: str, chunk_size: int = 400, overlap: int = 50
) -> List[str]:
    """Intelligently chunk text for better embedding (Vietnamese optimized)."""
    if not text or not text.strip():
        raise ValueError("Text cannot be empty")

    if chunk_size <= 0:
        raise ValueError("Chunk size must be greater than 0")

    if overlap < 0:
        raise ValueError("Overlap cannot be negative")

    if overlap >= chunk_size:
        raise ValueError("Overlap must be less than chunk size")

    # Validate input
    if not isinstance(text, str):
        text = str(text)

    text = text.strip()

    with OperationContext(
        "chunk_text_intelligent",
        text_length=len(text),
        chunk_size=chunk_size,
        overlap=overlap,
    ):
        # Ensure valid encoding
        try:
            text.encode("utf-8")
        except UnicodeEncodeError:
            text = text.encode("utf-8", errors="ignore").decode("utf-8")

        chunks = _chunk_text_intelligent(text, chunk_size, overlap)

        # Validate and clean output chunks
        cleaned_chunks = []
        for chunk in chunks:
            if isinstance(chunk, str) and chunk.strip():
                try:
                    chunk.encode("utf-8")
                    cleaned_chunks.append(chunk.strip())
                except UnicodeEncodeError:
                    cleaned_chunk = chunk.encode("utf-8", errors="ignore").decode(
                        "utf-8"
                    )
                    if cleaned_chunk.strip():
                        cleaned_chunks.append(cleaned_chunk.strip())

        logger.info(f"ðŸ“ Text chunked into {len(cleaned_chunks)} chunks")
        return cleaned_chunks


@mcp.tool()
@handle_mcp_tool_errors("get_performance_metrics")
async def get_performance_metrics() -> Any:
    """Get performance metrics for embedding operations."""
    embedding_manager = get_embedding_manager()

    with OperationContext("get_performance_metrics"):
        metrics = embedding_manager.get_metrics()

        logger.info("ðŸ“Š Retrieved performance metrics")
        return {
            "message": "Performance metrics retrieved successfully",
            "metrics": metrics,
        }


@mcp.tool()
@handle_mcp_tool_errors("clear_embedding_cache")
async def clear_embedding_cache() -> str:
    """Clear the embedding cache to free memory."""
    embedding_manager = get_embedding_manager()

    with OperationContext("clear_embedding_cache"):
        cache_stats = embedding_manager.clear_embedding_cache()

        if cache_stats is not None:
            message = f"Embedding cache cleared: {cache_stats['cleared_entries']} entries, freed {cache_stats['freed_memory_mb']} MB"
            logger.info(f"ðŸ§¹ {message}")
            return message
        else:
            message = (
                "No embedding cache available to clear (batch processing not enabled)"
            )
            logger.info(f"â„¹ï¸ {message}")
            return message


@mcp.tool()
@handle_mcp_tool_errors("get_cache_stats")
async def get_cache_stats() -> Any:
    """Get embedding cache statistics."""
    embedding_manager = get_embedding_manager()

    with OperationContext("get_cache_stats"):
        cache_stats = embedding_manager.get_cache_stats()

        if cache_stats is not None:
            logger.info("ðŸ“Š Retrieved cache statistics")
            return {
                "message": "Cache statistics retrieved successfully",
                "cache_stats": cache_stats,
            }
        else:
            return {
                "message": "No cache available (batch processing not enabled)",
                "cache_stats": None,
            }


@mcp.tool()
@handle_mcp_tool_errors("batch_process_documents")
async def batch_process_documents(
    texts: List[str], normalize: bool = True, show_progress: bool = True
) -> Any:
    """Process multiple documents with optimized batch processing."""
    if not texts:
        raise ValueError("Documents list cannot be empty")

    embedding_manager = get_embedding_manager()

    with OperationContext("batch_process_documents", doc_count=len(texts)):
        if embedding_manager.batch_processor is None:
            raise ValueError(
                "Batch processing not available - no embedding model loaded"
            )

        logger.info(f"ðŸš€ Starting batch processing for {len(texts)} documents")
        # Use the optimized batch processor directly
        embeddings = await embedding_manager.batch_processor.encode_documents_optimized(
            texts, normalize, show_progress
        )

        if embeddings is None:
            raise RuntimeError("Batch processing failed")

        result_message = (
            f"Successfully processed {len(texts)} documents with batch optimization"
        )
        logger.info(f"âœ… {result_message}")

        return {
            "message": result_message,
            "document_count": len(texts),
            "embedding_dimensions": len(embeddings[0]) if embeddings else 0,
            "processing_optimized": True,
        }

```

## File ..\py.chroma.mcp\src\__init__.py:
```python
"""
py-chroma-mcp: A simple Model Context Protocol server with ChromaDB integration.
Generated by Copilot
"""

__version__ = "0.1.0"
__author__ = "Mai ThÃ nh Duy An vs Copilot"
__email__ = "tiachop0102@gmail.com"

```

## File ..\py.chroma.mcp\src\embedding\batch_processor.py:
```python
# Generated by Copilot
"""
Batch processing optimization for embeddings.
Handles adaptive batching, caching, and memory management.
"""

import asyncio
import hashlib
import os
from typing import Dict, List, Optional, Any, Tuple, Generator, Union
from functools import lru_cache
import json
from concurrent.futures import ThreadPoolExecutor
import time

from utils.logger import get_logger

# Try to import psutil, fall back to basic memory estimation
try:
    import psutil

    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

logger = get_logger(__name__)


class EmbeddingCache:
    """LRU cache for embeddings with hash-based lookup."""

    def __init__(self, max_size: int = 10000):
        self.max_size = max_size
        self.cache: Dict[str, List[float]] = {}
        self.access_order: List[str] = []
        self.cache_hits = 0
        self.cache_misses = 0

    def _hash_text(self, text: str) -> str:
        """Generate hash for text content."""
        return hashlib.md5(text.encode("utf-8")).hexdigest()

    def get(self, text: str) -> Optional[List[float]]:
        """Get embedding from cache."""
        text_hash = self._hash_text(text)
        if text_hash in self.cache:
            # Move to end (most recently used)
            self.access_order.remove(text_hash)
            self.access_order.append(text_hash)
            self.cache_hits += 1
            return self.cache[text_hash]

        self.cache_misses += 1
        return None

    def set(self, text: str, embedding: List[float]) -> None:
        """Store embedding in cache."""
        text_hash = self._hash_text(text)

        # Remove if already exists
        if text_hash in self.cache:
            self.access_order.remove(text_hash)

        # Add to cache
        self.cache[text_hash] = embedding
        self.access_order.append(text_hash)

        # Evict oldest if over limit
        while len(self.cache) > self.max_size:
            oldest_hash = self.access_order.pop(0)
            del self.cache[oldest_hash]

    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0

        return {
            "cache_size": len(self.cache),
            "max_size": self.max_size,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "hit_rate": round(hit_rate, 3),
            "memory_usage_mb": self._estimate_memory_usage(),
        }

    def _estimate_memory_usage(self) -> float:
        """Estimate cache memory usage in MB."""
        if not self.cache:
            return 0.0

        # Estimate: hash (32 bytes) + embedding (768 * 4 bytes typical)
        avg_embedding_size = 768 * 4  # 4 bytes per float
        estimated_bytes = len(self.cache) * (32 + avg_embedding_size)
        return estimated_bytes / (1024 * 1024)


class BatchProcessor:
    """Optimized batch processor for embeddings."""

    def __init__(self, embedding_manager, cache_size: int = 10000):
        self.embedding_manager = embedding_manager
        self.cache = EmbeddingCache(cache_size)
        self.batch_stats = {
            "total_batches": 0,
            "total_documents": 0,
            "cache_hits": 0,
            "processing_time": 0.0,
        }  # Adaptive batch sizing parameters
        self.min_batch_size = 1
        self.max_batch_size = 1000
        self.default_batch_size = 100
        self.current_batch_size = self.default_batch_size

    def _get_available_memory_gb(self) -> float:
        """Get available system memory in GB."""
        try:
            if HAS_PSUTIL:
                import psutil

                memory = psutil.virtual_memory()
                return memory.available / (1024**3)
            else:
                # Fallback estimation
                return 4.0  # Conservative default
        except Exception:
            return 4.0  # Default assumption

    def _estimate_optimal_batch_size(self, text_lengths: List[int]) -> int:
        """Estimate optimal batch size based on memory and text characteristics."""
        available_memory_gb = self._get_available_memory_gb()
        avg_text_length = sum(text_lengths) / len(text_lengths) if text_lengths else 100

        # Conservative memory estimation:
        # - Text processing: ~2x text size
        # - Model computation: ~1KB per token
        # - Embedding storage: ~3KB per document (768 dims * 4 bytes)
        estimated_memory_per_doc = (avg_text_length * 2 + avg_text_length + 3072) / (
            1024**3
        )

        # Use 50% of available memory for safety
        safe_memory_gb = available_memory_gb * 0.5
        optimal_size = (
            int(safe_memory_gb / estimated_memory_per_doc)
            if estimated_memory_per_doc > 0
            else self.default_batch_size
        )

        # Clamp to reasonable bounds
        optimal_size = max(self.min_batch_size, min(optimal_size, self.max_batch_size))

        logger.debug(
            f"ðŸ§  Estimated optimal batch size: {optimal_size} "
            f"(mem: {available_memory_gb:.1f}GB, avg_len: {avg_text_length:.0f})"
        )

        return optimal_size

    def _create_batches(
        self, texts: List[str], batch_size: int
    ) -> Generator[List[str], None, None]:
        """Create batches of texts."""
        for i in range(0, len(texts), batch_size):
            yield texts[i : i + batch_size]

    def _check_cache_batch(
        self, texts: List[str]
    ) -> Tuple[List[str], List[int], List[Optional[List[float]]]]:
        """Check cache for batch of texts and return uncached texts, indices, and cached embeddings."""
        uncached_texts = []
        uncached_indices = []
        embeddings: List[Optional[List[float]]] = [None] * len(texts)

        for i, text in enumerate(texts):
            cached_embedding = self.cache.get(text)
            if cached_embedding is not None:
                embeddings[i] = cached_embedding
                self.batch_stats["cache_hits"] += 1
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)

        return uncached_texts, uncached_indices, embeddings

    async def encode_documents_optimized(
        self, texts: List[str], normalize: bool = True, show_progress: bool = True
    ) -> Optional[List[List[float]]]:
        """
        Optimized batch processing for document encoding.

        Args:
            texts: List of texts to encode
            normalize: Whether to normalize embeddings
            show_progress: Whether to show progress for large batches

        Returns:
            List of embeddings or None if failed
        """
        if not texts:
            return []

        start_time = time.time()

        # Estimate optimal batch size
        text_lengths = [len(text) for text in texts]
        optimal_batch_size = self._estimate_optimal_batch_size(text_lengths)

        logger.info(
            f"ðŸš€ Processing {len(texts)} documents with batch size {optimal_batch_size}"
        )

        all_embeddings: List[Optional[List[float]]] = [None] * len(texts)
        total_uncached = 0

        try:
            # Process in batches
            batches = list(self._create_batches(texts, optimal_batch_size))

            for batch_idx, batch_texts in enumerate(batches):
                batch_start_idx = batch_idx * optimal_batch_size

                # Check cache for this batch
                uncached_texts, uncached_indices, batch_embeddings = (
                    self._check_cache_batch(batch_texts)
                )
                total_uncached += len(uncached_texts)

                # Process uncached texts
                if uncached_texts:
                    logger.debug(
                        f"ðŸ”„ Processing batch {batch_idx + 1}/{len(batches)}: "
                        f"{len(uncached_texts)}/{len(batch_texts)} uncached"
                    )

                    # Use the original embedding manager for actual computation
                    uncached_embeddings = self.embedding_manager._do_encode_documents(
                        uncached_texts, normalize
                    )

                    if uncached_embeddings is None:
                        logger.error(f"âŒ Failed to encode batch {batch_idx + 1}")
                        return None

                    # Cache and assign the new embeddings
                    for i, embedding in enumerate(uncached_embeddings):
                        original_idx = batch_start_idx + uncached_indices[i]
                        all_embeddings[original_idx] = embedding
                        self.cache.set(uncached_texts[i], embedding)

                # Assign cached embeddings
                for i, embedding in enumerate(batch_embeddings):
                    if embedding is not None:
                        original_idx = batch_start_idx + i
                        all_embeddings[original_idx] = embedding

                # Progress reporting
                if show_progress and len(batches) > 1:
                    progress = (batch_idx + 1) / len(batches) * 100
                    logger.info(
                        f"ðŸ“Š Progress: {progress:.1f}% ({batch_idx + 1}/{len(batches)} batches)"
                    )

            # Update statistics
            processing_time = time.time() - start_time
            self.batch_stats["total_batches"] += len(batches)
            self.batch_stats["total_documents"] += len(texts)
            self.batch_stats["processing_time"] += processing_time

            # Calculate efficiency metrics
            cache_hit_rate = (
                (len(texts) - total_uncached) / len(texts) if len(texts) > 0 else 0
            )
            throughput = len(texts) / processing_time if processing_time > 0 else 0

            logger.info(
                f"âœ… Processed {len(texts)} documents in {processing_time:.2f}s "
                f"(cache hit rate: {cache_hit_rate:.1%}, throughput: {throughput:.1f} docs/s)"
            )

            # Convert to final format, filtering out None values
            final_embeddings: List[List[float]] = []
            for embedding in all_embeddings:
                if embedding is not None:
                    final_embeddings.append(embedding)
                else:
                    logger.error("âŒ Found None embedding in final results")
                    return None

            return final_embeddings

        except Exception as e:
            logger.error(f"âŒ Batch processing failed: {e}")
            return None

    async def encode_queries_parallel(
        self, queries: List[str], normalize: bool = True, max_workers: int = 4
    ) -> Optional[List[List[float]]]:
        """
        Process multiple queries in parallel for improved throughput.

        Args:
            queries: List of query texts
            normalize: Whether to normalize embeddings
            max_workers: Maximum number of parallel workers

        Returns:
            List of query embeddings or None if failed
        """
        if not queries:
            return []

        start_time = time.time()

        # For small numbers of queries, use sequential processing
        if len(queries) <= 2:
            return await self.encode_documents_optimized(
                queries, normalize, show_progress=False
            )

        logger.info(
            f"ðŸ”„ Processing {len(queries)} queries in parallel (max workers: {max_workers})"
        )

        try:
            # Use ThreadPoolExecutor for parallel processing
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Create tasks for each query
                loop = asyncio.get_event_loop()
                tasks = []

                for query in queries:
                    # Check cache first
                    cached_embedding = self.cache.get(query)
                    if cached_embedding is not None:
                        tasks.append(
                            asyncio.create_task(self._async_return(cached_embedding))
                        )
                        self.batch_stats["cache_hits"] += 1
                    else:
                        # Submit to thread pool
                        future = executor.submit(
                            self.embedding_manager._do_encode_query, query, normalize
                        )
                        task = loop.run_in_executor(None, lambda f=future: f.result())
                        tasks.append(task)

                # Wait for all tasks to complete
                embeddings = await asyncio.gather(*tasks, return_exceptions=True)
                # Process results and update cache
                final_embeddings = []
                for i, embedding in enumerate(embeddings):
                    if isinstance(embedding, Exception):
                        logger.error(f"âŒ Failed to process query {i}: {embedding}")
                        return None

                    # Ensure embedding is the right type
                    if not isinstance(embedding, list):
                        logger.error(
                            f"âŒ Invalid embedding type for query {i}: {type(embedding)}"
                        )
                        return None

                    # Cache the result if it's not from cache
                    if not self.cache.get(queries[i]):
                        self.cache.set(queries[i], embedding)

                    final_embeddings.append(embedding)

                processing_time = time.time() - start_time
                throughput = (
                    len(queries) / processing_time if processing_time > 0 else 0
                )

                logger.info(
                    f"âœ… Processed {len(queries)} queries in {processing_time:.2f}s "
                    f"(throughput: {throughput:.1f} queries/s)"
                )

                return final_embeddings

        except Exception as e:
            logger.error(f"âŒ Parallel query processing failed: {e}")
            return None

    async def _async_return(self, value):
        """Helper to return a value asynchronously."""
        return value

    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get comprehensive performance metrics."""
        cache_stats = self.cache.get_stats()

        # Calculate derived metrics
        total_docs = self.batch_stats["total_documents"]
        total_time = self.batch_stats["processing_time"]
        avg_throughput = total_docs / total_time if total_time > 0 else 0

        return {
            "batch_processing": {
                "total_batches_processed": self.batch_stats["total_batches"],
                "total_documents_processed": total_docs,
                "total_processing_time_seconds": round(total_time, 2),
                "average_throughput_docs_per_second": round(avg_throughput, 2),
                "current_batch_size": self.current_batch_size,
                "available_memory_gb": round(self._get_available_memory_gb(), 2),
            },
            "caching": cache_stats,
            "optimization_status": {
                "adaptive_batching": True,
                "embedding_caching": True,
                "memory_monitoring": True,
                "parallel_queries": True,
            },
        }

    def clear_cache(self) -> Dict[str, int]:
        """Clear the embedding cache."""
        cache_stats = self.cache.get_stats()
        self.cache = EmbeddingCache(self.cache.max_size)

        logger.info("ðŸ§¹ Embedding cache cleared")
        return {
            "cleared_entries": cache_stats["cache_size"],
            "freed_memory_mb": int(cache_stats["memory_usage_mb"]),
        }

```

## File ..\py.chroma.mcp\src\embedding\chunker.py:
```python
# Generated by Copilot
"""
Intelligent text chunking for optimal embedding generation.

This module provides advanced text chunking strategies optimized for Vietnamese text
and embedding models. It focuses on semantic coherence and embedding quality.

Features:
- Multiple chunking strategies (fixed, semantic, sentence-based)
- Vietnamese text optimization with proper tokenization
- Adaptive chunk sizing based on content type
- Overlap handling for context preservation
- Quality metrics for chunk assessment
"""

import re
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass
from enum import Enum
import unicodedata

from utils.logger import get_logger

# Vietnamese text processing - enhanced regex approach
HAS_UNDERTHESEA = False  # Fallback to enhanced regex due to build issues

logger = get_logger(__name__)

# Vietnamese-specific text patterns
VIETNAMESE_SENTENCE_PATTERNS = [
    r"[.!?]+\s+[A-ZÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬Ã‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†ÃÃŒá»ˆÄ¨á»ŠÃ“Ã’á»ŽÃ•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»žá» á»¢ÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´Ä]",
    r"[.!?]+\s+[0-9]",
    r"[.!?]+$",
]

VIETNAMESE_PUNCTUATION = '.,;:!?()[]{}""' '""â€¦â€“â€”'
VIETNAMESE_STOP_WORDS = {
    "vÃ ",
    "cá»§a",
    "trong",
    "vá»›i",
    "cho",
    "tá»«",
    "Ä‘á»ƒ",
    "vá»",
    "cÃ³",
    "Ä‘Æ°á»£c",
    "nÃ y",
    "Ä‘Ã³",
    "lÃ ",
    "má»™t",
    "cÃ¡c",
    "nhá»¯ng",
    "khi",
    "nhÆ°",
    "theo",
    "trÃªn",
    "dÆ°á»›i",
    "sau",
    "trÆ°á»›c",
}


class ChunkingStrategy(Enum):
    """Available text chunking strategies."""

    FIXED = "fixed"
    SEMANTIC = "semantic"
    SENTENCE = "sentence"
    PARAGRAPH = "paragraph"
    INTELLIGENT = "intelligent"


@dataclass
class ChunkInfo:
    """Information about a text chunk."""

    text: str
    start_pos: int
    end_pos: int
    chunk_id: int
    word_count: int
    char_count: int
    overlap_prev: int = 0
    overlap_next: int = 0
    quality_score: float = 0.0
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class ChunkingConfig:
    """Configuration for text chunking."""

    strategy: ChunkingStrategy = ChunkingStrategy.INTELLIGENT
    max_chunk_size: int = 400
    min_chunk_size: int = 50
    overlap_size: int = 50
    preserve_sentences: bool = True
    preserve_paragraphs: bool = True
    optimize_for_vietnamese: bool = True
    quality_threshold: float = 0.7


class TextChunker:
    """
    Advanced text chunker with multiple strategies and Vietnamese optimization.

    Provides intelligent chunking that maintains semantic coherence while
    optimizing for embedding model performance.
    """

    def __init__(self, config: Optional[ChunkingConfig] = None) -> None:
        """
        Initialize text chunker.

        Args:
            config: Optional chunking configuration
        """
        self.config = config or ChunkingConfig()
        self.chunk_cache: Dict[str, List[ChunkInfo]] = {}
        logger.debug(
            f"ðŸ”§ TextChunker initialized with strategy: {self.config.strategy}"
        )

    def chunk_text(
        self, text: str, strategy: Optional[ChunkingStrategy] = None
    ) -> List[ChunkInfo]:
        """
        Chunk text using specified or default strategy.

        Args:
            text: Text to chunk
            strategy: Optional chunking strategy override

        Returns:
            List of ChunkInfo objects
        """
        if not text or not text.strip():
            logger.warning("âš ï¸ Empty text provided for chunking")
            return []

        strategy = strategy or self.config.strategy
        cache_key = f"{hash(text)}_{strategy.value}_{self.config.max_chunk_size}"

        if cache_key in self.chunk_cache:
            logger.debug(f"ðŸ“¦ Using cached chunks for text (hash: {hash(text)})")
            return self.chunk_cache[cache_key]

        # Clean and normalize text
        cleaned_text = self._preprocess_text(text)

        # Apply chunking strategy
        if strategy == ChunkingStrategy.FIXED:
            chunks = self._chunk_fixed(cleaned_text)
        elif strategy == ChunkingStrategy.SEMANTIC:
            chunks = self._chunk_semantic(cleaned_text)
        elif strategy == ChunkingStrategy.SENTENCE:
            chunks = self._chunk_by_sentences(cleaned_text)
        elif strategy == ChunkingStrategy.PARAGRAPH:
            chunks = self._chunk_by_paragraphs(cleaned_text)
        elif strategy == ChunkingStrategy.INTELLIGENT:
            chunks = self._chunk_intelligent(cleaned_text)
        else:
            logger.warning(
                f"âš ï¸ Unknown chunking strategy: {strategy}, falling back to intelligent"
            )
            chunks = self._chunk_intelligent(cleaned_text)

        # Calculate quality scores
        chunks = self._calculate_quality_scores(chunks)

        # Cache results
        self.chunk_cache[cache_key] = chunks

        logger.info(
            f"âœ… Chunked text into {len(chunks)} chunks using {strategy.value} strategy"
        )
        return chunks

    def _preprocess_text(self, text: str) -> str:
        """
        Preprocess text for chunking.

        Args:
            text: Raw text to preprocess

        Returns:
            Cleaned and normalized text
        """
        # Normalize Unicode (important for Vietnamese)
        text = unicodedata.normalize("NFC", text)

        # Remove excessive whitespace
        text = re.sub(r"\s+", " ", text)

        # Fix common Vietnamese text issues
        if self.config.optimize_for_vietnamese:
            text = self._fix_vietnamese_text(text)

        return text.strip()

    def _fix_vietnamese_text(self, text: str) -> str:
        """
        Fix common Vietnamese text formatting issues.

        Args:
            text: Text to fix

        Returns:
            Fixed text
        """
        # Fix spacing around punctuation
        text = re.sub(r"\s+([.,;:!?])", r"\1", text)
        text = re.sub(
            r"([.,;:!?])\s*([a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘])",
            r"\1 \2",
            text,
            flags=re.IGNORECASE,
        )

        # Fix quotation marks
        text = re.sub(r'"([^"]*)"', r'"\1"', text)

        # Fix dash usage
        text = re.sub(r"\s*-\s*", " - ", text)

        return text

    def _chunk_fixed(self, text: str) -> List[ChunkInfo]:
        """
        Chunk text using fixed-size strategy.

        Args:
            text: Text to chunk

        Returns:
            List of fixed-size chunks
        """
        chunks = []
        words = text.split()

        if not words:
            return chunks

        current_chunk = []
        current_size = 0
        chunk_id = 0

        for word in words:
            if (
                current_size + len(word) + 1 > self.config.max_chunk_size
                and current_chunk
            ):
                # Create chunk
                chunk_text = " ".join(current_chunk)
                chunk_info = ChunkInfo(
                    text=chunk_text,
                    start_pos=0,  # Will be calculated later
                    end_pos=len(chunk_text),
                    chunk_id=chunk_id,
                    word_count=len(current_chunk),
                    char_count=len(chunk_text),
                )
                chunks.append(chunk_info)

                # Handle overlap
                if self.config.overlap_size > 0:
                    overlap_words = current_chunk[-self.config.overlap_size :]
                    current_chunk = overlap_words + [word]
                    current_size = (
                        sum(len(w) for w in current_chunk) + len(current_chunk) - 1
                    )
                else:
                    current_chunk = [word]
                    current_size = len(word)

                chunk_id += 1
            else:
                current_chunk.append(word)
                current_size += len(word) + (1 if current_chunk else 0)

        # Add final chunk
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            chunk_info = ChunkInfo(
                text=chunk_text,
                start_pos=0,
                end_pos=len(chunk_text),
                chunk_id=chunk_id,
                word_count=len(current_chunk),
                char_count=len(chunk_text),
            )
            chunks.append(chunk_info)

        return chunks

    def _chunk_by_sentences(self, text: str) -> List[ChunkInfo]:
        """
        Chunk text by sentences, respecting max chunk size.

        Args:
            text: Text to chunk

        Returns:
            List of sentence-based chunks
        """
        # Split by sentences (Vietnamese-optimized)
        sentences = self._split_sentences(text)

        if not sentences:
            return []

        chunks = []
        current_chunk = []
        current_size = 0
        chunk_id = 0

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            sentence_size = len(sentence)

            if (
                current_size + sentence_size > self.config.max_chunk_size
                and current_chunk
            ):
                # Create chunk from current sentences
                chunk_text = " ".join(current_chunk)
                chunk_info = ChunkInfo(
                    text=chunk_text,
                    start_pos=0,
                    end_pos=len(chunk_text),
                    chunk_id=chunk_id,
                    word_count=len(chunk_text.split()),
                    char_count=len(chunk_text),
                )
                chunks.append(chunk_info)

                current_chunk = [sentence]
                current_size = sentence_size
                chunk_id += 1
            else:
                current_chunk.append(sentence)
                current_size += sentence_size + (1 if len(current_chunk) > 1 else 0)

        # Add final chunk
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            chunk_info = ChunkInfo(
                text=chunk_text,
                start_pos=0,
                end_pos=len(chunk_text),
                chunk_id=chunk_id,
                word_count=len(chunk_text.split()),
                char_count=len(chunk_text),
            )
            chunks.append(chunk_info)

        return chunks

    def _chunk_by_paragraphs(self, text: str) -> List[ChunkInfo]:
        """
        Chunk text by paragraphs, splitting large paragraphs if needed.

        Args:
            text: Text to chunk

        Returns:
            List of paragraph-based chunks
        """
        paragraphs = text.split("\n\n")
        chunks = []
        chunk_id = 0

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            if len(paragraph) <= self.config.max_chunk_size:
                # Paragraph fits in one chunk
                chunk_info = ChunkInfo(
                    text=paragraph,
                    start_pos=0,
                    end_pos=len(paragraph),
                    chunk_id=chunk_id,
                    word_count=len(paragraph.split()),
                    char_count=len(paragraph),
                )
                chunks.append(chunk_info)
                chunk_id += 1
            else:
                # Split large paragraph
                para_chunks = self._chunk_by_sentences(paragraph)
                for chunk in para_chunks:
                    chunk.chunk_id = chunk_id
                    chunk_id += 1
                chunks.extend(para_chunks)

        return chunks

    def _chunk_semantic(self, text: str) -> List[ChunkInfo]:
        """
        Chunk text using semantic analysis (basic implementation).

        Args:
            text: Text to chunk

        Returns:
            List of semantically coherent chunks
        """
        # For now, use sentence-based chunking with semantic hints
        # Could be enhanced with actual semantic analysis
        sentences = self._split_sentences(text)

        if not sentences:
            return []

        chunks = []
        current_chunk = []
        current_size = 0
        chunk_id = 0

        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue

            sentence_size = len(sentence)

            # Simple semantic hint: if sentence starts with certain words, prefer to start new chunk
            semantic_break = self._is_semantic_break(sentence, i > 0)

            if (
                current_size + sentence_size > self.config.max_chunk_size
                and current_chunk
            ) or (semantic_break and current_size > self.config.min_chunk_size):
                # Create chunk
                chunk_text = " ".join(current_chunk)
                chunk_info = ChunkInfo(
                    text=chunk_text,
                    start_pos=0,
                    end_pos=len(chunk_text),
                    chunk_id=chunk_id,
                    word_count=len(chunk_text.split()),
                    char_count=len(chunk_text),
                )
                chunks.append(chunk_info)

                current_chunk = [sentence]
                current_size = sentence_size
                chunk_id += 1
            else:
                current_chunk.append(sentence)
                current_size += sentence_size + (1 if len(current_chunk) > 1 else 0)

        # Add final chunk
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            chunk_info = ChunkInfo(
                text=chunk_text,
                start_pos=0,
                end_pos=len(chunk_text),
                chunk_id=chunk_id,
                word_count=len(chunk_text.split()),
                char_count=len(chunk_text),
            )
            chunks.append(chunk_info)

        return chunks

    def _chunk_intelligent(self, text: str) -> List[ChunkInfo]:
        """
        Intelligent chunking combining multiple strategies.

        Args:
            text: Text to chunk

        Returns:
            List of intelligently chunked pieces
        """
        # Start with paragraph-based chunking
        para_chunks = self._chunk_by_paragraphs(text)

        # Refine with sentence analysis
        refined_chunks = []
        for chunk in para_chunks:
            if chunk.char_count > self.config.max_chunk_size:
                # Re-chunk large chunks using sentences
                sub_chunks = self._chunk_by_sentences(chunk.text)
                refined_chunks.extend(sub_chunks)
            elif chunk.char_count < self.config.min_chunk_size:
                # Try to merge small chunks
                if (
                    refined_chunks
                    and refined_chunks[-1].char_count + chunk.char_count
                    <= self.config.max_chunk_size
                ):
                    # Merge with previous chunk
                    prev_chunk = refined_chunks[-1]
                    merged_text = prev_chunk.text + " " + chunk.text
                    merged_chunk = ChunkInfo(
                        text=merged_text,
                        start_pos=prev_chunk.start_pos,
                        end_pos=prev_chunk.end_pos + chunk.char_count + 1,
                        chunk_id=prev_chunk.chunk_id,
                        word_count=prev_chunk.word_count + chunk.word_count,
                        char_count=len(merged_text),
                    )
                    refined_chunks[-1] = merged_chunk
                else:
                    refined_chunks.append(chunk)
            else:
                refined_chunks.append(chunk)

        # Re-number chunks
        for i, chunk in enumerate(refined_chunks):
            chunk.chunk_id = i

        return refined_chunks

    def _split_sentences(self, text: str) -> List[str]:
        """
        Split text into sentences using enhanced Vietnamese-optimized patterns.
        Uses improved regex-based approach for accurate Vietnamese sentence tokenization.

        Args:
            text: Text to split

        Returns:
            List of sentences
        """
        if not text or not text.strip():
            return []

        # Enhanced Vietnamese sentence splitting
        if self.config.optimize_for_vietnamese:
            return self._split_vietnamese_sentences(text)
        else:
            # Simple split for non-Vietnamese text
            sentences = re.split(r"[.!?]+\s+", text)
            return [s.strip() for s in sentences if s.strip() and len(s.strip()) > 3]

    def _split_vietnamese_sentences(self, text: str) -> List[str]:
        """
        Enhanced Vietnamese sentence splitting with better patterns.

        Args:
            text: Vietnamese text to split

        Returns:
            List of sentences
        """
        # Normalize text first
        text = unicodedata.normalize("NFC", text)

        # Enhanced Vietnamese sentence patterns
        # Pattern 1: Standard punctuation followed by capital letter or number
        pattern1 = r"([.!?]+)\s+([A-ZÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬Ã‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†ÃÃŒá»ˆÄ¨á»ŠÃ“Ã’á»ŽÃ•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»žá» á»¢ÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´Ä])"

        # Pattern 2: End of sentences
        pattern2 = r"([.!?]+)(\s*$)"

        # Pattern 3: Numbering (1. 2. etc.)
        pattern3 = r"([.!?]+)\s+(\d+\.)"

        # Common Vietnamese discourse markers
        discourse_markers = [
            "Tuy nhiÃªn",
            "NhÆ°ng",
            "Máº·t khÃ¡c",
            "BÃªn cáº¡nh Ä‘Ã³",
            "NgoÃ i ra",
            "Äáº§u tiÃªn",
            "Thá»© hai",
            "Thá»© ba",
            "Cuá»‘i cÃ¹ng",
            "Káº¿t luáº­n",
            "TÃ³m láº¡i",
            "VÃ­ dá»¥",
            "Cháº³ng háº¡n",
            "Cá»¥ thá»ƒ",
            "Theo Ä‘Ã³",
            "Do Ä‘Ã³",
        ]

        # Apply main patterns with proper group references
        # Add sentence breaks before discourse markers
        for marker in discourse_markers:
            marker_pattern = rf"([.!?]+)\s+({re.escape(marker)})"
            text = re.sub(marker_pattern, r"\1|SENT_BREAK|\2", text)

        # Apply main sentence patterns
        text = re.sub(pattern1, r"\1|SENT_BREAK|\2", text)
        text = re.sub(pattern2, r"\1|SENT_BREAK|", text)  # No group 2 for end pattern
        text = re.sub(pattern3, r"\1|SENT_BREAK|\2", text)

        # Split and clean
        parts = text.split("|SENT_BREAK|")
        sentences = []
        current_sentence = ""

        for part in parts:
            part = part.strip()
            if not part:
                continue

            # Accumulate very short fragments with previous sentence
            if (
                len(part) < 15
                and current_sentence
                and not any(p in part for p in [".", "!", "?"])
            ):
                current_sentence += " " + part
            else:
                if current_sentence:
                    sentences.append(current_sentence.strip())
                current_sentence = part

        # Add the last sentence
        if current_sentence and current_sentence.strip():
            sentences.append(current_sentence.strip())

        # Final filtering and cleaning
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            # Remove very short fragments and ensure meaningful content
            if sentence and len(sentence) > 10 and any(c.isalpha() for c in sentence):
                cleaned_sentences.append(sentence)

        logger.debug(
            f"ðŸ‡»ðŸ‡³ Enhanced Vietnamese sentence splitting: {len(cleaned_sentences)} sentences"
        )
        return cleaned_sentences

    def _is_semantic_break(self, sentence: str, has_previous: bool) -> bool:
        """
        Determine if sentence represents a semantic break.

        Args:
            sentence: Sentence to analyze
            has_previous: Whether there are previous sentences

        Returns:
            True if this sentence should start a new chunk
        """
        if not has_previous:
            return False

        # Vietnamese semantic break indicators
        break_indicators = [
            "Tuy nhiÃªn",
            "NhÆ°ng",
            "Máº·t khÃ¡c",
            "BÃªn cáº¡nh Ä‘Ã³",
            "NgoÃ i ra",
            "Äáº§u tiÃªn",
            "Thá»© hai",
            "Cuá»‘i cÃ¹ng",
            "Káº¿t luáº­n",
            "TÃ³m láº¡i",
            "VÃ­ dá»¥",
            "Cháº³ng háº¡n",
            "Cá»¥ thá»ƒ",
            "Theo Ä‘Ã³",
            "Do Ä‘Ã³",
        ]

        sentence_start = sentence[:50].lower()
        return any(
            indicator.lower() in sentence_start for indicator in break_indicators
        )

    def _calculate_quality_scores(self, chunks: List[ChunkInfo]) -> List[ChunkInfo]:
        """
        Calculate quality scores for chunks.

        Args:
            chunks: List of chunks to score

        Returns:
            Chunks with quality scores
        """
        for chunk in chunks:
            score = 0.0

            # Size score (prefer chunks near optimal size)
            optimal_size = self.config.max_chunk_size * 0.7
            size_ratio = min(chunk.char_count / optimal_size, 1.0)
            score += size_ratio * 0.3

            # Completeness score (prefer complete sentences/paragraphs)
            if chunk.text.endswith((".", "!", "?")):
                score += 0.2

            # Coherence score (basic word repetition analysis)
            words = chunk.text.lower().split()
            unique_words = set(words) - VIETNAMESE_STOP_WORDS
            if words:
                coherence = len(unique_words) / len(words)
                score += coherence * 0.3

            # Readability score (sentence length variance)
            sentences = self._split_sentences(chunk.text)
            if sentences:
                lengths = [len(s.split()) for s in sentences]
                if len(lengths) > 1:
                    avg_length = sum(lengths) / len(lengths)
                    variance = sum((l - avg_length) ** 2 for l in lengths) / len(
                        lengths
                    )
                    readability = 1.0 / (1.0 + variance / 100)  # Normalize variance
                    score += readability * 0.2

            chunk.quality_score = min(score, 1.0)

        return chunks

    def get_chunking_stats(self) -> Dict[str, Any]:
        """Get chunking statistics."""
        total_cached = len(self.chunk_cache)
        total_chunks = sum(len(chunks) for chunks in self.chunk_cache.values())

        return {
            "cached_texts": total_cached,
            "total_chunks_generated": total_chunks,
            "average_chunks_per_text": total_chunks / max(total_cached, 1),
            "config": {
                "strategy": self.config.strategy.value,
                "max_chunk_size": self.config.max_chunk_size,
                "min_chunk_size": self.config.min_chunk_size,
                "overlap_size": self.config.overlap_size,
            },
        }


# Convenience functions
def chunk_text_intelligent(
    text: str, max_chunk_size: int = 400, overlap: int = 50
) -> List[str]:
    """
    Quick intelligent chunking function.

    Args:
        text: Text to chunk
        max_chunk_size: Maximum chunk size in characters
        overlap: Overlap size in characters

    Returns:
        List of chunk texts
    """
    config = ChunkingConfig(
        strategy=ChunkingStrategy.INTELLIGENT,
        max_chunk_size=max_chunk_size,
        overlap_size=overlap,
    )
    chunker = TextChunker(config)
    chunks = chunker.chunk_text(text)
    return [chunk.text for chunk in chunks]


def chunk_text_semantic(text: str, max_chunk_size: int = 400) -> List[str]:
    """
    Quick semantic chunking function.

    Args:
        text: Text to chunk
        max_chunk_size: Maximum chunk size in characters

    Returns:
        List of chunk texts
    """
    config = ChunkingConfig(
        strategy=ChunkingStrategy.SEMANTIC, max_chunk_size=max_chunk_size
    )
    chunker = TextChunker(config)
    chunks = chunker.chunk_text(text)
    return [chunk.text for chunk in chunks]


def optimize_chunk_size(text: str, target_chunks: int) -> int:
    """
    Optimize chunk size for target number of chunks.

    Args:
        text: Sample text to analyze
        target_chunks: Target number of chunks

    Returns:
        Recommended chunk size
    """
    text_length = len(text)
    base_size = text_length // target_chunks

    # Test different sizes around the base
    sizes_to_test = [
        int(base_size * 0.8),
        base_size,
        int(base_size * 1.2),
        int(base_size * 1.5),
    ]

    best_size = base_size
    best_score = float("inf")

    for size in sizes_to_test:
        if size < 50:  # Minimum reasonable size
            continue

        chunker = TextChunker(ChunkingConfig(max_chunk_size=size))
        chunks = chunker.chunk_text(text)

        # Score based on how close we are to target and chunk quality
        chunk_count_diff = abs(len(chunks) - target_chunks)
        avg_quality = (
            sum(chunk.quality_score for chunk in chunks) / len(chunks) if chunks else 0
        )

        score = chunk_count_diff * 10 + (1.0 - avg_quality) * 5

        if score < best_score:
            best_score = score
            best_size = size

    logger.info(
        f"ðŸŽ¯ Optimized chunk size: {best_size} (targeting {target_chunks} chunks)"
    )
    return best_size


# Export symbols
__all__ = [
    "ChunkingStrategy",
    "ChunkInfo",
    "ChunkingConfig",
    "TextChunker",
    "chunk_text_intelligent",
    "chunk_text_semantic",
    "optimize_chunk_size",
]

```

## File ..\py.chroma.mcp\src\embedding\manager.py:
```python
# Generated by Copilot
"""
Embedding manager for MCP server.
Handles loading and managing different embedding models.
"""

import os
from typing import Dict, List, Optional, Any
import numpy as np

# Import config to setup project paths automatically
import config
from config import get_embedding_config

from utils.metrics import (
    track_execution_time,
    measure_memory_usage,
    MetricsCollector,
)
from utils.logger import get_logger

# Import batch processor for optimization (will be imported when needed)
# from .batch_processor import BatchProcessor

logger = get_logger(__name__)


class EmbeddingManager:
    """
    Manages embedding models for the MCP server.
    Supports model loading, caching, and fallback strategies.
    """

    def __init__(self):
        """Initialize the embedding manager."""
        self.models: Dict[str, Any] = {}  # Cache loaded models
        self.current_model = None
        self.current_model_name = (
            "chromadb-default"  # PRIORITY 1: Environment Variables (cao nháº¥t)
        )
        env_model = self._get_model_from_env()

        if env_model:
            self.default_model = env_model
            # Use env-based fallback priority
            self.model_priority = self._get_fallback_models_from_env()
        else:
            # PRIORITY 2: Fallback to config.py if env not set
            try:
                embedding_config = get_embedding_config()
                self.default_model = embedding_config["default_model"]
                self.model_priority = embedding_config["fallback_models"]
            except Exception as e:
                logger.warning(f"âš ï¸ Config load failed: {e}, using hardcoded defaults")
                # PRIORITY 3: Hardcoded defaults as final fallback
                self.default_model = "sentence-transformers/all-MiniLM-L6-v2"
                self.model_priority = [
                    "sentence-transformers/all-MiniLM-L6-v2",
                    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                ]

        # Initialize metrics collector
        self.metrics = MetricsCollector()
        # Simple operation counters for immediate metrics
        self.operation_counts = {
            "load_model": 0,
            "encode_documents": 0,
            "encode_query": 0,
        }
        # Initialize batch processor for optimization
        self.batch_processor: Optional[Any] = None

    def load_model(self, model_name: str, force_reload: bool = False) -> bool:
        """
        Load an embedding model.        Args:
            model_name: Name of the model to load
            force_reload: Whether to reload if already cached        Returns:
            True if model loaded successfully
        """
        with track_execution_time("load_model"):
            result = self._do_load_model(model_name, force_reload)
            if result:
                self.operation_counts["load_model"] += 1
            return result

    def _detect_optimal_device(self) -> str:
        """
        Detect the optimal device for running embedding models.

        Returns:
            Device string: "cuda", "mps" (Mac), or "cpu"
        """
        try:
            # Try to import torch to check for GPU availability
            import torch

            if torch.cuda.is_available():
                device = "cuda"
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                logger.info(f"ðŸš€ CUDA available: {gpu_count} GPU(s) - {gpu_name}")
                return device
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                logger.info("ðŸŽ MPS (Apple Silicon) available")
                return "mps"
            else:
                logger.info("ðŸ’» Using CPU for embeddings")
                return "cpu"

        except ImportError:
            logger.warning("âš ï¸ PyTorch not available, using CPU")
            return "cpu"
        except Exception as e:
            logger.warning(f"âš ï¸ Device detection failed: {e}, using CPU")
            return "cpu"

    def _do_load_model(self, model_name: str, force_reload: bool = False) -> bool:
        """Internal method for loading model without metrics tracking."""
        # Check if already loaded and cached
        if not force_reload and model_name in self.models:
            self.current_model = self.models[model_name]
            self.current_model_name = model_name
            logger.info(
                f"âœ… Using cached model: {model_name}"
            )  # Initialize batch processor with current model
            self._initialize_batch_processor()
            return True

        try:
            logger.info(f"ðŸ”„ Loading embedding model: {model_name}")
            logger.info("â³ This may take a few minutes for first-time downloads...")

            # Try sentence-transformers first
            try:
                from sentence_transformers import (
                    SentenceTransformer,
                )  # PRIORITY 1: Get device from environment variables

                device = self._get_device_from_env()

                # Load the model with appropriate device and progress indication
                logger.info(f"ðŸ“± Loading model on device: {device}")
                logger.info(
                    "ðŸ“¥ Downloading model files (if not cached)..."
                )  # PRIORITY 1: Get cache directory from environment variables
                cache_dir = self._get_cache_dir_from_env()

                model = SentenceTransformer(
                    model_name,
                    device=device,
                    cache_folder=cache_dir,
                    trust_remote_code=True,
                )

                # Cache the model
                self.models[model_name] = model
                self.current_model = model
                self.current_model_name = model_name

                # Initialize batch processor with new model
                self._initialize_batch_processor()

                logger.info(f"âœ… Successfully loaded model: {model_name} on {device}")
                logger.info(f"ðŸ’¾ Model cached to: {cache_dir}")
                return True

            except Exception as sentence_transformers_error:
                logger.warning(
                    f"âš ï¸ SentenceTransformers failed: {sentence_transformers_error}"
                )
                logger.info("ðŸ”„ Trying fallback with transformers AutoModel...")

                # Fallback to transformers AutoModel
                from transformers import AutoModel, AutoTokenizer
                import torch  # PRIORITY 1: Get device from environment variables

                device = self._get_device_from_env()
                logger.info(f"ðŸ“± Loading fallback model on device: {device}")

                # Load tokenizer and model
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name, trust_remote_code=True
                )
                model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
                model.to(device)
                model.eval()

                # Create a wrapper to make it compatible with our interface
                class TransformersModelWrapper:
                    def __init__(self, model, tokenizer, device):
                        self.model = model
                        self.tokenizer = tokenizer
                        self.device = device

                    def encode(self, texts, **kwargs):
                        """Encode texts to embeddings using transformers model."""
                        if isinstance(texts, str):
                            texts = [texts]

                        # Tokenize
                        inputs = self.tokenizer(
                            texts,
                            padding=True,
                            truncation=True,
                            return_tensors="pt",
                            max_length=512,
                        )
                        inputs = {k: v.to(self.device) for k, v in inputs.items()}

                        # Get embeddings
                        with torch.no_grad():
                            outputs = self.model(**inputs)
                            # Use [CLS] token embedding or mean pooling
                            embeddings = outputs.last_hidden_state.mean(dim=1)

                        # Normalize if requested
                        if kwargs.get("normalize_embeddings", True):
                            embeddings = torch.nn.functional.normalize(
                                embeddings, p=2, dim=1
                            )

                        return embeddings.cpu().numpy()

                wrapped_model = TransformersModelWrapper(model, tokenizer, device)

                # Cache the model
                self.models[model_name] = wrapped_model
                self.current_model = wrapped_model
                self.current_model_name = model_name

                # Note: batch processor may not work with fallback model
                self.batch_processor = None

                logger.info(
                    f"âœ… Successfully loaded fallback model: {model_name} on {device}"
                )
                return True

        except Exception as e:
            logger.error(f"âŒ Failed to load model {model_name}: {e}")
            return False

    def _initialize_batch_processor(self) -> None:
        """Initialize the batch processor with the current model."""
        if self.current_model is not None:
            # Import here to avoid circular dependency
            from .batch_processor import BatchProcessor

            self.batch_processor = BatchProcessor(self, cache_size=10000)
            logger.debug(
                "ðŸš€ Batch processor initialized for optimized embedding processing"
            )

    def load_default_model(self) -> bool:
        """
        Load the default embedding model from configuration.

        Returns:
            True if default model loaded successfully
        """
        logger.info(f"ðŸ”„ Loading default model: {self.default_model}")
        return self.load_model(self.default_model)

    def load_best_available_model(self) -> bool:
        """
        Load the best available embedding model from priority list.
        First tries the default model, then fallback models.

        Returns:
            True if any model was loaded successfully
        """
        logger.info("ðŸ”„ Loading best available embedding model...")

        # First try the default model
        logger.info(f"ðŸ”„ Trying default model: {self.default_model}")
        if self.load_model(self.default_model):
            logger.info(f"âœ… Successfully loaded default model: {self.default_model}")
            return True
        else:
            logger.warning(f"âŒ Failed to load default model: {self.default_model}")

        # Then try fallback models
        for model_name in self.model_priority:
            if model_name == self.default_model:
                continue  # Skip already tried default model

            logger.info(f"ðŸ”„ Trying fallback model: {model_name}")

            if self.load_model(model_name):
                logger.info(f"âœ… Successfully loaded fallback model: {model_name}")
                return True
            else:
                logger.warning(f"âŒ Failed to load: {model_name}")
                continue

        # If all models fail, use ChromaDB default
        logger.warning("âŒ All embedding models failed to load")
        logger.info("ðŸ”„ Falling back to ChromaDB default embedding...")
        self.current_model = None
        self.current_model_name = "chromadb-default"
        return False

    def encode_documents(
        self,
        texts: List[str],
        normalize: bool = True,
        use_batch_optimization: bool = True,
    ) -> Optional[List[List[float]]]:
        """
        Encode a list of documents into embeddings.

        Args:
            texts: List of texts to encode
            normalize: Whether to normalize embeddings
            use_batch_optimization: Whether to use batch processing optimization

        Returns:
            List of embeddings or None if no model available"""
        with track_execution_time("encode_documents"):
            # Use batch processor if available and requested
            if use_batch_optimization and self.batch_processor is not None:
                logger.debug(
                    f"ðŸš€ Using optimized batch processing for {len(texts)} documents"
                )
                # Note: batch processor is async, but we'll call it synchronously for now
                # In a real async environment, this would be awaited
                import asyncio

                try:
                    loop = asyncio.get_event_loop()
                    result = loop.run_until_complete(
                        self.batch_processor.encode_documents_optimized(
                            texts, normalize
                        )
                    )
                except RuntimeError:
                    # No event loop running, create one
                    result = asyncio.run(
                        self.batch_processor.encode_documents_optimized(
                            texts, normalize
                        )
                    )

                if result is not None:
                    self.operation_counts["encode_documents"] += 1
                return result
            else:
                # Fall back to original implementation
                result = self._do_encode_documents(texts, normalize)
                if result is not None:
                    self.operation_counts["encode_documents"] += 1
                return result

    def _do_encode_documents(
        self, texts: List[str], normalize: bool = True
    ) -> Optional[List[List[float]]]:
        """Internal method for encoding documents."""
        if not self.current_model:
            logger.warning("No embedding model available, returning None")
            return None

        try:
            logger.debug(
                f"ðŸ§  Encoding {len(texts)} documents with {self.current_model_name}"
            )
            embeddings = self.current_model.encode(
                texts, normalize_embeddings=normalize
            )

            # Convert numpy array to list for ChromaDB compatibility
            if isinstance(embeddings, np.ndarray):
                embeddings_list = embeddings.tolist()
            else:
                embeddings_list = embeddings

            logger.debug(
                f"âœ… Generated embeddings: {len(embeddings_list)} x {len(embeddings_list[0])}"
            )
            return embeddings_list

        except Exception as e:
            logger.error(f"âŒ Failed to encode documents: {e}")
            return None

    def encode_query(
        self, query: str, normalize: bool = True, use_cache: bool = True
    ) -> Optional[List[float]]:
        """
        Encode a single query into embedding.

        Args:
            query: Query text to encode
            normalize: Whether to normalize embedding
            use_cache: Whether to use cache for this query

        Returns:
            Query embedding or None if no model available
        """
        with track_execution_time("encode_query"):
            # Try cache first if batch processor is available
            if use_cache and self.batch_processor is not None:
                cached_embedding = self.batch_processor.cache.get(query)
                if cached_embedding is not None:
                    logger.debug(f"ðŸŽ¯ Cache hit for query: '{query[:50]}...'")
                    self.operation_counts["encode_query"] += 1
                    return cached_embedding

            # Encode the query
            result = self._do_encode_query(
                query, normalize
            )  # Cache the result if batch processor is available
            if result is not None and use_cache and self.batch_processor is not None:
                self.batch_processor.cache.set(query, result)

            if result is not None:
                self.operation_counts["encode_query"] += 1
            return result

    def _do_encode_query(
        self, query: str, normalize: bool = True
    ) -> Optional[List[float]]:
        """Internal method for encoding query."""
        if not self.current_model:
            logger.warning("No embedding model available, returning None")
            return None

        try:
            logger.debug(
                f"ðŸ§  Encoding query with {self.current_model_name}: '{query[:50]}...'"
            )
            embedding = self.current_model.encode(
                [query], normalize_embeddings=normalize
            )

            # Convert to list for ChromaDB compatibility
            if isinstance(embedding, np.ndarray):
                embedding_list = embedding[0].tolist()
            else:
                embedding_list = embedding[0]

            logger.debug(
                f"âœ… Generated query embedding: {len(embedding_list)} dimensions"
            )
            return embedding_list

        except Exception as e:
            logger.error(f"âŒ Failed to encode query: {e}")
            return None

    def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the current model.

        Returns:
            Model information dictionary including environment variables
        """
        # Get environment variable configuration
        env_model = self._get_model_from_env()
        env_device = os.getenv("EMBEDDING_DEVICE", "not set")
        env_cache_dir = os.getenv("EMBEDDING_CACHE_DIR", "not set")

        base_info = {
            "name": self.current_model_name,
            "configured_model": self.default_model,
            "environment_variables": {
                "EMBEDDING_MODEL": env_model or "not set",
                "EMBEDDING_DEVICE": env_device,
                "EMBEDDING_CACHE_DIR": env_cache_dir,
                "priority": "Environment Variables â†’ Config â†’ Defaults",
            },
            "fallback_models": self.model_priority,
        }

        if not self.current_model:
            base_info.update(
                {
                    "status": "No custom model loaded (using ChromaDB default)",
                    "embedding_dim": "Unknown",
                    "type": "ChromaDB default",
                    "device": "N/A",
                    "note": "Use configure_embedding_model() to load custom model",
                }
            )
            return base_info

        try:
            # Try to get embedding dimension
            test_embedding = self.current_model.encode(
                ["test"], normalize_embeddings=False
            )
            embedding_dim = (
                len(test_embedding[0]) if len(test_embedding) > 0 else "Unknown"
            )

            # Get device information
            device = "Unknown"
            try:
                if hasattr(self.current_model, "device"):
                    device = str(self.current_model.device)
                else:
                    device = self._detect_optimal_device()
            except:
                device = "Unknown"

        except:
            embedding_dim = "Unknown"
            device = "Unknown"

        base_info.update(
            {
                "status": "Loaded and ready",
                "embedding_dim": embedding_dim,
                "type": "SentenceTransformer",
                "device": device,
                "cached_models": list(self.models.keys()),
            }
        )

        return base_info

    def get_metrics(self) -> Dict[str, Any]:
        """
        Get performance metrics for embedding operations.

        Returns:
            Dictionary containing performance metrics
        """
        base_metrics = self.metrics.get_performance_summary()
        # Add our operation counts
        base_metrics.update(
            {
                "operation_counts": self.operation_counts,
                "total_custom_operations": sum(self.operation_counts.values()),
            }
        )

        # Add batch processing metrics if available
        if self.batch_processor is not None:
            batch_metrics = self.batch_processor.get_performance_metrics()
            base_metrics.update({"batch_processing_optimization": batch_metrics})

        return base_metrics

    def clear_embedding_cache(self) -> Optional[Dict[str, int]]:
        """
        Clear the embedding cache if batch processor is available.

        Returns:
            Cache statistics before clearing, or None if not available
        """
        if self.batch_processor is not None:
            return self.batch_processor.clear_cache()
        return None

    def get_cache_stats(self) -> Optional[Dict[str, Any]]:
        """
        Get embedding cache statistics.

        Returns:
            Cache statistics or None if not available
        """
        if self.batch_processor is not None:
            return self.batch_processor.cache.get_stats()
        return None

    def intelligent_chunk_text(
        self, text: str, chunk_size: int = 400, overlap: int = 50
    ) -> List[str]:
        """
        Intelligent text chunking for Vietnamese text.

        Args:
            text: Text to chunk
            chunk_size: Maximum chunk size
            overlap: Overlap between chunks

        Returns:
            List of text chunks
        """
        # TÃ¡ch theo Ä‘oáº¡n vÄƒn trÆ°á»›c
        paragraphs = [p.strip() for p in text.split("\\n\\n") if p.strip()]

        chunks = []
        current_chunk = ""

        for para in paragraphs:
            # Náº¿u Ä‘oáº¡n vÄƒn quÃ¡ dÃ i, chia nhá» hÆ¡n
            if len(para) > chunk_size:
                # TÃ¡ch theo cÃ¢u
                sentences = []
                temp_sentence = ""

                for char in para:
                    temp_sentence += char
                    if char in ".!?":
                        # Kiá»ƒm tra khÃ´ng pháº£i sá»‘ tháº­p phÃ¢n
                        remaining_text = para[
                            para.find(temp_sentence) + len(temp_sentence) :
                        ]
                        next_chars = (
                            remaining_text[:3]
                            if len(remaining_text) >= 3
                            else remaining_text
                        )
                        if not any(c.isdigit() for c in next_chars):
                            sentences.append(temp_sentence.strip())
                            temp_sentence = ""

                # ThÃªm cÃ¢u cuá»‘i náº¿u cÃ²n
                if temp_sentence.strip():
                    sentences.append(temp_sentence.strip())

                # Combine sentences to chunks
                for sentence in sentences:
                    if len(current_chunk) + len(sentence) <= chunk_size:
                        current_chunk += " " + sentence if current_chunk else sentence
                    else:
                        if current_chunk.strip():
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence
            else:
                # Äoáº¡n vÄƒn ngáº¯n, thÃªm trá»±c tiáº¿p
                if len(current_chunk) + len(para) <= chunk_size:
                    current_chunk += "\\n" + para if current_chunk else para
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    current_chunk = para

        # ThÃªm chunk cuá»‘i
        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return [chunk for chunk in chunks if len(chunk.strip()) > 20]

    def get_model_sizes_info(self) -> Dict[str, Any]:
        """
        Get information about model sizes for user to choose.

        Returns:
            Dictionary with model names and their approximate sizes
        """
        size_info = {
            "nomic-ai/nomic-embed-text-v2-moe": "305M active/475M total - Latest MoE version, SoTA multilingual, 768D embeddings, no token needed",
            # "nomic-ai/nomic-embed-text-v1.5": "137MB - Previous version, stable fallback, 768D embeddings",
            # "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "177MB - Fast, multilingual",
            # "sentence-transformers/paraphrase-multilingual-mpnet-base-v2": "1.1GB - High quality, multilingual",
        }

        return {
            "available_models": size_info,
            "recommendation": "For best multilingual performance: nomic-ai/nomic-embed-text-v2-moe",
            "note": "All models are publicly accessible, no HF token required",
        }

    def _get_model_from_env(self) -> Optional[str]:
        """
        Get embedding model from environment variables (PRIORITY 1).

        Environment Variables checked in order:
        1. EMBEDDING_MODEL - Primary model name
        2. MCP_EMBEDDING_MODEL - Alternative naming
        3. CHROMA_EMBEDDING_MODEL - ChromaDB specific

        Returns:
            Model name if found in env, None otherwise
        """
        env_vars = ["EMBEDDING_MODEL", "MCP_EMBEDDING_MODEL", "CHROMA_EMBEDDING_MODEL"]

        for env_var in env_vars:
            model_name = os.getenv(env_var)
            if model_name:
                logger.info(f"ðŸŒ Using model from {env_var}: {model_name}")
                return model_name.strip()

        logger.info("ðŸ” No embedding model found in environment variables")
        return None

    def _get_fallback_models_from_env(self) -> List[str]:
        """
        Get fallback models from environment variables.

        Environment Variables:
        - EMBEDDING_FALLBACK_MODELS: comma-separated list

        Returns:
            List of fallback model names
        """
        fallback_env = os.getenv("EMBEDDING_FALLBACK_MODELS", "")
        if fallback_env:
            models = [m.strip() for m in fallback_env.split(",") if m.strip()]
            logger.info(f"ðŸ”„ Fallback models from env: {models}")
            return models

        # Default fallback models if not in env
        default_fallbacks = [
            "sentence-transformers/all-MiniLM-L6-v2",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        ]
        logger.info(f"ðŸ“‹ Using default fallback models: {default_fallbacks}")
        return default_fallbacks

    def _get_device_from_env(self) -> str:
        """
        Get device preference from environment variables.

        Environment Variables:
        - EMBEDDING_DEVICE: cpu, cuda, mps, auto

        Returns:
            Device string
        """
        device_env = os.getenv("EMBEDDING_DEVICE", "auto").lower()

        if device_env == "auto":
            return self._detect_optimal_device()
        elif device_env in ["cpu", "cuda", "mps"]:
            logger.info(f"ðŸŽ¯ Using device from env: {device_env}")
            return device_env
        else:
            logger.warning(f"âš ï¸ Invalid device in env: {device_env}, using auto")
            return self._detect_optimal_device()

    def _get_cache_dir_from_env(self) -> str:
        """
        Get cache directory from environment variables.

        Environment Variables checked in order:
        1. EMBEDDING_CACHE_DIR
        2. SENTENCE_TRANSFORMERS_HOME
        3. MODEL_CACHE_DIR

        Returns:
            Cache directory path
        """
        env_vars = [
            "EMBEDDING_CACHE_DIR",
            "SENTENCE_TRANSFORMERS_HOME",
            "MODEL_CACHE_DIR",
        ]

        for env_var in env_vars:
            cache_dir = os.getenv(env_var)
            if cache_dir:
                logger.info(f"ðŸ“ Using cache dir from {env_var}: {cache_dir}")
                return cache_dir

        default_cache = "./models_cache"
        logger.info(f"ðŸ“ Using default cache dir: {default_cache}")
        return default_cache

```

## File ..\py.chroma.mcp\src\embedding\__init__.py:
```python
# Generated by Copilot
"""
Embedding module for ChromaDB MCP server.

This module provides comprehensive embedding functionality including:
- Embedding model management and loading
- Intelligent text chunking for better embeddings
- Hugging Face authentication and token management
- Vector quality assessment and optimization

Features:
- Support for multiple embedding models (sentence-transformers, etc.)
- Vietnamese-optimized text chunking
- Caching and performance optimization
- Error handling and graceful fallbacks
"""

from .manager import (
    EmbeddingManager,
)

# Batch processor imports removed to avoid circular dependency
# Can be imported directly as: from embedding.batch_processor import BatchProcessor

from .chunker import (
    TextChunker,
    ChunkingStrategy,
    chunk_text_intelligent,
    chunk_text_semantic,
    optimize_chunk_size,
)

__all__ = [
    # Manager
    "EmbeddingManager",
    # Batch Processing - import directly from embedding.batch_processor
    # "BatchProcessor",
    # "EmbeddingCache",
    # Chunking
    "TextChunker",
    "ChunkingStrategy",
    "chunk_text_intelligent",
    "chunk_text_semantic",
    "optimize_chunk_size",
]

# Version info
__version__ = "1.0.0"

```

## File ..\py.chroma.mcp\src\utils\error_handler.py:
```python
# Generated by Copilot
"""
Enhanced error handling and logging utilities for ChromaDB MCP server.

Provides comprehensive error tracking, logging, and recovery mechanisms.
"""

import traceback
import time
from typing import Dict, Any, Optional, Callable, TypeVar
from functools import wraps
from dataclasses import dataclass, field
import asyncio

from utils.logger import get_logger

logger = get_logger(__name__)

F = TypeVar("F", bound=Callable[..., Any])


@dataclass
class ErrorContext:
    """Context information for error tracking."""

    operation_name: str
    error_type: str
    error_message: str
    timestamp: float = field(default_factory=time.time)
    stack_trace: Optional[str] = None
    function_name: Optional[str] = None
    args: Optional[str] = None
    kwargs: Optional[str] = None
    execution_time: Optional[float] = None
    additional_context: Dict[str, Any] = field(default_factory=dict)


class ErrorTracker:
    """Centralized error tracking system."""

    def __init__(self):
        self.errors: list[ErrorContext] = []
        self._lock = asyncio.Lock()

    async def log_error(
        self,
        operation_name: str,
        error: Exception,
        context: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Log an error with context information."""
        error_context = ErrorContext(
            operation_name=operation_name,
            error_type=type(error).__name__,
            error_message=str(error),
            stack_trace=traceback.format_exc(),
            additional_context=context or {},
        )

        async with self._lock:
            self.errors.append(error_context)

        logger.error(
            f"âŒ {operation_name} failed: {type(error).__name__} - {str(error)}"
        )

    def log_error_sync(
        self,
        operation_name: str,
        error: Exception,
        context: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Synchronous version of log_error."""
        error_context = ErrorContext(
            operation_name=operation_name,
            error_type=type(error).__name__,
            error_message=str(error),
            stack_trace=traceback.format_exc(),
            additional_context=context or {},
        )

        self.errors.append(error_context)

        logger.error(
            f"âŒ {operation_name} failed: {type(error).__name__} - {str(error)}"
        )

    def get_error_summary(self) -> Dict[str, Any]:
        """Get summary of all tracked errors."""
        if not self.errors:
            return {"total_errors": 0}

        error_types = {}
        operations_with_errors = {}

        for error in self.errors:
            # Count by error type
            error_types[error.error_type] = error_types.get(error.error_type, 0) + 1
            # Count by operation
            operations_with_errors[error.operation_name] = (
                operations_with_errors.get(error.operation_name, 0) + 1
            )

        return {
            "total_errors": len(self.errors),
            "error_types": error_types,
            "operations_with_errors": operations_with_errors,
            "recent_errors": [
                {
                    "operation": error.operation_name,
                    "type": error.error_type,
                    "message": error.error_message,
                    "timestamp": error.timestamp,
                }
                for error in self.errors[-5:]  # Last 5 errors
            ],
        }


# Global error tracker instance
_error_tracker = ErrorTracker()


def get_error_tracker() -> ErrorTracker:
    """Get the global error tracker instance."""
    return _error_tracker


def handle_mcp_tool_errors(operation_name: Optional[str] = None):
    """
    Decorator Ä‘á»ƒ tá»± Ä‘á»™ng handle errors cho MCP tools.

    Args:
        operation_name: TÃªn operation Ä‘á»ƒ track. Náº¿u None sáº½ dÃ¹ng function name.
    """

    def decorator(func: F) -> F:
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            op_name = operation_name or func.__name__
            error_tracker = get_error_tracker()
            start_time = time.time()

            try:
                logger.info(f"ðŸš€ Starting {op_name}")
                result = await func(*args, **kwargs)

                execution_time = time.time() - start_time
                logger.info(f"âœ… {op_name} completed in {execution_time:.2f}s")

                return result

            except Exception as e:
                execution_time = time.time() - start_time

                # Log error vá»›i context
                await error_tracker.log_error(
                    op_name,
                    e,
                    {
                        "function": func.__name__,
                        "args": str(args)[:200],  # Truncate args for logging
                        "kwargs": str(kwargs)[:200],
                        "execution_time": execution_time,
                    },
                )

                # Re-raise vá»›i enhanced error message
                raise Exception(f"{op_name} failed: {str(e)}") from e

        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            op_name = operation_name or func.__name__
            error_tracker = get_error_tracker()
            start_time = time.time()

            try:
                logger.info(f"ðŸš€ Starting {op_name}")
                result = func(*args, **kwargs)

                execution_time = time.time() - start_time
                logger.info(f"âœ… {op_name} completed in {execution_time:.2f}s")

                return result

            except Exception as e:
                execution_time = time.time() - start_time

                # Log error vá»›i context
                error_tracker.log_error_sync(
                    op_name,
                    e,
                    {
                        "function": func.__name__,
                        "args": str(args)[:200],
                        "kwargs": str(kwargs)[:200],
                        "execution_time": execution_time,
                    },
                )

                # Re-raise vá»›i enhanced error message
                raise Exception(f"{op_name} failed: {str(e)}") from e

        # Return appropriate wrapper based on function type
        if asyncio.iscoroutinefunction(func):
            return async_wrapper  # type: ignore
        else:
            return sync_wrapper  # type: ignore

    return decorator


def log_operation_start(operation_name: str, **context) -> None:
    """Log the start of an operation."""
    context_str = ", ".join(f"{k}={v}" for k, v in context.items())
    logger.info(
        f"ðŸš€ Starting {operation_name}" + (f" ({context_str})" if context_str else "")
    )


def log_operation_success(operation_name: str, duration: float, **context) -> None:
    """Log successful completion of an operation."""
    context_str = ", ".join(f"{k}={v}" for k, v in context.items())
    logger.info(
        f"âœ… {operation_name} completed in {duration:.2f}s"
        + (f" ({context_str})" if context_str else "")
    )


def log_operation_error(
    operation_name: str, error: Exception, duration: float, **context
) -> None:
    """Log error in an operation."""
    context_str = ", ".join(f"{k}={v}" for k, v in context.items())
    logger.error(
        f"âŒ {operation_name} failed after {duration:.2f}s: {type(error).__name__} - {str(error)}"
        + (f" ({context_str})" if context_str else "")
    )


class OperationContext:
    """Context manager for operation logging."""

    def __init__(self, operation_name: str, **context):
        self.operation_name = operation_name
        self.context = context
        self.start_time = None

    def __enter__(self):
        self.start_time = time.time()
        log_operation_start(self.operation_name, **self.context)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.start_time is None:
            return False

        duration = time.time() - self.start_time

        if exc_type is None:
            log_operation_success(self.operation_name, duration, **self.context)
        else:
            log_operation_error(self.operation_name, exc_val, duration, **self.context)
            # Track error
            get_error_tracker().log_error_sync(
                self.operation_name, exc_val, self.context
            )

        return False  # Don't suppress exceptions

```

## File ..\py.chroma.mcp\src\utils\logger.py:
```python
#!/usr/bin/env python3
"""
Centralized Logger Configuration for ChromaDB MCP Server
Provides consistent logging configuration across all modules.
Generated by Copilot
"""

import logging
import sys
from pathlib import Path
from typing import Optional


class LoggerConfig:
    """Centralized logger configuration."""

    _configured = False
    _loggers = {}

    @classmethod
    def configure_logging(
        cls,
        level: int = logging.INFO,
        format_string: Optional[str] = None,
        log_to_file: bool = False,
        log_file_path: Optional[Path] = None,
    ) -> None:
        """Configure global logging settings.

        Args:
            level: Logging level (default: INFO)
            format_string: Custom format string
            log_to_file: Whether to log to file
            log_file_path: Path to log file (if log_to_file is True)
        """
        if cls._configured:
            return

        # Default format
        if format_string is None:
            format_string = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

        # Configure root logger
        handlers = []

        # Always add stderr handler to avoid stdout pollution
        stderr_handler = logging.StreamHandler(sys.stderr)
        stderr_handler.setFormatter(logging.Formatter(format_string))
        handlers.append(stderr_handler)

        # Add file handler if requested
        if log_to_file and log_file_path:
            log_file_path.parent.mkdir(parents=True, exist_ok=True)
            file_handler = logging.FileHandler(log_file_path, encoding="utf-8")
            file_handler.setFormatter(logging.Formatter(format_string))
            handlers.append(file_handler)

        # Configure root logger
        logging.basicConfig(
            level=level,
            format=format_string,
            handlers=handlers,
            force=True,  # Override any existing configuration
        )

        cls._configured = True

    @classmethod
    def get_logger(cls, name: str) -> logging.Logger:
        """Get a logger for the specified module.

        Args:
            name: Logger name (typically __name__)

        Returns:
            Configured logger instance
        """
        # Ensure logging is configured
        if not cls._configured:
            cls.configure_logging()

        # Return cached logger or create new one
        if name not in cls._loggers:
            cls._loggers[name] = logging.getLogger(name)

        return cls._loggers[name]

    @classmethod
    def set_level(cls, level: int) -> None:
        """Set logging level for all loggers.

        Args:
            level: New logging level
        """
        logging.getLogger().setLevel(level)
        for logger in cls._loggers.values():
            logger.setLevel(level)

    @classmethod
    def is_configured(cls) -> bool:
        """Check if logging is already configured."""
        return cls._configured


# Convenience function for getting loggers
def get_logger(name: str) -> logging.Logger:
    """Get a logger for the specified module.

    Args:
        name: Logger name (typically __name__)

    Returns:
        Configured logger instance

    Example:
        from utils.logger import get_logger
        logger = get_logger(__name__)
    """
    return LoggerConfig.get_logger(name)


# Auto-configure logging when module is imported
def auto_configure():
    """Auto-configure logging with default settings."""
    if not LoggerConfig.is_configured():
        LoggerConfig.configure_logging()


# Initialize logging when module is imported
auto_configure()

```

## File ..\py.chroma.mcp\src\utils\metrics.py:
```python
# Generated by Copilot
"""
Simple metrics collection for ChromaDB MCP server.

Provides basic performance and quality metrics.
"""

import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field

from utils.logger import get_logger

logger = get_logger(__name__)


@dataclass
class PerformanceMetrics:
    """Container for performance measurement data."""

    operation_name: str
    execution_time: float
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0
    peak_memory_mb: float = 0.0
    start_time: float = field(default_factory=time.time)
    end_time: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None
    success: bool = True

    def to_dict(self) -> Dict[str, Any]:
        """Convert metrics to dictionary format."""
        return {
            "operation_name": self.operation_name,
            "execution_time": self.execution_time,
            "memory_usage_mb": self.memory_usage_mb,
            "cpu_usage_percent": self.cpu_usage_percent,
            "peak_memory_mb": self.peak_memory_mb,
            "start_time": self.start_time,
            "end_time": self.end_time,
            "metadata": self.metadata,
            "error": self.error,
            "success": self.success,
        }


@dataclass
class QualityMetrics:
    """Container for data quality metrics."""

    operation_name: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    data_quality_score: float
    metadata: Dict[str, Any] = field(default_factory=dict)


class MetricsCollector:
    """Simple metrics collection system."""

    def __init__(self):
        self.performance_metrics: List[PerformanceMetrics] = []
        self.quality_metrics: List[QualityMetrics] = []

    def add_performance_metric(self, metric: PerformanceMetrics) -> None:
        """Add performance metric."""
        self.performance_metrics.append(metric)
        logger.debug(f"ðŸ“Š Added performance metric: {metric.operation_name}")

    def get_performance_summary(self) -> Dict[str, Any]:
        """Get performance summary."""
        if not self.performance_metrics:
            return {"total_operations": 0}

        execution_times = [m.execution_time for m in self.performance_metrics]

        return {
            "total_operations": len(self.performance_metrics),
            "avg_execution_time": sum(execution_times) / len(execution_times),
            "max_execution_time": max(execution_times),
            "min_execution_time": min(execution_times),
        }


# Global metrics collector
_metrics_collector = MetricsCollector()


def get_metrics_collector() -> MetricsCollector:
    """Get the global metrics collector."""
    return _metrics_collector


# Simple functions for compatibility
def benchmark_function(func):
    """Simple function benchmark decorator."""
    return func  # Simplified for now


def measure_memory_usage(operation_name: str):
    """Simple memory usage context manager."""

    class SimpleContext:
        def __enter__(self):
            return {"memory_delta_mb": 0.0, "peak_memory_mb": 0.0}

        def __exit__(self, *args):
            pass

    return SimpleContext()


def track_execution_time(operation_name: str):
    """Simple execution time context manager."""

    class SimpleContext:
        def __enter__(self):
            self.start_time = time.time()
            return self

        def __exit__(self, *args):
            execution_time = time.time() - self.start_time
            logger.info(f"â±ï¸ {operation_name} took {execution_time:.2f}s")

    return SimpleContext()

```

## File ..\py.chroma.mcp\src\utils\metrics_simple.py:
```python
# Generated by Copilot
"""
Simple metrics collection for ChromaDB MCP server.

Provides basic performance and quality metrics.
"""

import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field

from utils.logger import get_logger

logger = get_logger(__name__)


@dataclass
class PerformanceMetrics:
    """Container for performance measurement data."""

    operation_name: str
    execution_time: float
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0
    peak_memory_mb: float = 0.0
    start_time: float = field(default_factory=time.time)
    end_time: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None
    success: bool = True

    def to_dict(self) -> Dict[str, Any]:
        """Convert metrics to dictionary format."""
        return {
            "operation_name": self.operation_name,
            "execution_time": self.execution_time,
            "memory_usage_mb": self.memory_usage_mb,
            "cpu_usage_percent": self.cpu_usage_percent,
            "peak_memory_mb": self.peak_memory_mb,
            "start_time": self.start_time,
            "end_time": self.end_time,
            "metadata": self.metadata,
            "error": self.error,
            "success": self.success,
        }


@dataclass
class QualityMetrics:
    """Container for data quality metrics."""

    operation_name: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    data_quality_score: float
    metadata: Dict[str, Any] = field(default_factory=dict)


class MetricsCollector:
    """Simple metrics collection system."""

    def __init__(self):
        self.performance_metrics: List[PerformanceMetrics] = []
        self.quality_metrics: List[QualityMetrics] = []

    def add_performance_metric(self, metric: PerformanceMetrics) -> None:
        """Add performance metric."""
        self.performance_metrics.append(metric)
        logger.debug(f"ðŸ“Š Added performance metric: {metric.operation_name}")

    def get_performance_summary(self) -> Dict[str, Any]:
        """Get performance summary."""
        if not self.performance_metrics:
            return {"total_operations": 0}

        execution_times = [m.execution_time for m in self.performance_metrics]

        return {
            "total_operations": len(self.performance_metrics),
            "avg_execution_time": sum(execution_times) / len(execution_times),
            "max_execution_time": max(execution_times),
            "min_execution_time": min(execution_times),
        }


# Global metrics collector
_metrics_collector = MetricsCollector()


def get_metrics_collector() -> MetricsCollector:
    """Get the global metrics collector."""
    return _metrics_collector


# Simple functions for compatibility
def benchmark_function(func):
    """Simple function benchmark decorator."""
    return func  # Simplified for now


def measure_memory_usage(operation_name: str):
    """Simple memory usage context manager."""

    class SimpleContext:
        def __enter__(self):
            return {"memory_delta_mb": 0.0, "peak_memory_mb": 0.0}

        def __exit__(self, *args):
            pass

    return SimpleContext()


def track_execution_time(operation_name: str):
    """Simple execution time context manager."""

    class SimpleContext:
        def __enter__(self):
            self.start_time = time.time()
            return self

        def __exit__(self, *args):
            execution_time = time.time() - self.start_time
            logger.info(f"â±ï¸ {operation_name} took {execution_time:.2f}s")

    return SimpleContext()

```

## File ..\py.chroma.mcp\src\utils\model_loader.py:
```python
#!/usr/bin/env python3
"""
Add async model loading vá»›i proper synchronization Ä‘á»ƒ fix race condition.
Generated by Copilot
"""

import asyncio
import threading
from typing import Optional
from utils.logger import get_logger

logger = get_logger(__name__)


class ModelLoadingManager:
    """Manages async model loading vá»›i synchronization."""

    def __init__(self):
        self._loading_lock = asyncio.Lock()
        self._sync_lock = threading.Lock()
        self._is_loading = False
        self._load_complete = False
        self._load_event = asyncio.Event()

    async def ensure_model_loaded(self, embedding_manager, force_reload: bool = False):
        """Ensure model is loaded hoáº·c wait cho loading hoÃ n táº¥t.

        Args:
            embedding_manager: EmbeddingManager instance
            force_reload: Whether to force reload

        Returns:
            True if model loaded successfully
        """
        # Náº¿u model Ä‘Ã£ load vÃ  khÃ´ng force reload
        if self._load_complete and not force_reload:
            return True

        # Náº¿u Ä‘ang loading, wait cho nÃ³ hoÃ n táº¥t
        if self._is_loading:
            logger.info("â³ Model is loading, waiting for completion...")
            await self._load_event.wait()
            return self._load_complete

        # Load model vá»›i lock
        async with self._loading_lock:
            # Double check sau khi acquire lock
            if self._load_complete and not force_reload:
                return True

            self._is_loading = True
            self._load_event.clear()

            try:
                logger.info("ðŸ”„ Starting async model loading...")

                # Run model loading in thread pool Ä‘á»ƒ khÃ´ng block event loop
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(
                    None, self._sync_load_model, embedding_manager, force_reload
                )

                self._load_complete = result
                logger.info(f"âœ… Async model loading completed: {result}")
                return result

            except Exception as e:
                logger.error(f"âŒ Async model loading failed: {e}")
                self._load_complete = False
                return False

            finally:
                self._is_loading = False
                self._load_event.set()  # Notify waiting coroutines

    def _sync_load_model(self, embedding_manager, force_reload: bool) -> bool:
        """Synchronous model loading method."""
        try:
            from config import get_embedding_config

            embedding_config = get_embedding_config()
            default_model = embedding_config["default_model"]

            logger.info(f"ðŸŽ¯ Loading model: {default_model}")
            return embedding_manager.load_model(default_model, force_reload)

        except Exception as e:
            logger.error(f"âŒ Model loading failed: {e}")
            return False

    def reset(self):
        """Reset loading state (for testing)."""
        with self._sync_lock:
            self._is_loading = False
            self._load_complete = False

    @property
    def is_loaded(self) -> bool:
        """Check if model is loaded."""
        return self._load_complete

    @property
    def is_loading(self) -> bool:
        """Check if model is currently loading."""
        return self._is_loading


# Global instance
_model_loading_manager: Optional[ModelLoadingManager] = None


def get_model_loading_manager() -> ModelLoadingManager:
    """Get global model loading manager."""
    global _model_loading_manager
    if _model_loading_manager is None:
        _model_loading_manager = ModelLoadingManager()
    return _model_loading_manager

```

## File ..\py.chroma.mcp\src\utils\validators.py:
```python
# Generated by Copilot
"""
Input validation utilities for ChromaDB MCP server.

This module provides validation for ChromaDB operations including:
- Collection name validation
- Document validation
- Query parameter validation
- Input sanitization
"""

import re
import json
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass

from utils.logger import get_logger

logger = get_logger(__name__)

# Constants
MAX_COLLECTION_NAME_LENGTH = 63
MAX_DOCUMENT_SIZE = 1024 * 1024  # 1MB
MAX_DOCUMENTS_PER_BATCH = 1000


@dataclass
class ValidationResult:
    """Result of input validation with detailed feedback."""

    is_valid: bool
    errors: List[str]
    warnings: List[str]
    sanitized_data: Optional[Any] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert validation result to dictionary."""
        return {
            "is_valid": self.is_valid,
            "errors": self.errors,
            "warnings": self.warnings,
            "sanitized_data": self.sanitized_data,
        }


class InputValidator:
    """Base validator class with common validation utilities."""

    def __init__(self) -> None:
        """Initialize validator."""
        self.validation_count = 0
        self.error_count = 0
        logger.debug("ðŸ”§ InputValidator initialized")

    def _create_result(self, is_valid: bool = True) -> ValidationResult:
        """Create new validation result."""
        self.validation_count += 1
        if not is_valid:
            self.error_count += 1

        return ValidationResult(is_valid=is_valid, errors=[], warnings=[])

    def get_validation_stats(self) -> Dict[str, Union[int, float]]:
        """Get validation statistics."""
        return {
            "total_validations": self.validation_count,
            "total_errors": self.error_count,
            "success_rate": (self.validation_count - self.error_count)
            / max(self.validation_count, 1),
        }


class CollectionValidator(InputValidator):
    """Validator for ChromaDB collection operations."""

    def validate_collection_name(self, name: str) -> ValidationResult:
        """Validate collection name according to ChromaDB requirements."""
        result = self._create_result()

        if not isinstance(name, str):
            result.is_valid = False
            result.errors.append("Collection name must be a string")
            return result

        if len(name) == 0:
            result.is_valid = False
            result.errors.append("Collection name cannot be empty")
            return result

        if len(name) > MAX_COLLECTION_NAME_LENGTH:
            result.is_valid = False
            result.errors.append(
                f"Collection name exceeds maximum length of {MAX_COLLECTION_NAME_LENGTH}"
            )
            return result

        if not re.match(r"^[a-zA-Z0-9_-]+$", name):
            result.is_valid = False
            result.errors.append(
                "Collection name can only contain letters, numbers, underscores, and hyphens"
            )
            return result

        if name.startswith("_"):
            result.warnings.append(
                "Collection names starting with underscore are reserved"
            )

        result.sanitized_data = name.lower()
        logger.debug(f"âœ… Collection name '{name}' validated successfully")
        return result

    def validate_metadata(self, metadata: Optional[Dict[str, Any]]) -> ValidationResult:
        """Validate collection metadata."""
        result = self._create_result()

        if metadata is None:
            result.sanitized_data = {}
            return result

        if not isinstance(metadata, dict):
            result.is_valid = False
            result.errors.append("Metadata must be a dictionary")
            return result

        result.sanitized_data = metadata
        logger.debug(f"âœ… Metadata validated with {len(metadata)} fields")
        return result


class DocumentValidator(InputValidator):
    """Validator for document operations."""

    def validate_documents(self, documents: List[str]) -> ValidationResult:
        """Validate list of documents."""
        result = self._create_result()

        if not isinstance(documents, list):
            result.is_valid = False
            result.errors.append("Documents must be provided as a list")
            return result

        if len(documents) == 0:
            result.is_valid = False
            result.errors.append("Documents list cannot be empty")
            return result

        if len(documents) > MAX_DOCUMENTS_PER_BATCH:
            result.is_valid = False
            result.errors.append(
                f"Too many documents: {len(documents)} > {MAX_DOCUMENTS_PER_BATCH}"
            )
            return result

        sanitized_docs = []
        for i, doc in enumerate(documents):
            if not isinstance(doc, str):
                result.errors.append(f"Document at index {i} must be a string")
                continue

            if len(doc.encode("utf-8")) > MAX_DOCUMENT_SIZE:
                result.errors.append(f"Document at index {i} exceeds maximum size")
                continue

            sanitized_doc = doc.strip()
            if not sanitized_doc:
                result.warnings.append(
                    f"Document at index {i} is empty after sanitization"
                )
                continue

            sanitized_docs.append(sanitized_doc)

        if result.errors:
            result.is_valid = False
        else:
            result.sanitized_data = sanitized_docs
            logger.debug(f"âœ… Validated {len(sanitized_docs)} documents successfully")

        return result


class QueryValidator(InputValidator):
    """Validator for query operations."""

    def validate_query_texts(self, query_texts: List[str]) -> ValidationResult:
        """Validate query text list."""
        result = self._create_result()

        if not isinstance(query_texts, list):
            result.is_valid = False
            result.errors.append("Query texts must be provided as a list")
            return result

        if len(query_texts) == 0:
            result.is_valid = False
            result.errors.append("Query texts list cannot be empty")
            return result

        sanitized_queries = []
        for i, query in enumerate(query_texts):
            if not isinstance(query, str):
                result.errors.append(f"Query at index {i} must be a string")
                continue

            sanitized_query = query.strip()
            if not sanitized_query:
                result.errors.append(f"Query at index {i} cannot be empty")
                continue

            sanitized_queries.append(sanitized_query)

        if result.errors:
            result.is_valid = False
        else:
            result.sanitized_data = sanitized_queries
            logger.debug(
                f"âœ… Validated {len(sanitized_queries)} query texts successfully"
            )

        return result

    def validate_n_results(self, n_results: int) -> ValidationResult:
        """Validate number of results parameter."""
        result = self._create_result()

        if not isinstance(n_results, int):
            try:
                n_results = int(n_results)
            except (ValueError, TypeError):
                result.is_valid = False
                result.errors.append("n_results must be an integer")
                return result

        if n_results <= 0:
            result.is_valid = False
            result.errors.append("n_results must be a positive integer")
            return result

        if n_results > 1000:
            result.warnings.append(
                f"Large n_results value ({n_results}) may impact performance"
            )

        result.sanitized_data = n_results
        logger.debug(f"âœ… Validated n_results: {n_results}")
        return result


class ValidationOrchestrator:
    """Orchestrates validation across multiple validators."""

    def __init__(self) -> None:
        """Initialize orchestrator with all validators."""
        self.collection_validator = CollectionValidator()
        self.document_validator = DocumentValidator()
        self.query_validator = QueryValidator()
        logger.debug("ðŸ—ï¸ ValidationOrchestrator initialized")

    def validate_collection_creation(
        self, name: str, metadata: Optional[Dict[str, Any]] = None
    ) -> ValidationResult:
        """Validate complete collection creation request."""
        name_result = self.collection_validator.validate_collection_name(name)
        if not name_result.is_valid:
            return name_result

        metadata_result = self.collection_validator.validate_metadata(metadata)
        if not metadata_result.is_valid:
            return metadata_result

        combined_result = ValidationResult(
            is_valid=True,
            errors=[],
            warnings=name_result.warnings + metadata_result.warnings,
            sanitized_data={
                "name": name_result.sanitized_data,
                "metadata": metadata_result.sanitized_data,
            },
        )

        logger.info(f"âœ… Collection creation validation passed for '{name}'")
        return combined_result

    def get_overall_stats(self) -> Dict[str, Union[int, float]]:
        """Get validation statistics from all validators."""
        collection_stats = self.collection_validator.get_validation_stats()
        document_stats = self.document_validator.get_validation_stats()
        query_stats = self.query_validator.get_validation_stats()

        total_validations = (
            collection_stats["total_validations"]
            + document_stats["total_validations"]
            + query_stats["total_validations"]
        )
        total_errors = (
            collection_stats["total_errors"]
            + document_stats["total_errors"]
            + query_stats["total_errors"]
        )

        return {
            "total_validations": total_validations,
            "total_errors": total_errors,
            "overall_success_rate": (total_validations - total_errors)
            / max(total_validations, 1),
        }


# Global validator instance
validator = ValidationOrchestrator()

# Export main classes and functions
__all__ = [
    "ValidationResult",
    "InputValidator",
    "CollectionValidator",
    "DocumentValidator",
    "QueryValidator",
    "ValidationOrchestrator",
    "validator",
]

```

## File ..\py.chroma.mcp\src\utils\__init__.py:
```python
# Generated by Copilot
"""
Utils module for ChromaDB MCP server utilities.

Provides common utilities for metrics, validation, and helper functions.
"""

from .metrics import (
    MetricsCollector,
    get_metrics_collector,
    benchmark_function,
    measure_memory_usage,
    track_execution_time,
)

from .validators import (
    ValidationResult,
    InputValidator,
    CollectionValidator,
    DocumentValidator,
    QueryValidator,
    ValidationOrchestrator,
    validator,
)

from .error_handler import (
    ErrorContext,
    ErrorTracker,
    get_error_tracker,
    handle_mcp_tool_errors,
    log_operation_start,
    log_operation_success,
    log_operation_error,
    OperationContext,
)

__all__ = [
    # Metrics
    "MetricsCollector",
    "get_metrics_collector",
    "benchmark_function",
    "measure_memory_usage",
    "track_execution_time",
    # Validators
    "ValidationResult",
    "InputValidator",
    "CollectionValidator",
    "DocumentValidator",
    "QueryValidator",
    "ValidationOrchestrator",
    "validator",
    # Error handling
    "ErrorContext",
    "ErrorTracker",
    "get_error_tracker",
    "handle_mcp_tool_errors",
    "log_operation_start",
    "log_operation_success",
    "log_operation_error",
    "OperationContext",
]

# Version info
__version__ = "1.0.0"

```

